{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "ADPET68xjlzr",
    "tags": []
   },
   "source": [
    "# 第10章: 事前学習済み言語モデル（GPT型）\n",
    "\n",
    "本章では、GPT型（Transformerのデコーダ型）の事前学習済みモデルを利用して、言語生成、評判分析器（ポジネガ分類器）の構築、ファインチューニング、強化学習などに取り組む。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "C1xKmMckti92",
    "tags": []
   },
   "source": [
    "## 90. 次単語予測\n",
    "\n",
    "“The movie was full of\"に続くトークン（トークン列ではなく一つのトークンであることに注意せよ）として適切なもの上位10個と、その確率（尤度）を求めよ。ただし、言語モデルへのプロンプトがどのようなトークン列に変換されたか、確認せよ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.11.10 environment at: /Users/ryuichi/.venv\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 14ms\u001b[0m\u001b[0m\n",
      "\u001b[2mUsing Python 3.11.10 environment at: /Users/ryuichi/.venv\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 2ms\u001b[0m\u001b[0m\n",
      "\u001b[2mUsing Python 3.11.10 environment at: /Users/ryuichi/.venv\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 3ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install openai\n",
    "!uv pip install python-dotenv\n",
    "!uv pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import AzureOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# .envファイルをロードして環境変数を読み込む\n",
    "load_dotenv()\n",
    "\n",
    "# 環境変数から値を取得\n",
    "azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
    "\n",
    "# 必須の環境変数が欠けている場合エラーをスロー\n",
    "if not azure_endpoint or not api_key or not api_version:\n",
    "    raise ValueError(\"必須の環境変数の値が取得できていません。環境変数を確認してください。\")\n",
    "\n",
    "# Azure OpenAI Clientの初期化\n",
    "client = AzureOpenAI(\n",
    "    azure_endpoint=azure_endpoint,\n",
    "    api_key=api_key,\n",
    "    api_version=api_version\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "「'The movie was full of'」に続くトークン上位10個と、その確率（尤度）を求めよ。\n",
      "ただし、言語モデルへのプロンプトがどのようなトークン列に変換されたか、確認せよ。\n",
      "\n",
      "--- プロンプトのトークン化結果 ---\n",
      "元のプロンプト: 'The movie was full of'\n",
      "トークンID列: [976, 8249, 673, 3149, 328]\n",
      "トークンテキスト列: ['The', ' movie', ' was', ' full', ' of']\n",
      "トークン数: 5\n",
      "\n",
      "--- 続くトークン候補と確率 ---\n",
      "API呼び出しで実際に生成された最初のトークン (max_tokens=1 の結果): 'The'\n",
      "\n",
      "'The movie was full of' に続く位置でのトークン候補上位10件とその確率:\n",
      "  'The': 0.17045\n",
      "  'It': 0.00539\n",
      "  'Could': 0.00303\n",
      "  'intr': 0.00096\n",
      "  'st': 0.00054\n",
      "  'em': 0.0003\n",
      "  'unexpected': 0.0003\n",
      "  'Can': 0.00017\n",
      "  'v': 0.0001\n",
      "  'spect': 0.0001\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "import tiktoken # tiktoken ライブラリをインポート\n",
    "\n",
    "def fetch_top_tokens_and_probabilities(payload, system_message=\"あなたは優秀な映画の専門家です。\"):\n",
    "    \"\"\"\n",
    "    Azure OpenAIのモデルに対してプロンプトに続くトークンとその確率を取得します。\n",
    "\n",
    "    Parameters:\n",
    "        - payload (dict): モデル名、プロンプト、温度、top_kを含む辞書。\n",
    "        - system_message (str): システムメッセージ（デフォルト値あり）。\n",
    "\n",
    "    Returns:\n",
    "        tuple: (APIが実際に生成した最初のトークン文字列, 上位トークンとその確率のリスト)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # API呼び出し: 次のトークン予測\n",
    "        # logprobs=True に加えて、top_logprobs パラメータを追加\n",
    "        response = client.chat.completions.create(\n",
    "            model=payload[\"model\"],\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_message},\n",
    "                {\"role\": \"user\", \"content\": payload[\"prompt\"]}\n",
    "            ],\n",
    "            temperature=payload[\"temperature\"],\n",
    "            max_tokens=1, # 続く「一つの」トークンを見るため max_tokens=1\n",
    "            logprobs=True,\n",
    "            top_logprobs=payload[\"top_k\"] # 返却する上位トークン数を指定\n",
    "        )\n",
    "\n",
    "        # print(f\"APIレスポンス: {response}\") # デバッグが必要な場合のみ表示推奨\n",
    "\n",
    "        response_dict = response.model_dump()\n",
    "\n",
    "        # max_tokens=1 なので、生成された最初のトークンを取得\n",
    "        generated_token_text = response_dict[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "        # 上位トークンとその確率を取得\n",
    "        # logprobs -> content -> [0] (最初の出力トークン) -> top_logprobs にアクセス\n",
    "        # top_logprobs はリスト内の辞書です: [{'token': ' abc', 'logprob': -0.1, 'bytes': [...]}, ...]\n",
    "        # content リストが空でないかチェック（max_tokens=1なので通常は1つの要素があるはず）\n",
    "        if not response_dict[\"choices\"][0].get(\"logprobs\") or not response_dict[\"choices\"][0][\"logprobs\"].get(\"content\"):\n",
    "             print(\"警告: APIレスポンスにlogprobs情報が含まれていませんでした。\")\n",
    "             return generated_token_text, []\n",
    "\n",
    "        # 最初の生成トークン位置での上位候補トークン情報を取得\n",
    "        top_logprobs_list_of_dicts = response_dict[\"choices\"][0][\"logprobs\"][\"content\"][0][\"top_logprobs\"]\n",
    "\n",
    "        if not top_logprobs_list_of_dicts:\n",
    "             print(\"警告: APIから上位トークン情報が返されませんでした。top_logprobs=0 の可能性があります。\")\n",
    "             return generated_token_text, []\n",
    "\n",
    "\n",
    "        top_tokens_and_probabilities = [\n",
    "            (token_info[\"token\"], round(pow(10, token_info[\"logprob\"]), 5))\n",
    "            for token_info in top_logprobs_list_of_dicts\n",
    "        ]\n",
    "\n",
    "        # APIが top_logprobs で指定した数のトークンを確率付きで返しているので、ソートして返します\n",
    "        return generated_token_text, sorted(top_tokens_and_probabilities, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"API呼び出し中にエラーが発生しました: {e}\")\n",
    "        # エラー発生時は生成されたトークンも確率リストも取得できない\n",
    "        return None, None\n",
    "\n",
    "payload = {\n",
    "    \"model\": \"gpt-4o\",  # あなたのAzure OpenAIでデプロイしたモデルのデプロイメント名を指定\n",
    "    # \"prompt\": \"The movie was full of\",\n",
    "    \"prompt\": \"The movie was full of\",\n",
    "    \"temperature\": 0.7, # Temperature affects sampling of the *generated* token, but logprobs shows potential options regardless.\n",
    "    \"top_k\": 10 # APIに返すように要求する上位トークンの数\n",
    "}\n",
    "\n",
    "print(f\"「'{payload['prompt']}'」に続くトークン上位{payload['top_k']}個と、その確率（尤度）を求めよ。\")\n",
    "print(f\"ただし、言語モデルへのプロンプトがどのようなトークン列に変換されたか、確認せよ。\")\n",
    "\n",
    "# --- プロンプトのトークン化を表示 ---\n",
    "print(\"\\n--- プロンプトのトークン化結果 ---\")\n",
    "try:\n",
    "    # モデル名に対応するエンコーディングを取得\n",
    "    encoding = tiktoken.encoding_for_model(payload[\"model\"])\n",
    "\n",
    "    # プロンプトをトークンIDのリストにエンコード\n",
    "    input_token_ids = encoding.encode(payload[\"prompt\"])\n",
    "\n",
    "    # 各トークンIDを元のテキストに戻す（デコード）\n",
    "    # 各IDごとにデコードすると、元の区切り（スペースなど）を維持しやすい\n",
    "    input_tokens_text = [encoding.decode([token_id]) for token_id in input_token_ids]\n",
    "\n",
    "    print(f\"元のプロンプト: '{payload['prompt']}'\")\n",
    "    print(f\"トークンID列: {input_token_ids}\")\n",
    "    print(f\"トークンテキスト列: {input_tokens_text}\")\n",
    "    print(f\"トークン数: {len(input_token_ids)}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"プロンプトのトークン化中にエラーが発生しました: {e}\")\n",
    "    print(\"tiktoken がインストールされているか確認してください (`pip install tiktoken`)。\")\n",
    "\n",
    "\n",
    "# --- 続くトークンの確率を取得 ---\n",
    "print(\"\\n--- 続くトークン候補と確率 ---\")\n",
    "# APIを呼び出して続くトークンの確率を取得\n",
    "generated_token, top_tokens_and_probabilities = fetch_top_tokens_and_probabilities(payload)\n",
    "\n",
    "# 結果を表示\n",
    "if generated_token is not None and top_tokens_and_probabilities is not None:\n",
    "    # API呼び出しによって実際に生成された最初のトークン\n",
    "    # これは logprobs リストに含まれる可能性が高いですが、サンプリングによる結果です。\n",
    "    print(f\"API呼び出しで実際に生成された最初のトークン (max_tokens=1 の結果): '{generated_token}'\")\n",
    "\n",
    "    print(f\"\\n'{payload['prompt']}' に続く位置でのトークン候補上位{payload['top_k']}件とその確率:\")\n",
    "    if top_tokens_and_probabilities:\n",
    "        # 確率の高い順に表示\n",
    "        for token, probability in top_tokens_and_probabilities:\n",
    "            print(f\"  '{token}': {probability}\") # トークンを引用符で囲むと見やすい\n",
    "    else:\n",
    "        print(\"上位トークン候補情報は取得できませんでした。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "s1RhOldA0meh",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 91. 続きのテキストの予測\n",
    "\n",
    "“The movie was full of\"に続くテキストを複数予測せよ。このとき、デコーディングの方法や温度パラメータ（temperature）を変えながら、予測される複数のテキストの変化を観察せよ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 課題 ===\n",
      "「'The movie was full of'」に続くテキストを複数予測し、デコーディング方法（温度、top_p）による変化を観察する。\n",
      "モデル: gpt-4o\n",
      "生成最大トークン数: 100\n",
      "============\n",
      "\n",
      "=== テキスト補完の生成と観察 ===\n",
      "\n",
      "--- 設定: 温度=0.0 (決定論的サンプリング) ---\n",
      "  補完 1:\n",
      "    The movie was full of **unexpected twists**, **emotional depth**, and\n",
      "    **stunning visuals** that kept the audience engaged from start to finish.\n",
      "    Whether it was the gripping storyline, the powerful performances, or the\n",
      "    breathtaking cinematography, it delivered an unforgettable experience.\n",
      "  補完 2:\n",
      "    The movie was full of **unexpected twists**, **emotional depth**, and\n",
      "    **stunning visuals** that kept the audience engaged from start to finish.\n",
      "    Whether it was the gripping storyline, the powerful performances, or the\n",
      "    breathtaking cinematography, it delivered an unforgettable experience.\n",
      "  補完 3:\n",
      "    The movie was full of **unexpected twists**, **emotional depth**, and\n",
      "    **stunning visuals** that kept the audience engaged from start to finish.\n",
      "    Whether it was the gripping storyline, the powerful performances, or the\n",
      "    breathtaking cinematography, it delivered an unforgettable experience.\n",
      "\n",
      "--- 設定: 温度=0.7, top_p=0.1 ---\n",
      "  補完 1:\n",
      "    The movie was full of **unexpected twists** and **emotional depth**, keeping\n",
      "    the audience engaged from start to finish. It balanced moments of intense\n",
      "    drama with lighter, humorous scenes, creating a dynamic and memorable\n",
      "    experience. The cinematography and soundtrack added layers of richness,\n",
      "    making it visually and emotionally captivating.\n",
      "  補完 2:\n",
      "    The movie was full of **unexpected twists** and **emotional depth**, keeping\n",
      "    the audience engaged from start to finish. It balanced moments of intense\n",
      "    drama with lighter, humorous scenes, creating a dynamic and memorable\n",
      "    experience. The cinematography and soundtrack added layers of richness,\n",
      "    making it visually and emotionally captivating.\n",
      "  補完 3:\n",
      "    The movie was full of **unexpected twists** and **emotional depth**, keeping\n",
      "    the audience engaged from start to finish. It balanced moments of intense\n",
      "    drama with lighter, humorous scenes, creating a dynamic and memorable\n",
      "    experience. The cinematography and soundtrack added layers of richness,\n",
      "    making it visually and emotionally captivating.\n",
      "\n",
      "--- 設定: 温度=0.7, top_p=0.5 ---\n",
      "  補完 1:\n",
      "    unexpected twists and emotional depth, leaving the audience captivated from\n",
      "    start to finish.\n",
      "  補完 2:\n",
      "    The movie was full of **unexpected twists**, **emotional depth**, and\n",
      "    **stunning visuals** that kept the audience engaged from start to finish.\n",
      "    Whether it was the gripping storyline, the well-developed characters, or the\n",
      "    thrilling action sequences, it delivered a memorable cinematic experience.\n",
      "  補完 3:\n",
      "    unexpected twists and emotional depth, leaving the audience captivated from\n",
      "    start to finish.\n",
      "\n",
      "--- 設定: 温度=0.7, top_p=1.0 ---\n",
      "  補完 1:\n",
      "    unexpected twists and turns, leaving the audience on the edge of their\n",
      "    seats. Its compelling storyline, emotional depth, and stunning visuals made\n",
      "    it a truly unforgettable experience.\n",
      "  補完 2:\n",
      "    suspense and unexpected twists. It kept the audience on the edge of their\n",
      "    seats, delivering emotional highs and shocking revelations. The characters\n",
      "    were complex, and the plot was layered with moments that made you question\n",
      "    everything. Truly, it was an unforgettable cinematic experience!\n",
      "  補完 3:\n",
      "    The movie was full of **suspense**, **action**, and **unexpected twists**!\n",
      "    It kept you on the edge of your seat, delivering emotional moments and jaw-\n",
      "    dropping scenes. Would you like to discuss the plot, characters, or any\n",
      "    specific part that stood out to you?\n",
      "\n",
      "--- 設定: 温度=1.0, top_p=0.1 ---\n",
      "  補完 1:\n",
      "    The movie was full of **unexpected twists**, **emotional depth**, and\n",
      "    **stunning visuals** that kept the audience engaged from start to finish.\n",
      "    Whether it was the gripping storyline, the powerful performances, or the\n",
      "    breathtaking cinematography, it delivered an unforgettable experience.\n",
      "  補完 2:\n",
      "    The movie was full of **unexpected twists**, **emotional depth**, and\n",
      "    **stunning visuals** that kept the audience engaged from start to finish.\n",
      "    Whether it was the gripping storyline, the powerful performances, or the\n",
      "    breathtaking cinematography, it delivered an unforgettable experience.\n",
      "  補完 3:\n",
      "    The movie was full of **unexpected twists**, **emotional depth**, and\n",
      "    **stunning visuals** that kept the audience engaged from start to finish.\n",
      "    Whether it was the gripping storyline, the powerful performances, or the\n",
      "    breathtaking cinematography, it delivered an unforgettable experience.\n",
      "\n",
      "--- 設定: 温度=1.0, top_p=0.5 ---\n",
      "  補完 1:\n",
      "    The movie was full of **unexpected twists and turns**, keeping the audience\n",
      "    on the edge of their seats. It had a perfect blend of **emotion, suspense,\n",
      "    and action**, with moments that were both heartwarming and shocking. The\n",
      "    characters were well-developed, and the cinematography added depth to the\n",
      "    storytelling. Whether it was the gripping dialogue, stunning visuals, or the\n",
      "    intense climax, the film delivered an unforgettable experience!\n",
      "  補完 2:\n",
      "    unexpected twists and turns, keeping the audience on the edge of their\n",
      "    seats. The plot was rich with suspense, emotional depth, and moments of\n",
      "    sheer brilliance that left viewers captivated until the very end.\n",
      "  補完 3:\n",
      "    The movie was full of **twists and turns**, keeping the audience on the edge\n",
      "    of their seats. It was packed with **emotional depth**, **action-packed\n",
      "    sequences**, and **stunning visuals** that left a lasting impression.\n",
      "    Whether it was the gripping storyline, the unexpected revelations, or the\n",
      "    heartfelt performances, the film delivered a memorable experience.\n",
      "\n",
      "--- 設定: 温度=1.0, top_p=1.0 ---\n",
      "  補完 1:\n",
      "    The movie was full of suspense, captivating moments, and unexpected twists\n",
      "    that kept the audience on the edge of their seats. It also delivered\n",
      "    powerful performances and stunning visuals, complementing its gripping\n",
      "    storyline.\n",
      "  補完 2:\n",
      "    The movie was full of suspense, drama, and unexpected twists—keeping the\n",
      "    audience hooked from start to finish! Would you like to discuss a specific\n",
      "    part or theme of the film?\n",
      "  補完 3:\n",
      "    The movie was full of **unexpected twists** and **heartfelt moments**,\n",
      "    weaving its story with **suspense**, **raw emotion**, and even splashes of\n",
      "    **humor** to keep the audience on edge. Would you like a deeper dive into\n",
      "    its themes, characters, or plot?\n",
      "\n",
      "--- 設定: 温度=1.5, top_p=0.1 ---\n",
      "  補完 1:\n",
      "    unexpected twists and turns, keeping the audience on the edge of their\n",
      "    seats. The plot was layered with emotional depth, gripping suspense, and\n",
      "    moments of sheer brilliance that left viewers captivated until the very end.\n",
      "  補完 2:\n",
      "    unexpected twists and turns, keeping the audience on the edge of their\n",
      "    seats. The plot was layered with emotional depth, gripping suspense, and\n",
      "    moments of sheer brilliance that left viewers captivated until the very end.\n",
      "  補完 3:\n",
      "    unexpected twists and turns, keeping the audience on the edge of their\n",
      "    seats. The plot was layered with emotional depth, gripping suspense, and\n",
      "    moments of sheer brilliance that left viewers captivated until the very end.\n",
      "\n",
      "--- 設定: 温度=1.5, top_p=0.5 ---\n",
      "  補完 1:\n",
      "    The movie was full of **suspense**, **action**, and **unexpected twists**\n",
      "    that kept the audience on the edge of their seats. It also delivered moments\n",
      "    of **emotional depth** and **stunning visuals**, making it a memorable\n",
      "    experience.\n",
      "  補完 2:\n",
      "    The movie was full of **suspense and unexpected twists**, keeping the\n",
      "    audience on the edge of their seats. It masterfully blended emotional depth\n",
      "    with thrilling action, leaving viewers captivated until the very end.\n",
      "  補完 3:\n",
      "    The movie was full of suspense, drama, and unexpected twists that kept the\n",
      "    audience on the edge of their seats. It had moments of intense emotion,\n",
      "    stunning visuals, and a storyline that left viewers deeply engaged. What\n",
      "    kind of movie are you referring to?\n",
      "\n",
      "--- 設定: 温度=1.5, top_p=1.0 ---\n",
      "  補完 1:\n",
      "    tension and unexpected twists! It kept me on the edge of my seat until the\n",
      "    very end. Would you like a more detailed breakdown or a discussion about key\n",
      "    moments in the movie? Let me know!\n",
      "  補完 2:\n",
      "    The movie was full of *heart-stopping action*, *unexpected twists*, and\n",
      "    *emotional depth*. It captured the audience’s attention and left them on an\n",
      "    unforgettable rollercoaster of suspense, laughter, and tears.\n",
      "  補完 3:\n",
      "    chilling suspense, unexpected twists, and profound emotional moments that\n",
      "    kept the audience on edge from start to finish. It masterfully blended\n",
      "    strong performances, striking visuals, and a gripping storyline to make the\n",
      "    experience truly memorable. Would you like me to break this down or dive\n",
      "    into other aspects of the movie?\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "import tiktoken\n",
    "import os # 環境変数からAPIキーなどを読み込む場合に使用\n",
    "\n",
    "# --- テキスト補完生成関数 ---\n",
    "def generate_text_completions(client, model, prompt, max_tokens, temperature, top_p, n):\n",
    "    \"\"\"\n",
    "    指定されたパラメータでAzure OpenAIモデルから複数のテキスト補完を生成します。\n",
    "\n",
    "    Parameters:\n",
    "        - client: AzureOpenAI クライアントインスタンス\n",
    "        - model (str): モデル名\n",
    "        - prompt (str): 入力プロンプト\n",
    "        - max_tokens (int): 生成する最大トークン数 (1以上)\n",
    "        - temperature (float): サンプリング温度 (0.0-2.0)。0で決定論的。\n",
    "        - top_p (float): top_p サンプリング確率 (0.0-1.0)。1.0でtemperatureサンプリングに近い。\n",
    "        - n (int): 生成する補完の数 (1以上)\n",
    "\n",
    "    Returns:\n",
    "        list: 生成されたテキスト文字列のリスト。エラー発生時はエラーメッセージを含むリスト。\n",
    "    \"\"\"\n",
    "    if client is None:\n",
    "        return [\"Error: Azure OpenAI client is not initialized.\"] * n\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            n=n, # 複数の補完を生成\n",
    "            # logprobs, top_logprobs はここでは使用しない\n",
    "        )\n",
    "\n",
    "        # 生成されたテキストをリストとして抽出\n",
    "        completions = [choice.message.content for choice in response.choices]\n",
    "\n",
    "        return completions\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"API呼び出し中にエラーが発生しました (温度={temperature}, top_p={top_p}): {e}\")\n",
    "        return [f\"Error generating completion: {e}\"] * n # エラー時はエラーメッセージを含むリストを返す\n",
    "\n",
    "# --- 主処理 ---\n",
    "\n",
    "model_name = \"gpt-4o\"\n",
    "prompt_text = \"The movie was full of\"\n",
    "generate_max_tokens = 100 # 生成するテキストの最大長（トークン単位）。必要に応じて調整。\n",
    "\n",
    "# 試行する温度とtop_pの値のリスト\n",
    "# 温度 (Temperature): 高いほどランダム性が増す (多様だが脱線も)。低いほど決定論的 (毎回同じ傾向)。\n",
    "temperatures_to_test = [0.0, 0.7, 1.0, 1.5]\n",
    "# top_p (Nucleus Sampling): 累積確率が top_p になるまで確率の高いトークン候補を選び、その中からサンプリング。\n",
    "# 低いほど候補が絞られ予測可能に。高いほど候補が増え多様に (1.0 は temperature サンプリングに近い)。\n",
    "top_p_to_test = [0.1, 0.5, 1.0]\n",
    "\n",
    "# 各設定で生成するテキストの数\n",
    "num_completions_per_setting = 3 # 各温度・top_pの組み合わせで3つの補完を生成\n",
    "\n",
    "print(f\"=== 課題 ===\")\n",
    "print(f\"「'{prompt_text}'」に続くテキストを複数予測し、デコーディング方法（温度、top_p）による変化を観察する。\")\n",
    "print(f\"モデル: {model_name}\")\n",
    "print(f\"生成最大トークン数: {generate_max_tokens}\")\n",
    "print(\"============\")\n",
    "\n",
    "print(\"\\n=== テキスト補完の生成と観察 ===\")\n",
    "\n",
    "# 温度とtop_pの組み合わせごとに補完を生成\n",
    "for temp in temperatures_to_test:\n",
    "    for p in top_p_to_test:\n",
    "        # temperature=0.0 の場合、top_p は通常無視され決定論的なサンプリングになります。\n",
    "        # 重複を避けるため、temperature=0.0 の場合は top_p=1.0 の設定のみ実行します。\n",
    "        if temp == 0.0 and p != 1.0:\n",
    "            continue # 他の top_p はスキップ\n",
    "\n",
    "        # 設定の表示\n",
    "        if temp == 0.0:\n",
    "             print(f\"\\n--- 設定: 温度={temp} (決定論的サンプリング) ---\")\n",
    "        else:\n",
    "             print(f\"\\n--- 設定: 温度={temp}, top_p={p} ---\")\n",
    "\n",
    "        # テキスト補完を生成\n",
    "        completions = generate_text_completions(\n",
    "            client, # クライアントオブジェクトを渡す\n",
    "            model_name,\n",
    "            prompt_text,\n",
    "            generate_max_tokens,\n",
    "            temp,\n",
    "            p,\n",
    "            num_completions_per_setting\n",
    "        )\n",
    "\n",
    "        # 生成された補完を表示\n",
    "        for i, text in enumerate(completions):\n",
    "            print(f\"  補完 {i+1}:\")\n",
    "            # テキストを整形して表示\n",
    "            print(textwrap.fill(text, width=80, initial_indent='    ', subsequent_indent='    '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "7ZFadg6B8VdA",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 92. 予測されたテキストの確率を計算\n",
    "\n",
    "“The movie was full of\"に続くテキストを予測し、生成された各単語の尤度を表示せよ（生成されるテキストが長いと出力が読みにくくなるので、適当な長さで生成を打ち切るとよい）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 課題 ===\n",
      "「'The movie was full of'」に続くテキストを予測し、生成された各トークンの尤度を表示する。\n",
      "モデル: gpt-4o\n",
      "生成最大トークン数: 30\n",
      "============\n",
      "\n",
      "--- 元のプロンプトのトークン化結果 ---\n",
      "元のプロンプト: 'The movie was full of'\n",
      "トークンID列: [976, 8249, 673, 3149, 328]\n",
      "トークンテキスト列: ['The', ' movie', ' was', ' full', ' of']\n",
      "トークン数: 5\n",
      "\n",
      "=== 生成されたテキストとそのトークン確率 ===\n",
      "\n",
      "生成されたテキスト (最大30トークン):\n",
      "    The movie was full of **unexpected twists**, **emotional depth**, and\n",
      "    **stunning visuals** that kept the audience engaged from start to finish.\n",
      "\n",
      "各生成トークンとその確率:\n",
      "  トークン: 'The'          , 確率: 0.29954\n",
      "  トークン: ' movie'       , 確率: 0.99663\n",
      "  トークン: ' was'         , 確率: 0.99999\n",
      "  トークン: ' full'        , 確率: 1.0\n",
      "  トークン: ' of'          , 確率: 1.0\n",
      "  トークン: ' **'          , 確率: 0.54964\n",
      "  トークン: 'unexpected'   , 確率: 0.51165\n",
      "  トークン: ' twists'      , 確率: 0.99525\n",
      "  トークン: '**,'          , 確率: 0.36773\n",
      "  トークン: ' **'          , 確率: 0.96097\n",
      "  トークン: 'em'           , 確率: 0.56557\n",
      "  トークン: 'otional'      , 確率: 0.99946\n",
      "  トークン: ' depth'       , 確率: 0.85965\n",
      "  トークン: '**,'          , 確率: 0.99997\n",
      "  トークン: ' and'         , 確率: 0.97596\n",
      "  トークン: ' **'          , 確率: 0.99475\n",
      "  トークン: 'st'           , 確率: 0.49993\n",
      "  トークン: 'unning'       , 確率: 0.99998\n",
      "  トークン: ' visuals'     , 確率: 0.98943\n",
      "  トークン: '**'           , 確率: 0.87373\n",
      "  トークン: ' that'        , 確率: 0.58993\n",
      "  トークン: ' kept'        , 確率: 0.95833\n",
      "  トークン: ' the'         , 確率: 0.84681\n",
      "  トークン: ' audience'    , 確率: 0.9999\n",
      "  トークン: ' engaged'     , 確率: 0.51036\n",
      "  トークン: ' from'        , 確率: 0.86594\n",
      "  トークン: ' start'       , 確率: 0.9577\n",
      "  トークン: ' to'          , 確率: 1.0\n",
      "  トークン: ' finish'      , 確率: 0.99998\n",
      "  トークン: '.'            , 確率: 0.96641\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "import tiktoken\n",
    "import os\n",
    "import math # logprob（対数確率）を通常の確率に変換するためにmathモジュールを使用\n",
    "\n",
    "# --- テキスト生成と確率取得関数 ---\n",
    "def generate_text_with_probabilities(client, model, prompt, max_tokens):\n",
    "    \"\"\"\n",
    "    プロンプトに続くテキストを生成し、各生成トークンの確率を返します。\n",
    "\n",
    "    Parameters:\n",
    "        - client: 初期化済みの AzureOpenAI クライアントインスタンス\n",
    "        - model (str): Azureでデプロイしたモデルのデプロイメント名\n",
    "        - prompt (str): 入力プロンプト\n",
    "        - max_tokens (int): 生成する最大トークン数 (1以上)\n",
    "\n",
    "    Returns:\n",
    "        tuple: (生成されたテキスト文字列, 各トークンと確率のリスト)。\n",
    "               エラー発生時は (None, None)。\n",
    "    \"\"\"\n",
    "    if client is None:\n",
    "        print(\"エラー: OpenAI client is not initialized. Cannot make API call.\")\n",
    "        return None, None\n",
    "\n",
    "    try:\n",
    "        # API呼び出し: テキスト生成とトークン確率の取得\n",
    "        # temperature は 0.0 (決定論的) を推奨 - 最も尤もらしいシーケンスが生成される\n",
    "        # n=1 で単一の補完を取得\n",
    "        # logprobs=True で生成トークンそれぞれの確率を要求\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=0.0, # 確率を観察しやすくするため決定論的にする\n",
    "            n=1,             # 1つの補完を取得\n",
    "            logprobs=True    # 生成トークンの確率情報を取得\n",
    "            # top_p, top_logprobs は今回は使用しない\n",
    "        )\n",
    "\n",
    "        # print(f\"APIレスポンス: {response}\") # 必要に応じてデバッグ用にコメント解除\n",
    "\n",
    "        response_dict = response.model_dump()\n",
    "\n",
    "        # 生成されたテキスト全体を取得 (choices[0] は n=1 なので最初の要素)\n",
    "        generated_text = response_dict[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "        # logprobs 情報があるか確認\n",
    "        # logprobs=True にしても、生成テキストがない場合など content が None になる可能性あり\n",
    "        logprobs_info = response_dict[\"choices\"][0].get(\"logprobs\")\n",
    "        if not logprobs_info or not logprobs_info.get(\"content\"):\n",
    "             print(\"警告: APIレスポンスにlogprobs情報が含まれていませんでした。\")\n",
    "             # テキスト自体は返せたかもしれないが、確率は不明\n",
    "             return generated_text, []\n",
    "\n",
    "        # 生成された各トークンとそのlogprobを取得\n",
    "        token_logprob_list = logprobs_info[\"content\"]\n",
    "\n",
    "        # 各トークンと確率のリストを作成\n",
    "        token_probability_list = []\n",
    "        for token_info in token_logprob_list:\n",
    "             token_text = token_info[\"token\"]\n",
    "             logprob = token_info[\"logprob\"]\n",
    "             # logprob (対数確率) を通常の確率に変換\n",
    "             # OpenAI API の logprob は自然対数 (底 e) です\n",
    "             probability = math.exp(logprob)\n",
    "             # 確率を小数点以下5桁で丸める\n",
    "             token_probability_list.append((token_text, round(probability, 5)))\n",
    "\n",
    "        return generated_text, token_probability_list\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"API呼び出し中にエラーが発生しました: {e}\")\n",
    "        # エラー発生時はテキストも確率も取得できていない\n",
    "        return None, None\n",
    "\n",
    "# --- 主処理 ---\n",
    "\n",
    "model_name = \"gpt-4o\" # あなたのAzure OpenAIでデプロイしたモデルのデプロイメント名を指定\n",
    "prompt_text = \"The movie was full of\"\n",
    "generate_max_tokens_limit = 30 # 生成を打ち切る最大トークン数（適当な長さ）\n",
    "\n",
    "print(f\"=== 課題 ===\")\n",
    "print(f\"「'{prompt_text}'」に続くテキストを予測し、生成された各トークンの尤度を表示する。\")\n",
    "print(f\"モデル: {model_name}\")\n",
    "print(f\"生成最大トークン数: {generate_max_tokens_limit}\")\n",
    "print(\"============\")\n",
    "\n",
    "# --- 元のプロンプトのトークン化を表示 (参考情報として維持) ---\n",
    "print(\"\\n--- 元のプロンプトのトークン化結果 ---\")\n",
    "# tiktoken は API クライアント初期化の成功とは独立して動作\n",
    "# ただし、モデル名がtiktokenでサポートされている必要がある\n",
    "try:\n",
    "    encoding = tiktoken.encoding_for_model(model_name)\n",
    "    input_token_ids = encoding.encode(prompt_text)\n",
    "    input_tokens_text = [encoding.decode([token_id]) for token_id in input_token_ids]\n",
    "\n",
    "    print(f\"元のプロンプト: '{prompt_text}'\")\n",
    "    print(f\"トークンID列: {input_token_ids}\")\n",
    "    print(f\"トークンテキスト列: {input_tokens_text}\")\n",
    "    print(f\"トークン数: {len(input_token_ids)}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"プロンプトのトークン化中にエラーが発生しました: {e}\")\n",
    "    print(\"モデル名が正しいか、tiktoken がインストールされているか確認してください (`pip install tiktoken`)。\")\n",
    "\n",
    "\n",
    "print(\"\\n=== 生成されたテキストとそのトークン確率 ===\")\n",
    "\n",
    "# テキストを生成し、各トークンの確率を取得\n",
    "# generate_text_with_probabilities 関数内でクライアント初期化の成功をチェック\n",
    "generated_text, token_probabilities = generate_text_with_probabilities(\n",
    "    client, # 初期化されたクライアントを渡す\n",
    "    model_name,\n",
    "    prompt_text,\n",
    "    generate_max_tokens_limit\n",
    ")\n",
    "\n",
    "# 結果を表示\n",
    "if generated_text is not None:\n",
    "    print(f\"\\n生成されたテキスト (最大{generate_max_tokens_limit}トークン):\")\n",
    "    # 生成テキスト全体を表示\n",
    "    print(textwrap.fill(generated_text, width=80, initial_indent='    ', subsequent_indent='    '))\n",
    "\n",
    "    if token_probabilities:\n",
    "        print(\"\\n各生成トークンとその確率:\")\n",
    "        # 各トークンと確率を表示\n",
    "        # 見やすくするために、トークンを repr() で囲み、確率を調整して表示\n",
    "        # repr() を使うと '\\n', '\\t', ' ' (スペース) など、トークンの厳密な内容が見える\n",
    "        for token, probability in token_probabilities:\n",
    "             display_token = repr(token)\n",
    "             # 表示を揃えるために幅を指定 (必要に応じて調整)\n",
    "             print(f\"  トークン: {display_token:<15}, 確率: {probability}\")\n",
    "    else:\n",
    "        print(\"\\nトークン確率情報は取得できませんでした。（API呼び出しエラーまたはlogprobs情報なし）\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "FvNCTMj6OegF",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 93. パープレキシティ\n",
    "\n",
    "適当な文を準備して、事前学習済み言語モデルでパープレキシティを測定せよ。例えば、\n",
    "\n",
    "+ The movie was full of surprises\n",
    "+ The movies were full of surprises\n",
    "+ The movie were full of surprises\n",
    "+ The movies was full of surprises\n",
    "\n",
    "の4文に対して、パープレキシティを測定して観察せよ（最後の2つの文は故意に文法的な間違いを入れた）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 課題 ===\n",
      "与えられた文に対するパープレキシティを測定し、比較する。\n",
      "モデル: gpt-4o\n",
      "logprob取得のための上位K (API制限): 20\n",
      "API遅延: 0.01秒\n",
      "============\n",
      "\n",
      "=== パープレキシティ測定結果 ===\n",
      "  'The movie was full of surprises' (6トークン) のパープレキシティを計算中...\n",
      "  警告: 対象トークン ' movie' (' movie') が上位 20 候補に見つかりませんでした (プロンプト: 'The')。代用logprob (-20.0) を使用します。\n",
      "  警告: 対象トークン ' was' (' was') が上位 20 候補に見つかりませんでした (プロンプト: 'The movie')。代用logprob (-20.0) を使用します。\n",
      "  警告: 対象トークン ' full' (' full') が上位 20 候補に見つかりませんでした (プロンプト: 'The movie was')。代用logprob (-20.0) を使用します。\n",
      "  警告: 対象トークン ' of' (' of') が上位 20 候補に見つかりませんでした (プロンプト: 'The movie was full')。代用logprob (-20.0) を使用します。\n",
      "  警告: 対象トークン ' surprises' (' surprises') が上位 20 候補に見つかりませんでした (プロンプト: 'The movie was full of')。代用logprob (-20.0) を使用します。\n",
      "文: 'The movie was full of surprises'\n",
      "パープレキシティ: 485165195.4098\n",
      "------------------------------\n",
      "  'The movies were full of surprises' (6トークン) のパープレキシティを計算中...\n",
      "  警告: 対象トークン ' movies' (' movies') が上位 20 候補に見つかりませんでした (プロンプト: 'The')。代用logprob (-20.0) を使用します。\n",
      "  警告: 対象トークン ' were' (' were') が上位 20 候補に見つかりませんでした (プロンプト: 'The movies')。代用logprob (-20.0) を使用します。\n",
      "  警告: 対象トークン ' full' (' full') が上位 20 候補に見つかりませんでした (プロンプト: 'The movies were')。代用logprob (-20.0) を使用します。\n",
      "  警告: 対象トークン ' of' (' of') が上位 20 候補に見つかりませんでした (プロンプト: 'The movies were full')。代用logprob (-20.0) を使用します。\n",
      "  警告: 対象トークン ' surprises' (' surprises') が上位 20 候補に見つかりませんでした (プロンプト: 'The movies were full of')。代用logprob (-20.0) を使用します。\n",
      "文: 'The movies were full of surprises'\n",
      "パープレキシティ: 485165195.4098\n",
      "------------------------------\n",
      "  'The movie were full of surprises' (6トークン) のパープレキシティを計算中...\n",
      "  警告: 対象トークン ' movie' (' movie') が上位 20 候補に見つかりませんでした (プロンプト: 'The')。代用logprob (-20.0) を使用します。\n",
      "  警告: 対象トークン ' were' (' were') が上位 20 候補に見つかりませんでした (プロンプト: 'The movie')。代用logprob (-20.0) を使用します。\n",
      "  警告: 対象トークン ' full' (' full') が上位 20 候補に見つかりませんでした (プロンプト: 'The movie were')。代用logprob (-20.0) を使用します。\n",
      "  警告: 対象トークン ' of' (' of') が上位 20 候補に見つかりませんでした (プロンプト: 'The movie were full')。代用logprob (-20.0) を使用します。\n",
      "  警告: 対象トークン ' surprises' (' surprises') が上位 20 候補に見つかりませんでした (プロンプト: 'The movie were full of')。代用logprob (-20.0) を使用します。\n",
      "文: 'The movie were full of surprises'\n",
      "パープレキシティ: 485165195.4098\n",
      "------------------------------\n",
      "  'The movies was full of surprises' (6トークン) のパープレキシティを計算中...\n",
      "  警告: 対象トークン ' movies' (' movies') が上位 20 候補に見つかりませんでした (プロンプト: 'The')。代用logprob (-20.0) を使用します。\n",
      "  警告: 対象トークン ' was' (' was') が上位 20 候補に見つかりませんでした (プロンプト: 'The movies')。代用logprob (-20.0) を使用します。\n",
      "  警告: 対象トークン ' full' (' full') が上位 20 候補に見つかりませんでした (プロンプト: 'The movies was')。代用logprob (-20.0) を使用します。\n",
      "  警告: 対象トークン ' of' (' of') が上位 20 候補に見つかりませんでした (プロンプト: 'The movies was full')。代用logprob (-20.0) を使用します。\n",
      "  警告: 対象トークン ' surprises' (' surprises') が上位 20 候補に見つかりませんでした (プロンプト: 'The movies was full of')。代用logprob (-20.0) を使用します。\n",
      "文: 'The movies was full of surprises'\n",
      "パープレキシティ: 485165195.4098\n",
      "------------------------------\n",
      "  'Surprises full of was movie The' (7トークン) のパープレキシティを計算中...\n",
      "  警告: 対象トークン 'prises' ('prises') が上位 20 候補に見つかりませんでした (プロンプト: 'Sur')。代用logprob (-20.0) を使用します。\n",
      "  警告: 対象トークン ' full' (' full') が上位 20 候補に見つかりませんでした (プロンプト: 'Surprises')。代用logprob (-20.0) を使用します。\n",
      "  警告: 対象トークン ' of' (' of') が上位 20 候補に見つかりませんでした (プロンプト: 'Surprises full')。代用logprob (-20.0) を使用します。\n",
      "  警告: 対象トークン ' was' (' was') が上位 20 候補に見つかりませんでした (プロンプト: 'Surprises full of')。代用logprob (-20.0) を使用します。\n",
      "  警告: 対象トークン ' movie' (' movie') が上位 20 候補に見つかりませんでした (プロンプト: 'Surprises full of was')。代用logprob (-20.0) を使用します。\n",
      "  警告: 対象トークン ' The' (' The') が上位 20 候補に見つかりませんでした (プロンプト: 'Surprises full of was movie')。代用logprob (-20.0) を使用します。\n",
      "文: 'Surprises full of was movie The'\n",
      "パープレキシティ: 485165195.4098\n",
      "------------------------------\n",
      "  'This is a very common sentence' (6トークン) のパープレキシティを計算中...\n",
      "  警告: 対象トークン ' is' (' is') が上位 20 候補に見つかりませんでした (プロンプト: 'This')。代用logprob (-20.0) を使用します。\n",
      "  警告: 対象トークン ' a' (' a') が上位 20 候補に見つかりませんでした (プロンプト: 'This is')。代用logprob (-20.0) を使用します。\n",
      "  警告: 対象トークン ' very' (' very') が上位 20 候補に見つかりませんでした (プロンプト: 'This is a')。代用logprob (-20.0) を使用します。\n",
      "  警告: 対象トークン ' common' (' common') が上位 20 候補に見つかりませんでした (プロンプト: 'This is a very')。代用logprob (-20.0) を使用します。\n",
      "  警告: 対象トークン ' sentence' (' sentence') が上位 20 候補に見つかりませんでした (プロンプト: 'This is a very common')。代用logprob (-20.0) を使用します。\n",
      "文: 'This is a very common sentence'\n",
      "パープレキシティ: 485165195.4098\n",
      "------------------------------\n",
      "  'This very is a common sentence' (6トークン) のパープレキシティを計算中...\n",
      "  警告: 対象トークン ' very' (' very') が上位 20 候補に見つかりませんでした (プロンプト: 'This')。代用logprob (-20.0) を使用します。\n",
      "  警告: 対象トークン ' is' (' is') が上位 20 候補に見つかりませんでした (プロンプト: 'This very')。代用logprob (-20.0) を使用します。\n",
      "  警告: 対象トークン ' a' (' a') が上位 20 候補に見つかりませんでした (プロンプト: 'This very is')。代用logprob (-20.0) を使用します。\n",
      "  警告: 対象トークン ' common' (' common') が上位 20 候補に見つかりませんでした (プロンプト: 'This very is a')。代用logprob (-20.0) を使用します。\n",
      "  警告: 対象トークン ' sentence' (' sentence') が上位 20 候補に見つかりませんでした (プロンプト: 'This very is a common')。代用logprob (-20.0) を使用します。\n",
      "文: 'This very is a common sentence'\n",
      "パープレキシティ: 485165195.4098\n",
      "------------------------------\n",
      "  'The quick brown fox jumps over the lazy dog' (9トークン) のパープレキシティを計算中...\n",
      "  警告: 対象トークン ' quick' (' quick') が上位 20 候補に見つかりませんでした (プロンプト: 'The')。代用logprob (-20.0) を使用します。\n",
      "  警告: 対象トークン ' brown' (' brown') が上位 20 候補に見つかりませんでした (プロンプト: 'The quick')。代用logprob (-20.0) を使用します。\n",
      "  警告: 対象トークン ' fox' (' fox') が上位 20 候補に見つかりませんでした (プロンプト: 'The quick brown')。代用logprob (-20.0) を使用します。\n",
      "  警告: 対象トークン ' jumps' (' jumps') が上位 20 候補に見つかりませんでした (プロンプト: 'The quick brown fox')。代用logprob (-20.0) を使用します。\n",
      "  警告: 対象トークン ' over' (' over') が上位 20 候補に見つかりませんでした (プロンプト: 'The quick brown fox jumps')。代用logprob (-20.0) を使用します。\n",
      "  警告: 対象トークン ' the' (' the') が上位 20 候補に見つかりませんでした (プロンプト: 'The quick brown fox jumps over')。代用logprob (-20.0) を使用します。\n",
      "  警告: 対象トークン ' dog' (' dog') が上位 20 候補に見つかりませんでした (プロンプト: 'The quick brown fox jumps over the lazy')。代用logprob (-20.0) を使用します。\n",
      "文: 'The quick brown fox jumps over the lazy dog'\n",
      "パープレキシティ: 203733994.0114\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "import tiktoken\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "\n",
    "\n",
    "# --- パープレキシティ計算関数 ---\n",
    "# top_k_for_logprobs のデフォルト値を 20 に修正\n",
    "def calculate_perplexity(client, model, sentence, top_k_for_logprobs=20, delay_seconds=0.05):\n",
    "    \"\"\"\n",
    "    指定された文のパープレキシティを計算します。\n",
    "    文中の各トークンについて、その前のトークン列をプロンプトとしてAPIを呼び出し、\n",
    "    当該トークンの確率を取得します。\n",
    "\n",
    "    Parameters:\n",
    "        - client: 初期化済みの AzureOpenAI クライアントインスタンス\n",
    "        - model (str): Azureでデプロイしたモデルのデプロイメント名\n",
    "        - sentence (str): パープレキシティを測定する文\n",
    "        - top_k_for_logprobs (int): APIで取得する上位トークン確率の数。\n",
    "                                   モデルの制限により最大値は通常20です。\n",
    "                                   対象トークンがこの中に含まれる必要があります。\n",
    "        - delay_seconds (float): API呼び出し間の待ち時間（秒）。レート制限対策。\n",
    "\n",
    "    Returns:\n",
    "        float: 計算されたパープレキシティ。計算できない場合は float('inf')。\n",
    "    \"\"\"\n",
    "    # APIの top_logprobs の制限値を確認し、指定値がそれを超えていないかチェック（念のため）\n",
    "    actual_top_k = min(top_k_for_logprobs, 20) # API制限を考慮\n",
    "    if top_k_for_logprobs > 20:\n",
    "         print(f\"  警告: 指定された top_k_for_logprobs ({top_k_for_logprobs}) はモデルの上限 (20) を超えています。実際には {actual_top_k} を使用します。\")\n",
    "\n",
    "\n",
    "    if client is None:\n",
    "        print(\"エラー: OpenAI client is not initialized. Cannot calculate perplexity.\")\n",
    "        return float('inf')\n",
    "\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "        # 文全体をトークンIDに変換\n",
    "        sentence_token_ids = encoding.encode(sentence)\n",
    "        num_tokens = len(sentence_token_ids)\n",
    "\n",
    "        # 1トークン以下の文はパープレキシティ計算が定義されない（あるいは1となる）\n",
    "        if num_tokens <= 1:\n",
    "            print(f\"警告: 文 '{sentence}' のトークン数が1以下です ({num_tokens}トークン)。パープレキシティは定義されません。\")\n",
    "            return 1.0 if num_tokens == 1 else float('inf') # 1トークンの場合は通常1、0トークンは無限大\n",
    "\n",
    "        total_logprob = 0.0\n",
    "        num_terms = 0 # logprobを合計できた項の数\n",
    "\n",
    "        print(f\"  '{sentence}' ({num_tokens}トークン) のパープレキシティを計算中...\")\n",
    "\n",
    "        # 文の2番目のトークンから最後までループ (w_i | w_1...w_{i-1}) を取得するため\n",
    "        # ループ変数 i は、sentence_token_ids リストにおける次のトークンのインデックス\n",
    "        # プロンプトは sentence_token_ids[:i] となり、次のトークンは sentence_token_ids[i]\n",
    "        # i=1 から num_tokens-1 まで回す (sentence_token_ids[1] から sentence_token_ids[num_tokens-1] まで)\n",
    "        for i in range(1, num_tokens):\n",
    "            target_token_id = sentence_token_ids[i]\n",
    "            # 対象トークンをテキストにデコード (APIレスポンスのトークンと比較するため)\n",
    "            target_token_text = encoding.decode([target_token_id])\n",
    "\n",
    "            # プロンプトとして使用するトークン列 (対象トークンの直前まで)\n",
    "            prompt_token_ids = sentence_token_ids[:i]\n",
    "            prompt_text = encoding.decode(prompt_token_ids)\n",
    "\n",
    "            # プロンプトが空になる場合（文頭のトークンに対する確率）は、\n",
    "            # APIのlogprobs機能では直接取得が難しいため、2番目のトークンから開始しています。\n",
    "            # パープレキシティの定義によっては最初のトークンの確率も考慮しますが、\n",
    "            # 一般的なLM評価では2番目以降で計算することが多いです。\n",
    "            # ここでは i=1 から開始するため、最初のトークン(i=0)をプロンプトにした次のトークン(i=1)から評価します。\n",
    "            if not prompt_text: # 最初のトークンに対する評価の場合（i=0の時だが、ここではi=1から開始）\n",
    "                 # print(f\"  スキップ: 最初のトークン '{target_token_text}' の確率計算はスキップされます。\")\n",
    "                 # num_terms += 1 # 項数に含める場合はコメント解除（分母に影響）\n",
    "                 continue # ここでは i=1 からループしているので、i=0 のケースは発生しない\n",
    "\n",
    "\n",
    "            # API呼び出し: プロンプトに続く最初の1トークンの確率を取得\n",
    "            # temperature=0.0 で最も尤もらしい分布を取得しやすくする\n",
    "            # top_logprobs で対象トークンが分布に含まれる可能性を高める（ただし上限あり）\n",
    "            try:\n",
    "                api_response = client.chat.completions.create(\n",
    "                    model=model,\n",
    "                    messages=[\n",
    "                        {\"role\": \"user\", \"content\": prompt_text}\n",
    "                    ],\n",
    "                    max_tokens=1, # 次の1トークンのみ予測\n",
    "                    temperature=0.0, # サンプリングのランダム性をなくす（確率分布は変わらない）\n",
    "                    logprobs=True, # ログ確率を要求\n",
    "                    top_logprobs=actual_top_k # 実際の制限値を適用\n",
    "                )\n",
    "\n",
    "                # API呼び出し間に遅延を入れる（レート制限対策）\n",
    "                time.sleep(delay_seconds)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"  エラー: API呼び出し失敗 (プロンプト: '{prompt_text}'): {e}\")\n",
    "                # API呼び出しに失敗した場合は、計算不可として無限大を返す\n",
    "                return float('inf')\n",
    "\n",
    "\n",
    "            # レスポンスからログ確率情報を抽出\n",
    "            logprobs_info = api_response.choices[0].logprobs\n",
    "            # logprobs_info.content は生成トークンに関するリスト。max_tokens=1なので通常は要素が1つ。\n",
    "            # logprobs_info.content[0].top_logprobs が上位候補リスト。\n",
    "            # top_logprobs_list の各要素は TopLogprob オブジェクトであることに注意\n",
    "            if not logprobs_info or not logprobs_info.content or not logprobs_info.content[0].top_logprobs:\n",
    "                print(f\"  警告: APIレスポンスに十分な logprobs 情報が含まれていませんでした (プロンプト: '{prompt_text}')。\")\n",
    "                 # logprob 情報がなければ計算不可\n",
    "                return float('inf')\n",
    "\n",
    "\n",
    "            top_logprobs_list = logprobs_info.content[0].top_logprobs\n",
    "\n",
    "            # 上位候補リストから、対象トークンの logprob を探す\n",
    "            found_logprob = None\n",
    "            # APIレスポンスのトークンテキストと、tiktokenでデコードしたトークンテキストを比較\n",
    "            # repr() を使うとスペースや改行などの非表示文字を含めて比較できるためより安全\n",
    "            target_token_repr = repr(target_token_text)\n",
    "\n",
    "            for token_info in top_logprobs_list:\n",
    "                 # ここを修正: ドット記法 (.) で属性にアクセスする\n",
    "                 if repr(token_info.token) == target_token_repr:\n",
    "                     found_logprob = token_info.logprob # ここを修正\n",
    "                     break # 見つかったらループを抜ける\n",
    "\n",
    "\n",
    "            # 対象トークンの logprob が上位K個の中に見つかったか確認\n",
    "            if found_logprob is not None:\n",
    "                total_logprob += found_logprob\n",
    "                num_terms += 1\n",
    "                # print(f\"    Found '{target_token_text}' ({target_token_repr}): {found_logprob}\") # デバッグ用\n",
    "            else:\n",
    "                # 対象トークンが上位K個に含まれていない場合、その確率は非常に低い\n",
    "                # ここでは警告を表示し、非常に小さい確率（大きい負のlogprob）を代用する\n",
    "                fallback_logprob = -20.0 # 代用する大きな負のlogprob\n",
    "                total_logprob += fallback_logprob\n",
    "                num_terms += 1\n",
    "                print(f\"  警告: 対象トークン '{target_token_text}' ({target_token_repr}) が上位 {actual_top_k} 候補に見つかりませんでした (プロンプト: '{prompt_text}')。代用logprob ({fallback_logprob}) を使用します。\")\n",
    "                # 見つからなかった時点で計算不可とするなら以下のコメントを解除\n",
    "                # return float('inf')\n",
    "\n",
    "\n",
    "        # パープレキシティの計算\n",
    "        # Perplexity = exp(- (1/N) * Sum(log P))\n",
    "        # ここで N は合計したlogprobの数 (num_terms)\n",
    "        if num_terms == 0:\n",
    "            print(\"  エラー: logprobを計算できたトークンがありません。\")\n",
    "            return float('inf')\n",
    "\n",
    "        average_logprob = total_logprob / num_terms\n",
    "        perplexity = math.exp(-average_logprob)\n",
    "\n",
    "        return perplexity\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  致命的なエラーが発生しました: {e}\")\n",
    "        return float('inf')\n",
    "\n",
    "\n",
    "# --- 主処理 ---\n",
    "\n",
    "# パープレキシティを測定する文のリスト\n",
    "sentences_to_test = [\n",
    "    \"The movie was full of surprises\",       # 文法的にも意味的にも自然\n",
    "    \"The movies were full of surprises\",     # 主語・動詞が複数形で一致、自然\n",
    "    \"The movie were full of surprises\",      # 主語が単数、動詞が複数形で不一致 (文法ミス)\n",
    "    \"The movies was full of surprises\",      # 主語が複数、動詞が単数形で不一致 (文法ミス)\n",
    "    \"Surprises full of was movie The\",       # 単語順序が不自然 (文法ミス)\n",
    "    \"This is a very common sentence\",        # 別の自然な文\n",
    "    \"This very is a common sentence\",        # 単語順序が不自然 (文法ミス)\n",
    "    \"The quick brown fox jumps over the lazy dog\", # より長い標準的な文\n",
    "]\n",
    "\n",
    "model_name = \"gpt-4o\" # あなたのAzure OpenAIでデプロイしたモデル名を指定\n",
    "# APIの制限に合わせて top_k_for_logprobs を 20 に修正\n",
    "top_k_for_logprobs = 20\n",
    "api_delay_seconds = 0.01 # API呼び出し間の短い遅延 (推奨)\n",
    "\n",
    "print(f\"=== 課題 ===\")\n",
    "print(f\"与えられた文に対するパープレキシティを測定し、比較する。\")\n",
    "print(f\"モデル: {model_name}\")\n",
    "print(f\"logprob取得のための上位K (API制限): {top_k_for_logprobs}\")\n",
    "print(f\"API遅延: {api_delay_seconds}秒\")\n",
    "print(\"============\")\n",
    "\n",
    "# クライアントが初期化されているか確認\n",
    "if client is None:\n",
    "    print(\"\\nエラー: Azure OpenAI クライアントが初期化されていません。パープレキシティ計算を実行できません。\")\n",
    "else:\n",
    "    print(\"\\n=== パープレキシティ測定結果 ===\")\n",
    "    # 各文に対してパープレキシティを計算し表示\n",
    "    for sentence in sentences_to_test:\n",
    "        # calculate_perplexity 関数内でクライアント初期化の成功をチェック\n",
    "        perplexity_value = calculate_perplexity(client, model_name, sentence, top_k_for_logprobs, api_delay_seconds)\n",
    "\n",
    "        if perplexity_value == float('inf'):\n",
    "            print(f\"文: '{sentence}'\")\n",
    "            print(f\"パープレキシティ: 計算不可\") # エラーメッセージは関数内で出力済み\n",
    "        else:\n",
    "            print(f\"文: '{sentence}'\")\n",
    "            print(f\"パープレキシティ: {perplexity_value:.4f}\") # 小数点以下4桁で表示\n",
    "        print(\"-\" * 30) # 区切り線"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8-7fB-n9suYg"
   },
   "source": [
    "## 94. チャットテンプレート\n",
    "\n",
    "\"What do you call a sweet eaten after dinner?\"という問いかけに対する応答を生成するため、チャットテンプレートを適用し、言語モデルに与えるべきプロンプトを作成せよ。また、そのプロンプトに対する応答を生成し、表示せよ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "エラー: 前提となる変数 'train_bert_dataset' が定義されていません。\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "Variable 'train_bert_dataset' is not defined or None.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     20\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m var_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlocals\u001b[39m() \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlocals\u001b[39m()[var_name] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     21\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mエラー: 前提となる変数 \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvar_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m が定義されていません。\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNameError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mVariable \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvar_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m is not defined or None.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mBertSST2Dataset\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlocals\u001b[39m():\n\u001b[32m     25\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mエラー: 前提となるクラス \u001b[39m\u001b[33m'\u001b[39m\u001b[33mBertSST2Dataset\u001b[39m\u001b[33m'\u001b[39m\u001b[33m が定義されていません。\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: Variable 'train_bert_dataset' is not defined or None."
     ]
    }
   ],
   "source": [
    "# 問題89のセルを実行する前に、問題80や問題85でHugging FaceのBERTトークナイザをロード\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup # AutoModelを使用\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# --- 前提となる変数・クラス・関数の定義 (問題87と同様) ---\n",
    "# tokenizer, train_bert_dataset, dev_bert_dataset, MAX_LENGTH\n",
    "# BertSST2Dataset クラス\n",
    "\n",
    "# --- 変数・クラスが現在のセッションに存在するか確認 ---\n",
    "required_vars_p89 = ['tokenizer', 'train_bert_dataset', 'dev_bert_dataset', 'MAX_LENGTH']\n",
    "for var_name in required_vars_p89:\n",
    "    if var_name not in locals() or locals()[var_name] is None:\n",
    "        print(f\"エラー: 前提となる変数 '{var_name}' が定義されていません。\")\n",
    "        raise NameError(f\"Variable '{var_name}' is not defined or None.\")\n",
    "\n",
    "if 'BertSST2Dataset' not in locals():\n",
    "        print(f\"エラー: 前提となるクラス 'BertSST2Dataset' が定義されていません。\")\n",
    "        raise NameError(f\"Class 'BertSST2Dataset' is not defined.\")\n",
    "\n",
    "# --- ここから問題89の処理 ---\n",
    "\n",
    "# 1. 新しいアーキテクチャのモデルクラス定義 (平均プーリング)\n",
    "class BertMeanPoolingClassifier(nn.Module):\n",
    "    def __init__(self, model_name, num_labels, dropout_rate=0.1):\n",
    "        super(BertMeanPoolingClassifier, self).__init__()\n",
    "        # 事前学習済みBERTモデルをロード (分類ヘッドなし)\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        # BERTの隠れ層サイズを取得 (例: bert-base-uncased なら 768)\n",
    "        self.bert_hidden_size = self.bert.config.hidden_size \n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        # 分類用の線形層\n",
    "        self.classifier = nn.Linear(self.bert_hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # BERTモデルから最終層の隠れ状態を取得\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_state = outputs.last_hidden_state # (batch_size, seq_len, hidden_size)\n",
    "        \n",
    "        # 平均プーリング (attention_mask を利用してパディングを無視)\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n",
    "        sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9) # ゼロ除算を避ける\n",
    "        mean_pooled_output = sum_embeddings / sum_mask\n",
    "        # mean_pooled_output: (batch_size, hidden_size)\n",
    "        \n",
    "        # ドロップアウトと分類\n",
    "        pooled_output_dropout = self.dropout(mean_pooled_output)\n",
    "        logits = self.classifier(pooled_output_dropout) # (batch_size, num_labels)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "# --- モデルと学習の準備 ---\n",
    "model_name_p89 = \"bert-base-uncased\" # 問題87と同じ事前学習モデルを使用\n",
    "num_labels_p89 = 2 # ポジネガ2値分類\n",
    "\n",
    "# デバイス設定\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"GPU ({torch.cuda.get_device_name(0)}) を使用します。\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPUが利用できません。CPUを使用します。\")\n",
    "\n",
    "# モデルのインスタンス化とデバイスへの転送\n",
    "model_meanpool = BertMeanPoolingClassifier(model_name_p89, num_labels_p89)\n",
    "model_meanpool.to(device)\n",
    "print(f\"\\n平均プーリングを用いたBERT分類モデル '{model_name_p89}' をロードし、デバイスに転送しました。\")\n",
    "print(model_meanpool)\n",
    "\n",
    "\n",
    "# データローダーの準備 (問題87と同様)\n",
    "# BertSST2Dataset は {key: val[idx]} を返すので、モデルの入力に合わせて調整が必要な場合がある\n",
    "# BertMeanPoolingClassifier は input_ids と attention_mask を想定\n",
    "class BertInputDataset(Dataset): # BertSST2Dataset を少し変更\n",
    "    def __init__(self, encodings_dict, labels_tensor):\n",
    "        self.input_ids = encodings_dict['input_ids']\n",
    "        self.attention_mask = encodings_dict['attention_mask']\n",
    "        # token_type_ids があればそれも\n",
    "        self.token_type_ids = encodings_dict.get('token_type_ids', None) # なければNone\n",
    "        self.labels = labels_tensor\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {\n",
    "            'input_ids': self.input_ids[idx].clone().detach(),\n",
    "            'attention_mask': self.attention_mask[idx].clone().detach()\n",
    "        }\n",
    "        if self.token_type_ids is not None:\n",
    "             item['token_type_ids'] = self.token_type_ids[idx].clone().detach()\n",
    "        item['labels'] = self.labels[idx].clone().detach()\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset_mp = BertInputDataset(\n",
    "    {'input_ids': train_bert_dataset['input_ids'], \n",
    "     'attention_mask': train_bert_dataset['attention_mask'],\n",
    "     'token_type_ids': train_bert_dataset.get('token_type_ids', None)}, # token_type_idsも渡す\n",
    "    train_bert_dataset['labels']\n",
    ")\n",
    "dev_dataset_mp = BertInputDataset(\n",
    "    {'input_ids': dev_bert_dataset['input_ids'], \n",
    "     'attention_mask': dev_bert_dataset['attention_mask'],\n",
    "     'token_type_ids': dev_bert_dataset.get('token_type_ids', None)},\n",
    "    dev_bert_dataset['labels']\n",
    ")\n",
    "\n",
    "batch_size_mp = 16\n",
    "train_dataloader_mp = DataLoader(train_dataset_mp, batch_size=batch_size_mp, shuffle=True)\n",
    "dev_dataloader_mp = DataLoader(dev_dataset_mp, batch_size=batch_size_mp, shuffle=False)\n",
    "\n",
    "\n",
    "# 学習パラメータ\n",
    "learning_rate_mp = 2e-5\n",
    "num_epochs_mp = 3 # 問題87と同程度のエポック数で比較\n",
    "\n",
    "# 損失関数 (クラス数が2なのでCrossEntropyLossが一般的)\n",
    "# モデルの出力ロジットは (batch_size, num_labels) の形状になる\n",
    "criterion_mp = nn.CrossEntropyLoss() \n",
    "optimizer_mp = optim.AdamW(model_meanpool.parameters(), lr=learning_rate_mp)\n",
    "total_steps_mp = len(train_dataloader_mp) * num_epochs_mp\n",
    "scheduler_mp = get_linear_schedule_with_warmup(optimizer_mp, num_warmup_steps=0, num_training_steps=total_steps_mp)\n",
    "\n",
    "print(f\"\\n平均プーリングモデルのファインチューニング (バッチサイズ={batch_size_mp}, エポック数={num_epochs_mp}) を開始します...\")\n",
    "start_time_mp = time.time()\n",
    "\n",
    "# --- 学習ループ (問題87とほぼ同じだが、損失計算の仕方が少し異なる可能性) ---\n",
    "for epoch in range(num_epochs_mp):\n",
    "    print(f\"\\n--- Epoch {epoch+1}/{num_epochs_mp} ---\")\n",
    "    model_meanpool.train()\n",
    "    total_train_loss = 0\n",
    "    \n",
    "    for batch_idx, batch in enumerate(train_dataloader_mp):\n",
    "        optimizer_mp.zero_grad()\n",
    "        \n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device) # ラベルは (batch_size) の形状で、クラスインデックス (0 or 1)\n",
    "        # token_type_ids は今回のモデルでは使わないが、渡せるようにしておく\n",
    "        # token_type_ids = batch.get('token_type_ids', None)\n",
    "        # if token_type_ids is not None:\n",
    "        #     token_type_ids = token_type_ids.to(device)\n",
    "        \n",
    "        # logits = model_meanpool(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # AutoModelは **kwargs を受け付けるので、token_type_ids も渡せる\n",
    "        model_inputs = {'input_ids': input_ids, 'attention_mask': attention_mask}\n",
    "        # if token_type_ids is not None: # BertMeanPoolingClassifier側で **kwargs を受け取らないので、直接渡せない\n",
    "        #     model_inputs['token_type_ids'] = token_type_ids\n",
    "        logits = model_meanpool(**model_inputs) # **model_inputs で展開して渡す\n",
    "\n",
    "        loss = criterion_mp(logits, labels) # CrossEntropyLossはロジットと整数のラベルを期待\n",
    "        total_train_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer_mp.step()\n",
    "        scheduler_mp.step()\n",
    "        \n",
    "        if (batch_idx + 1) % (len(train_dataloader_mp) // 10) == 0 or (batch_idx + 1) == len(train_dataloader_mp):\n",
    "            print(f\"  Batch [{batch_idx+1}/{len(train_dataloader_mp)}], Avg Train Loss so far: {total_train_loss / (batch_idx+1):.4f}\")\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader_mp)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs_mp}] 完了, 平均訓練損失: {avg_train_loss:.4f}\")\n",
    "\n",
    "    # エポックごとに検証データで評価\n",
    "    model_meanpool.eval()\n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dev_dataloader_mp:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            # token_type_ids = batch.get('token_type_ids', None)\n",
    "            # if token_type_ids is not None:\n",
    "            #     token_type_ids = token_type_ids.to(device)\n",
    "            \n",
    "            model_inputs_dev = {'input_ids': input_ids, 'attention_mask': attention_mask}\n",
    "            # if token_type_ids is not None:\n",
    "            #     model_inputs_dev['token_type_ids'] = token_type_ids\n",
    "            logits = model_meanpool(**model_inputs_dev)\n",
    "            \n",
    "            loss = criterion_mp(logits, labels)\n",
    "            total_eval_loss += loss.item()\n",
    "            \n",
    "            predictions = torch.argmax(logits, dim=-1) # (batch_size) の形状\n",
    "            total_eval_accuracy += accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy()) * labels.size(0)\n",
    "\n",
    "    avg_val_accuracy = total_eval_accuracy / len(dev_dataset_mp)\n",
    "    avg_val_loss = total_eval_loss / len(dev_dataloader_mp)\n",
    "    print(f\"  Epoch [{epoch+1}/{num_epochs_mp}], 検証データ: 平均損失={avg_val_loss:.4f}, 正解率={avg_val_accuracy:.4f}\")\n",
    "\n",
    "end_time_mp = time.time()\n",
    "print(f\"\\n平均プーリングモデルのファインチューニングが完了しました。所要時間: {end_time_mp - start_time_mp:.2f} 秒\")\n",
    "\n",
    "# 5. 最終評価\n",
    "print(\"\\n--- 最終評価 (開発セット、平均プーリングモデル) ---\")\n",
    "model_meanpool.eval()\n",
    "final_predictions_mp = []\n",
    "final_true_labels_mp = []\n",
    "with torch.no_grad():\n",
    "    for batch in dev_dataloader_mp:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        # token_type_ids = batch.get('token_type_ids', None)\n",
    "        # if token_type_ids is not None:\n",
    "        #     token_type_ids = token_type_ids.to(device)\n",
    "\n",
    "        model_inputs_final_eval = {'input_ids': input_ids, 'attention_mask': attention_mask}\n",
    "        # if token_type_ids is not None:\n",
    "        #     model_inputs_final_eval['token_type_ids'] = token_type_ids\n",
    "        logits = model_meanpool(**model_inputs_final_eval)\n",
    "        \n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        \n",
    "        final_predictions_mp.extend(predictions.cpu().numpy())\n",
    "        final_true_labels_mp.extend(labels.cpu().numpy())\n",
    "\n",
    "final_accuracy_mp = accuracy_score(final_true_labels_mp, final_predictions_mp)\n",
    "print(f\"開発セットにおける最終正解率 (平均プーリングモデル): {final_accuracy_mp:.4f}\")\n",
    "print(f\"  正解した事例数: {int(final_accuracy_mp * len(final_true_labels_mp))}\")\n",
    "print(f\"  総事例数: {len(final_true_labels_mp)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "PT-bk0XWIZ2E",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 95. マルチターンのチャット\n",
    "\n",
    "問題94で生成された応答に対して、追加で\"Please give me the plural form of the word with its spelling in reverse order.\"と問いかけたときの応答を生成・表示せよ。また、その時に言語モデルに与えるプロンプトを確認せよ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# from openai import AzureOpenAI # 問題90のセルで初期化済みのはず\n",
    "# from dotenv import load_dotenv # 問題90のセルで実行済みのはず\n",
    "\n",
    "# --- 前提となる変数 ---\n",
    "# client: 問題90で初期化された AzureOpenAI クライアントのインスタンス\n",
    "# model_to_use: 問題94で使用したモデル名 (例: \"gpt-4o\")\n",
    "\n",
    "# これらの変数が現在のセッションに存在することを確認してください。\n",
    "if 'client' not in locals() or client is None:\n",
    "    print(\"エラー: AzureOpenAIクライアント 'client' が初期化されていません。\")\n",
    "    print(\"問題90のセルを先に実行してクライアントを準備してください。\")\n",
    "    raise NameError(\"AzureOpenAI client 'client' is not defined.\")\n",
    "if 'model_to_use' not in locals() or model_to_use is None:\n",
    "    # 問題94で使用したモデル名をここで設定するか、問題94のセルで定義してください\n",
    "    model_to_use = \"gpt-4o\" # デフォルトまたは前回使用したモデル名\n",
    "    print(f\"警告: 'model_to_use' が未定義でしたので、'{model_to_use}' を使用します。\")\n",
    "\n",
    "\n",
    "# --- ここから問題95の処理 ---\n",
    "\n",
    "# 1. 問題94のやり取りと、問題95の新しい質問を準備\n",
    "# !!! 重要: assistant_response_94 は、実際に問題94を実行して得られたLLMの応答に置き換えてください !!!\n",
    "# もし問題94をまだ実行していない場合、まず問題94を実行してこの応答を取得してください。\n",
    "# 以下はダミーの応答です。必ず実際の応答で上書きしてください。\n",
    "user_query_94 = \"What do you call a sweet eaten after dinner?\"\n",
    "assistant_response_94 = \"That's typically called dessert.\" # ← ★★★ 必ず問題94の実際のLLM応答に置き換えてください ★★★\n",
    "\n",
    "if assistant_response_94 == \"That's typically called dessert.\": # ダミーのままなら警告\n",
    "    print(\"警告: 'assistant_response_94' がダミーのままです。問題94を実行して実際のLLMの応答に置き換えてください。\")\n",
    "    print(\"このまま実行すると、LLMはダミーの応答を文脈として扱います。\")\n",
    "\n",
    "\n",
    "user_query_95 = \"Please give me the plural form of the word with its spelling in reverse order.\"\n",
    "system_prompt_95 = \"You are a helpful assistant that answers questions precisely and follows instructions.\" # 例\n",
    "\n",
    "# 2. messages リストの構築\n",
    "messages_for_model_95 = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt_95},\n",
    "    {\"role\": \"user\", \"content\": user_query_94},\n",
    "    {\"role\": \"assistant\", \"content\": assistant_response_94}, # 問題94のLLMの応答\n",
    "    {\"role\": \"user\", \"content\": user_query_95}        # 問題95の新しい質問\n",
    "]\n",
    "\n",
    "print(\"--- 言語モデルに与える messages リスト (プロンプト全体) ---\")\n",
    "for message in messages_for_model_95:\n",
    "    print(message)\n",
    "\n",
    "# 3. 言語モデルによる応答生成\n",
    "try:\n",
    "    print(f\"\\n--- モデル '{model_to_use}' からの応答生成中 (問題95)... ---\")\n",
    "    completion_95 = client.chat.completions.create(\n",
    "        model=model_to_use,\n",
    "        messages=messages_for_model_95,\n",
    "        temperature=0.5, # 少し創造性を抑えめに (適宜調整)\n",
    "        max_tokens=80    # 応答の最大長 (適宜調整)\n",
    "    )\n",
    "    \n",
    "    # 4. 応答の表示\n",
    "    if completion_95.choices:\n",
    "        assistant_response_95 = completion_95.choices[0].message.content\n",
    "        print(\"\\nモデルの応答 (問題95):\")\n",
    "        print(assistant_response_95)\n",
    "    else:\n",
    "        print(\"モデルから応答が得られませんでした。\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"API呼び出し中にエラーが発生しました: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "qH0YortL0afd",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 96. プロンプトによる感情分析\n",
    "\n",
    "事前学習済み言語モデルで感情分析を行いたい。テキストを含むプロンプトを事前学習済み言語モデルに与え、（ファインチューニングは行わずに）テキストのポジネガを予測するという戦略で、[SST-2](https://dl.fbaipublicfiles.com/glue/data/SST-2.zip)の開発データにおける正解率を測定せよ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "# from openai import AzureOpenAI # 問題90のセルで初期化済みのはず\n",
    "from sklearn.metrics import accuracy_score # 正解率計算用\n",
    "\n",
    "# --- 前提となる変数 ---\n",
    "# client: 問題90で初期化された AzureOpenAI クライアントのインスタンス\n",
    "# model_to_use: 使用するモデル名 (例: \"gpt-4o\")\n",
    "# dev_file_path: SST-2のdev.tsvへのパス (問題60, 71, 85で定義済み)\n",
    "\n",
    "# --- 変数・クラスが現在のセッションに存在するか確認 ---\n",
    "if 'client' not in locals() or client is None:\n",
    "    print(\"エラー: AzureOpenAIクライアント 'client' が初期化されていません。\")\n",
    "    raise NameError(\"AzureOpenAI client 'client' is not defined.\")\n",
    "if 'model_to_use' not in locals() or model_to_use is None:\n",
    "    model_to_use = \"gpt-4o\" \n",
    "    print(f\"警告: 'model_to_use' が未定義でしたので、'{model_to_use}' を使用します。\")\n",
    "\n",
    "base_data_dir_p96 = '../data/SST-2_data' # 問題85, 60などで使用したパスを想定\n",
    "dev_file_path_p96 = os.path.join(base_data_dir_p96, \"SST-2/dev.tsv\")\n",
    "\n",
    "if not os.path.exists(dev_file_path_p96):\n",
    "    print(f\"エラー: 検証データファイル '{dev_file_path_p96}' が見つかりません。\")\n",
    "    print(\"問題60を先に実行してSST-2データセットをダウンロード・展開してください。\")\n",
    "    raise FileNotFoundError(\"SST-2 dev.tsv not found.\")\n",
    "\n",
    "# --- ここから問題96の処理 ---\n",
    "\n",
    "# 1. SST-2 開発データの読み込み\n",
    "try:\n",
    "    df_dev = pd.read_csv(dev_file_path_p96, sep='\\t')\n",
    "    print(f\"検証データ ({os.path.basename(dev_file_path_p96)}) を読み込みました。件数: {len(df_dev)}\")\n",
    "except Exception as e:\n",
    "    print(f\"検証データの読み込み中にエラー: {e}\")\n",
    "    raise\n",
    "\n",
    "def get_sentiment_from_llm(text_to_analyze, llm_client, model_name):\n",
    "    \"\"\"LLMにテキストの感情を問いかけ、応答から感情ラベルを抽出する\"\"\"\n",
    "    prompt_template = \"\"\"以下の映画レビューの感情は「ポジティブ」ですか、それとも「ネガティブ」ですか？ 回答は「ポジティブ」または「ネガティブ」のどちらか一言でお願いします。\n",
    "\n",
    "レビュー: \"{text}\"\n",
    "感情:\"\"\"\n",
    "    \n",
    "    prompt = prompt_template.format(text=text_to_analyze)\n",
    "    \n",
    "    try:\n",
    "        completion = llm_client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=[\n",
    "                # {\"role\": \"system\", \"content\": \"You are an expert sentiment classifier.\"}, # 必要に応じてシステムプロンプト追加\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.0, # 再現性のため、できるだけ決定的な出力を得る\n",
    "            max_tokens=10    # \"ポジティブ\" or \"ネガティブ\" と少しの周辺語で十分\n",
    "        )\n",
    "        response_text = completion.choices[0].message.content.strip().lower()\n",
    "\n",
    "        # LLMの応答からラベルを抽出 (より頑健な方法も検討可能)\n",
    "        if \"ポジティブ\" in response_text or \"positive\" in response_text:\n",
    "            return 1 # ポジティブ\n",
    "        elif \"ネガティブ\" in response_text or \"negative\" in response_text:\n",
    "            return 0 # ネガティブ\n",
    "        else:\n",
    "            # print(f\"  警告: 予期せぬ応答 '{response_text}'。判定不能として扱います。\")\n",
    "            return -1 # 判定不能\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"  API呼び出し中にエラーが発生 (テキスト: \\\"{text_to_analyze[:50]}...\\\"): {e}\")\n",
    "        return -1 # エラー時は判定不能\n",
    "\n",
    "# 3. 各レビュー文に対する感情予測と正解率計算\n",
    "true_labels = []\n",
    "predicted_labels_by_prompt = []\n",
    "\n",
    "# APIのレート制限を考慮し、また全件実行すると時間がかかるため、\n",
    "# devデータの一部 (例: 先頭100件) で試すことを推奨します。\n",
    "# 全件 (872件) 実行する場合は時間がかかることとAPIコストに注意してください。\n",
    "num_dev_samples_to_process = len(df_dev) # 全件処理する場合\n",
    "# num_dev_samples_to_process = 100 # テスト用に件数を絞る場合\n",
    "\n",
    "print(f\"\\nSST-2開発データの最初の{num_dev_samples_to_process}件について、プロンプトベースの感情分析を実行します...\")\n",
    "\n",
    "for index, row in df_dev.head(num_dev_samples_to_process).iterrows():\n",
    "    text = str(row['sentence'])\n",
    "    true_label = int(row['label'])\n",
    "    \n",
    "    predicted_label = get_sentiment_from_llm(text, client, model_to_use)\n",
    "    \n",
    "    if predicted_label != -1: # 判定不能・エラーでなかった場合のみ\n",
    "        true_labels.append(true_label)\n",
    "        predicted_labels_by_prompt.append(predicted_label)\n",
    "    \n",
    "    if (index + 1) % 20 == 0 or (index + 1) == num_dev_samples_to_process:\n",
    "        print(f\"  処理中: {index + 1}/{num_dev_samples_to_process} 件完了...\")\n",
    "    \n",
    "    time.sleep(0.1) # APIのレート制限を避けるための短い待機 (必要に応じて調整)\n",
    "\n",
    "\n",
    "# 4. 正解率の計算\n",
    "if true_labels: # 有効な予測が1つでもあれば\n",
    "    accuracy = accuracy_score(true_labels, predicted_labels_by_prompt)\n",
    "    print(f\"\\n--- プロンプトによる感情分析の結果 (開発データ) ---\")\n",
    "    print(f\"評価した事例数: {len(true_labels)}\")\n",
    "    print(f\"正解率: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "else:\n",
    "    print(\"\\n有効な予測が得られなかったため、正解率を計算できませんでした。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "giA6FivrKaSf",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 97. 埋め込みに基づく感情分析\n",
    "\n",
    "事前学習済み言語モデルでテキストをベクトルで表現（エンコード）し、そのベクトルにフィードフォワード層を通すことで極性ラベルを予測するモデルを学習せよ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "import os\n",
    "import pandas as pd # 万が一の再読み込み用\n",
    "\n",
    "# --- 前提となる変数・データ (問題85から) ---\n",
    "# train_bert_dataset: 問題85で作成した訓練データ {'input_ids': tensor, 'attention_mask': tensor, 'labels': tensor}\n",
    "# dev_bert_dataset: 問題85で作成した検証データ {'input_ids': tensor, 'attention_mask': tensor, 'labels': tensor}\n",
    "# MAX_LENGTH: 問題85で使用した最大シーケンス長\n",
    "\n",
    "# --- 変数・クラスが現在のセッションに存在するか確認 ---\n",
    "# (問題87と同様の変数チェックをここに記述)\n",
    "required_vars_p97 = ['train_bert_dataset', 'dev_bert_dataset', 'MAX_LENGTH']\n",
    "for var_name in required_vars_p97:\n",
    "    if var_name not in locals() or locals()[var_name] is None:\n",
    "        print(f\"エラー: 前提となる変数 '{var_name}' が定義されていません。\")\n",
    "        raise NameError(f\"Variable '{var_name}' is not defined or None.\")\n",
    "\n",
    "# --- ここから問題97の処理 ---\n",
    "\n",
    "# 1. モデルとトークナイザの準備\n",
    "model_name_gpt_cls = \"distilgpt2\" # GPT-2系の比較的小さなモデルを例として使用\n",
    "# model_name_gpt_cls = \"gpt2\" # もしリソースがあればgpt2も試せる\n",
    "\n",
    "try:\n",
    "    tokenizer_gpt_cls = AutoTokenizer.from_pretrained(model_name_gpt_cls)\n",
    "    # GPT-2系トークナイザは通常パディングトークンが定義されていないので、EOSトークンで代用\n",
    "    if tokenizer_gpt_cls.pad_token is None:\n",
    "        tokenizer_gpt_cls.pad_token = tokenizer_gpt_cls.eos_token\n",
    "        print(f\"トークナイザ '{model_name_gpt_cls}' の pad_token を eos_token に設定しました。\")\n",
    "    \n",
    "    # num_labels=2 で2値分類用のヘッドを持つモデルをロード\n",
    "    # GPT-2系は pad_token_id も config に設定する必要がある場合がある\n",
    "    model_gpt_cls = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name_gpt_cls, \n",
    "        num_labels=2,\n",
    "        pad_token_id=tokenizer_gpt_cls.pad_token_id # pad_token_idを設定\n",
    "    )\n",
    "    print(f\"事前学習済みモデル '{model_name_gpt_cls}' (分類用) をロードしました。\")\n",
    "    \n",
    "    # GPT-2系モデルの分類ヘッドは、通常シーケンスの最後のトークンの隠れ状態を使うように設定されている。\n",
    "    # そのため、トークナイザのパディング方向も重要になることがある。\n",
    "    # tokenizer_gpt_cls.padding_side = 'left' # モデルによっては左パディングが良い場合も (今回はデフォルトの右を使用)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"モデル '{model_name_gpt_cls}' またはトークナイザのロード中にエラーが発生しました: {e}\")\n",
    "    raise\n",
    "\n",
    "# 2. データセットとデータローダーの準備 (問題87のBertSST2Datasetを流用)\n",
    "class GPTClassificationDataset(Dataset): # 問題87のBertSST2Datasetと同様の構造\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True, # GPT-2も通常は特殊トークンを期待 (入力形式による)\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False, # GPT-2では通常不要\n",
    "            padding='max_length',        # DataLoaderでバッチ化するために最大長にパディング\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long) # CrossEntropyLossはlong型のラベルを期待\n",
    "        }\n",
    "\n",
    "# SST-2の元のテキストとラベルを再度取得する必要がある\n",
    "# (train_bert_datasetは既にID化されているため、ここでは元のDataFrameを想定)\n",
    "# 問題60, 85で読み込んだdf_train, df_dev を使う\n",
    "base_data_dir_p97 = '../data/SST-2_data' \n",
    "train_file_path_p97 = os.path.join(base_data_dir_p97, \"SST-2/train.tsv\")\n",
    "dev_file_path_p97 = os.path.join(base_data_dir_p97, \"SST-2/dev.tsv\")\n",
    "\n",
    "try:\n",
    "    df_train_p97 = pd.read_csv(train_file_path_p97, sep='\\t')\n",
    "    df_dev_p97 = pd.read_csv(dev_file_path_p97, sep='\\t')\n",
    "except FileNotFoundError:\n",
    "    print(\"エラー: SST-2のTSVファイルが見つかりません。問題60を再実行してください。\")\n",
    "    raise\n",
    "\n",
    "train_texts_p97 = df_train_p97['sentence'].tolist()\n",
    "train_labels_p97 = df_train_p97['label'].tolist()\n",
    "dev_texts_p97 = df_dev_p97['sentence'].tolist()\n",
    "dev_labels_p97 = df_dev_p97['label'].tolist()\n",
    "\n",
    "\n",
    "train_dataset_gpt_cls = GPTClassificationDataset(train_texts_p97, train_labels_p97, tokenizer_gpt_cls, MAX_LENGTH)\n",
    "dev_dataset_gpt_cls = GPTClassificationDataset(dev_texts_p97, dev_labels_p97, tokenizer_gpt_cls, MAX_LENGTH)\n",
    "\n",
    "batch_size_gpt_cls = 16 \n",
    "train_dataloader_gpt_cls = DataLoader(train_dataset_gpt_cls, batch_size=batch_size_gpt_cls, shuffle=True)\n",
    "dev_dataloader_gpt_cls = DataLoader(dev_dataset_gpt_cls, batch_size=batch_size_gpt_cls, shuffle=False)\n",
    "\n",
    "\n",
    "# 3. 学習の準備\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"\\nGPU ({torch.cuda.get_device_name(0)}) を使用します。\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"\\nGPUが利用できません。CPUを使用します。\")\n",
    "\n",
    "model_gpt_cls.to(device)\n",
    "\n",
    "optimizer_gpt_cls = optim.AdamW(model_gpt_cls.parameters(), lr=5e-5) # GPT系のファインチューニングではBERTより少し高めの学習率も試される\n",
    "num_epochs_gpt_cls = 3\n",
    "total_steps_gpt_cls = len(train_dataloader_gpt_cls) * num_epochs_gpt_cls\n",
    "scheduler_gpt_cls = get_linear_schedule_with_warmup(optimizer_gpt_cls, num_warmup_steps=0, num_training_steps=total_steps_gpt_cls)\n",
    "# 損失関数はモデル内部で計算される (CrossEntropyLoss相当)\n",
    "\n",
    "print(f\"\\nGPT系モデルのファインチューニング (バッチサイズ={batch_size_gpt_cls}, エポック数={num_epochs_gpt_cls}) を開始します...\")\n",
    "start_time_gpt_cls = time.time()\n",
    "\n",
    "# 4. ファインチューニング（学習ループ） - 問題87とほぼ同じ\n",
    "for epoch in range(num_epochs_gpt_cls):\n",
    "    print(f\"\\n--- Epoch {epoch+1}/{num_epochs_gpt_cls} ---\")\n",
    "    model_gpt_cls.train()\n",
    "    total_train_loss = 0\n",
    "    \n",
    "    for batch_idx, batch in enumerate(train_dataloader_gpt_cls):\n",
    "        optimizer_gpt_cls.zero_grad()\n",
    "        \n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        outputs = model_gpt_cls(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        total_train_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer_gpt_cls.step()\n",
    "        scheduler_gpt_cls.step()\n",
    "        \n",
    "        if (batch_idx + 1) % (len(train_dataloader_gpt_cls) // 10) == 0 or (batch_idx + 1) == len(train_dataloader_gpt_cls):\n",
    "             print(f\"  Batch [{batch_idx+1}/{len(train_dataloader_gpt_cls)}], Avg Train Loss so far: {total_train_loss / (batch_idx+1):.4f}\")\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader_gpt_cls)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs_gpt_cls}] 完了, 平均訓練損失: {avg_train_loss:.4f}\")\n",
    "\n",
    "    model_gpt_cls.eval()\n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in dev_dataloader_gpt_cls:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model_gpt_cls(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            total_eval_loss += loss.item()\n",
    "            logits = outputs.logits\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "            total_eval_accuracy += accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy()) * labels.size(0)\n",
    "\n",
    "    avg_val_accuracy = total_eval_accuracy / len(dev_dataset_gpt_cls.dataset) # Datasetの長さを分母に\n",
    "    avg_val_loss = total_eval_loss / len(dev_dataloader_gpt_cls)\n",
    "    print(f\"  Epoch [{epoch+1}/{num_epochs_gpt_cls}], 検証データ: 平均損失={avg_val_loss:.4f}, 正解率={avg_val_accuracy:.4f}\")\n",
    "\n",
    "end_time_gpt_cls = time.time()\n",
    "print(f\"\\nGPT系モデルのファインチューニングが完了しました。所要時間: {end_time_gpt_cls - start_time_gpt_cls:.2f} 秒\")\n",
    "\n",
    "# 5. 最終評価\n",
    "print(\"\\n--- 最終評価 (開発セット、GPT系分類モデル) ---\")\n",
    "model_gpt_cls.eval()\n",
    "final_predictions_gpt_cls = []\n",
    "final_true_labels_gpt_cls = []\n",
    "with torch.no_grad():\n",
    "    for batch in dev_dataloader_gpt_cls:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model_gpt_cls(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        final_predictions_gpt_cls.extend(predictions.cpu().numpy())\n",
    "        final_true_labels_gpt_cls.extend(labels.cpu().numpy())\n",
    "\n",
    "final_accuracy_gpt_cls = accuracy_score(final_true_labels_gpt_cls, final_predictions_gpt_cls)\n",
    "print(f\"開発セットにおける最終正解率 (GPT系分類モデル): {final_accuracy_gpt_cls:.4f}\")\n",
    "print(f\"  正解した事例数: {int(final_accuracy_gpt_cls * len(final_true_labels_gpt_cls))}\")\n",
    "print(f\"  総事例数: {len(final_true_labels_gpt_cls)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "UnREZD3nTWUr",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 98. ファインチューニング\n",
    "\n",
    "問題96のプロンプトに対して、正解の感情ラベルをテキストの応答として返すように事前学習済みモデルをファインチューニングせよ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "# from openai import AzureOpenAI # 問題90のセルで初期化済みのはず\n",
    "\n",
    "# --- 前提となる変数 ---\n",
    "# client: 問題90で初期化された AzureOpenAI クライアントのインスタンス\n",
    "# model_to_use_for_ft_base: ファインチューニングのベースとなるモデル名 \n",
    "#                           (例: \"gpt-3.5-turbo-0125\", Azureの場合はデプロイ名ではなくモデルバージョン名)\n",
    "# train_file_path_p98: SST-2のtrain.tsvへのパス\n",
    "# dev_file_path_p98: SST-2のdev.tsvへのパス\n",
    "\n",
    "# --- 変数・クラスが現在のセッションに存在するか確認 ---\n",
    "if 'client' not in locals() or client is None:\n",
    "    print(\"エラー: AzureOpenAIクライアント 'client' が初期化されていません。\")\n",
    "    raise NameError(\"AzureOpenAI client 'client' is not defined.\")\n",
    "\n",
    "# ファインチューニングのベースモデル名 (Azureのモデルバージョン名か、OpenAIのモデルID)\n",
    "# 例: Azureでgpt-3.5-turboのバージョン0125をファインチューニングする場合など\n",
    "# OpenAI直接なら \"gpt-3.5-turbo\" など (利用可能なモデルを確認してください)\n",
    "model_to_use_for_ft_base = \"gpt-3.5-turbo-0125\" # ★★★ ご自身の環境に合わせて変更 ★★★\n",
    "                                            # Azureの場合、ファインチューニング可能なモデルを確認してください。\n",
    "                                            # 例: gpt-35-turbo (バージョン指定が必要な場合あり)\n",
    "                                            #      davinci-002, babbage-002 など (旧世代のモデル)\n",
    "\n",
    "base_data_dir_p98 = '../data/SST-2_data' \n",
    "train_file_path_p98 = os.path.join(base_data_dir_p98, \"SST-2/train.tsv\")\n",
    "dev_file_path_p98 = os.path.join(base_data_dir_p98, \"SST-2/dev.tsv\")\n",
    "\n",
    "if not (os.path.exists(train_file_path_p98) and os.path.exists(dev_file_path_p98)):\n",
    "    print(f\"エラー: SST-2のTSVファイルが見つかりません。\")\n",
    "    raise FileNotFoundError(\"SST-2 TSV files not found.\")\n",
    "\n",
    "# --- ここから問題98の処理 ---\n",
    "\n",
    "# 1. 学習データの準備 (JSONL形式)\n",
    "def create_finetuning_data_jsonl(input_tsv_path, output_jsonl_path):\n",
    "    \"\"\"SST-2のTSVからファインチューニング用JSONLファイルを作成\"\"\"\n",
    "    df = pd.read_csv(input_tsv_path, sep='\\t')\n",
    "    records = []\n",
    "    prompt_prefix = \"以下の映画レビューの感情は「ポジティブ」ですか、それとも「ネガティブ」ですか？ 回答は「ポジティブ」または「ネガティブ」のどちらか一言でお願いします。\\n\\nレビュー: \"\n",
    "    prompt_suffix = \"\\n感情:\"\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        sentence = str(row['sentence'])\n",
    "        label = int(row['label'])\n",
    "        \n",
    "        completion_text = \" ポジティブ\" if label == 1 else \" ネガティブ\" # 先頭にスペース\n",
    "        \n",
    "        prompt_full = f\"{prompt_prefix}{sentence}{prompt_suffix}\"\n",
    "        records.append({\"prompt\": prompt_full, \"completion\": completion_text})\n",
    "        # OpenAIの新しいChat Completions Fine-tuning形式では \"messages\" を使う\n",
    "        # records.append({\"messages\": [\n",
    "        #     {\"role\": \"system\", \"content\": \"You are a sentiment classifier. Respond with 'ポジティブ' or 'ネガティブ'.\"},\n",
    "        #     {\"role\": \"user\", \"content\": f\"レビュー: {sentence}\\n感情:\"},\n",
    "        #     {\"role\": \"assistant\", \"content\": completion_text.strip()} # assistantの回答はスペースなし\n",
    "        # ]})\n",
    "\n",
    "\n",
    "    with open(output_jsonl_path, 'w', encoding='utf-8') as f:\n",
    "        for record in records:\n",
    "            f.write(json.dumps(record, ensure_ascii=False) + '\\n')\n",
    "    print(f\"ファインチューニング用データ '{output_jsonl_path}' を作成しました。 ({len(records)}件)\")\n",
    "    return output_jsonl_path\n",
    "\n",
    "# 学習用JSONLファイルを作成 (SST-2訓練データの一部で試すことを推奨)\n",
    "# 例: df_train.head(100) などで件数を絞る\n",
    "# 全件 (約6万件) で行うと時間とコストがかかります\n",
    "# ここでは、動作確認のため、非常に小さいダミーデータで例示します。\n",
    "# 実際には train_file_path_p98 を使ってください。\n",
    "# ---- ダミーデータ作成の例 (実際にはSST-2 train.tsvを使う) ----\n",
    "dummy_train_data = {\n",
    "    'sentence': [\n",
    "        \"this movie is fantastic and amazing\", \n",
    "        \"a truly boring and dull experience\",\n",
    "        \"quite good, I enjoyed it\",\n",
    "        \"I would not recommend this to anyone\"\n",
    "    ],\n",
    "    'label': [1, 0, 1, 0]\n",
    "}\n",
    "df_dummy_train = pd.DataFrame(dummy_train_data)\n",
    "dummy_train_tsv_path = \"dummy_train_for_ft.tsv\"\n",
    "df_dummy_train.to_csv(dummy_train_tsv_path, sep='\\t', index=False)\n",
    "# ---- ダミーデータ作成ここまで ----\n",
    "\n",
    "# 実際に使うファイルパス (最初はダミーで、慣れたら本番データで)\n",
    "# training_file_jsonl_path = create_finetuning_data_jsonl(train_file_path_p98, \"sst2_train_ft.jsonl\")\n",
    "training_file_jsonl_path = create_finetuning_data_jsonl(dummy_train_tsv_path, \"dummy_sst2_train_ft.jsonl\")\n",
    "\n",
    "\n",
    "# 2. 学習ファイルのアップロード (OpenAI APIの例)\n",
    "# Azure OpenAI の場合は、アップロード方法やAPI呼び出しが若干異なる可能性があります。\n",
    "# Azure のドキュメントを参照してください。\n",
    "uploaded_file_id = None\n",
    "if os.path.exists(training_file_jsonl_path):\n",
    "    try:\n",
    "        print(f\"\\n学習ファイル '{training_file_jsonl_path}' をアップロードしています...\")\n",
    "        with open(training_file_jsonl_path, \"rb\") as f:\n",
    "            # OpenAI Python v1.x.x 以降のファイルアップロード\n",
    "            response = client.files.create(file=f, purpose=\"fine-tune\")\n",
    "            uploaded_file_id = response.id\n",
    "        print(f\"ファイルアップロード完了。File ID: {uploaded_file_id}\")\n",
    "    except Exception as e:\n",
    "        print(f\"ファイルアップロード中にエラー: {e}\")\n",
    "else:\n",
    "    print(f\"エラー: 学習ファイル '{training_file_jsonl_path}' が見つかりません。\")\n",
    "\n",
    "# 3. ファインチューニングジョブの作成\n",
    "fine_tuned_model_id = None # ファインチューニング後のモデルIDを格納\n",
    "if uploaded_file_id:\n",
    "    try:\n",
    "        print(f\"\\nファインチューニングジョブを作成しています... (ベースモデル: {model_to_use_for_ft_base}, File ID: {uploaded_file_id})\")\n",
    "        # OpenAI Python v1.x.x 以降\n",
    "        # hyperparameters (エポック数など) も指定可能\n",
    "        # suffix でカスタムモデル名に接尾辞を付けられる\n",
    "        job = client.fine_tuning.jobs.create(\n",
    "            training_file=uploaded_file_id,\n",
    "            model=model_to_use_for_ft_base, # 例: \"gpt-3.5-turbo-0125\", \"babbage-002\", \"davinci-002\"\n",
    "            # hyperparameters={\"n_epochs\": 1}, # エポック数など\n",
    "            # suffix=\"sst2-sentiment\" # カスタムモデル名の接尾辞\n",
    "        )\n",
    "        job_id = job.id\n",
    "        print(f\"ファインチューニングジョブ作成完了。Job ID: {job_id}\")\n",
    "\n",
    "        # 4. ファインチューニングジョブの監視\n",
    "        print(\"\\nジョブのステータスを監視します... (完了まで時間がかかります)\")\n",
    "        while True:\n",
    "            job_status = client.fine_tuning.jobs.retrieve(job_id)\n",
    "            status = job_status.status\n",
    "            print(f\"  現在のジョブステータス: {status} (Job ID: {job_id})\")\n",
    "            if status == \"succeeded\":\n",
    "                fine_tuned_model_id = job_status.fine_tuned_model\n",
    "                print(f\"ファインチューニング成功！ ファインチューニング済みモデルID: {fine_tuned_model_id}\")\n",
    "                break\n",
    "            elif status in [\"failed\", \"cancelled\"]:\n",
    "                print(f\"ファインチューニング失敗またはキャンセル。理由: {job_status.error}\")\n",
    "                break\n",
    "            time.sleep(60) # 60秒ごとにステータス確認\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ファインチューニングジョブの作成または監視中にエラー: {e}\")\n",
    "\n",
    "# 5. ファインチューニング済みモデルの利用 (検証データで評価)\n",
    "if fine_tuned_model_id:\n",
    "    print(f\"\\n--- ファインチューニング済みモデル ({fine_tuned_model_id}) で検証データを評価 ---\")\n",
    "    df_dev_p98 = pd.read_csv(dev_file_path_p98, sep='\\t')\n",
    "    \n",
    "    true_labels_ft_eval = []\n",
    "    predicted_labels_ft_eval = []\n",
    "    \n",
    "    # 問題96の get_sentiment_from_llm を参考に、ファインチューニング済みモデルで予測\n",
    "    # ただし、プロンプトは学習時と同じものを使用\n",
    "    prompt_prefix_eval = \"以下の映画レビューの感情は「ポジティブ」ですか、それとも「ネガティブ」ですか？ 回答は「ポジティブ」または「ネガティブ」のどちらか一言でお願いします。\\n\\nレビュー: \"\n",
    "    prompt_suffix_eval = \"\\n感情:\"\n",
    "\n",
    "    # APIのレート制限を考慮し、一部でテスト推奨\n",
    "    num_eval_samples = len(df_dev_p98) # 全件評価\n",
    "    # num_eval_samples = 20 # テスト用に絞る場合\n",
    "\n",
    "    for index, row in df_dev_p98.head(num_eval_samples).iterrows():\n",
    "        text = str(row['sentence'])\n",
    "        true_label = int(row['label'])\n",
    "        \n",
    "        prompt_for_eval = f\"{prompt_prefix_eval}{text}{prompt_suffix_eval}\"\n",
    "        \n",
    "        try:\n",
    "            # ファインチューニング済みモデルIDを指定してCompletion APIを呼び出す\n",
    "            # ChatCompletion形式のファインチューニングなら ChatCompletion API を使う\n",
    "            # 以前のCompletion形式のファインチューニングなら Completion API (client.completions.create)\n",
    "            # ここでは ChatCompletion 形式を仮定 (ベースモデルがgpt-3.5-turboなどの場合)\n",
    "            completion_eval = client.chat.completions.create(\n",
    "                model=fine_tuned_model_id, # ★★★ ファインチューニング済みモデルID ★★★\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt_for_eval}],\n",
    "                temperature=0.0,\n",
    "                max_tokens=10 \n",
    "            )\n",
    "            response_text = completion_eval.choices[0].message.content.strip().lower()\n",
    "\n",
    "            pred_label_val = -1\n",
    "            if \"ポジティブ\" in response_text or \"positive\" in response_text:\n",
    "                pred_label_val = 1\n",
    "            elif \"ネガティブ\" in response_text or \"negative\" in response_text:\n",
    "                pred_label_val = 0\n",
    "            \n",
    "            if pred_label_val != -1:\n",
    "                true_labels_ft_eval.append(true_label)\n",
    "                predicted_labels_ft_eval.append(pred_label_val)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  評価中のAPI呼び出しエラー (テキスト: \\\"{text[:30]}...\\\"): {e}\")\n",
    "        \n",
    "        if (index + 1) % 20 == 0 or (index + 1) == num_eval_samples:\n",
    "            print(f\"  評価処理中: {index + 1}/{num_eval_samples} 件完了...\")\n",
    "        time.sleep(0.1) # レート制限対策\n",
    "\n",
    "    if true_labels_ft_eval:\n",
    "        accuracy_ft_eval = accuracy_score(true_labels_ft_eval, predicted_labels_ft_eval)\n",
    "        print(f\"\\nファインチューニング済みモデルの検証データにおける正解率: {accuracy_ft_eval:.4f} ({accuracy_ft_eval*100:.2f}%)\")\n",
    "        print(f\"  評価した事例数: {len(true_labels_ft_eval)}\")\n",
    "    else:\n",
    "        print(\"\\n有効な予測が得られず、正解率を計算できませんでした。\")\n",
    "else:\n",
    "    print(\"\\nファインチューニング済みモデルIDが得られなかったため、評価をスキップします。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "4f0St5Ce0l34",
    "tags": []
   },
   "source": [
    "## 99. 選好チューニング\n",
    "\n",
    "問題96のプロンプトに対して、正解の感情ラベルを含むテキストを望ましい応答、間違った感情ラベルを含むテキストを望ましくない応答として、事前学習済み言語モデルを選好チューニング (preference tuning) を実施せよ。選好チューニングのアルゴリズムとしては、近傍方策最適化 (PPO: Proximal Policy Optimization) や直接選好最適化 (DPO: Direct Preference Optimization) などが考えられる。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.11.10 environment at: /Users/ryuichi/.venv\u001b[0m\n",
      "\u001b[2K\u001b[37m⠙\u001b[0m \u001b[2mResolving dependencies...                                                     \u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[2mResolved \u001b[1m51 packages\u001b[0m \u001b[2min 676ms\u001b[0m\u001b[0m                                        \u001b[0m\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m20 packages\u001b[0m \u001b[2min 13.98s\u001b[0m\u001b[0m                                           \n",
      "\u001b[2mUninstalled \u001b[1m1 package\u001b[0m \u001b[2min 24ms\u001b[0m\u001b[0m\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m20 packages\u001b[0m \u001b[2min 122ms\u001b[0m\u001b[0m                              \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1maccelerate\u001b[0m\u001b[2m==1.6.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1maiohappyeyeballs\u001b[0m\u001b[2m==2.6.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1maiohttp\u001b[0m\u001b[2m==3.11.18\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1maiosignal\u001b[0m\u001b[2m==1.3.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mbitsandbytes\u001b[0m\u001b[2m==0.42.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mdatasets\u001b[0m\u001b[2m==3.6.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mdill\u001b[0m\u001b[2m==0.3.8\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfrozenlist\u001b[0m\u001b[2m==1.6.0\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mfsspec\u001b[0m\u001b[2m==2025.3.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfsspec\u001b[0m\u001b[2m==2025.3.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmarkdown-it-py\u001b[0m\u001b[2m==3.0.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmdurl\u001b[0m\u001b[2m==0.1.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmultidict\u001b[0m\u001b[2m==6.4.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmultiprocess\u001b[0m\u001b[2m==0.70.16\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpeft\u001b[0m\u001b[2m==0.15.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpropcache\u001b[0m\u001b[2m==0.3.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpyarrow\u001b[0m\u001b[2m==20.0.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mrich\u001b[0m\u001b[2m==14.0.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtrl\u001b[0m\u001b[2m==0.17.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mxxhash\u001b[0m\u001b[2m==3.5.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1myarl\u001b[0m\u001b[2m==1.20.0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install transformers torch datasets trl peft accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments\n",
    "from trl import DPOTrainer, DPOConfig\n",
    "from peft import LoraConfig # LoRAを使う場合\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# --- 0. 基本設定と前提変数の確認 ---\n",
    "# model_name_dpo_base = \"gpt2\"  # 例: GPT-2 (より高性能なオープンモデルを推奨)\n",
    "# model_name_dpo_base = \"distilgpt2\" # 小さなモデルで試す場合\n",
    "model_name_dpo_base = \"EleutherAI/gpt-neo-125M\" # より新しい小さめのオープンモデルの例\n",
    "\n",
    "# SST-2データパス (問題85, 97などと同様)\n",
    "base_data_dir_p99 = '../data/SST-2_data' \n",
    "train_file_path_p99 = os.path.join(base_data_dir_p99, \"SST-2/train.tsv\")\n",
    "dev_file_path_p99 = os.path.join(base_data_dir_p99, \"SST-2/dev.tsv\")\n",
    "\n",
    "if not os.path.exists(train_file_path_p99):\n",
    "    print(f\"エラー: 訓練データファイル '{train_file_path_p99}' が見つかりません。\")\n",
    "    raise FileNotFoundError(\"SST-2 train.tsv not found.\")\n",
    "\n",
    "# --- 1. モデルとトークナイザのロード ---\n",
    "print(f\"ベースモデル '{model_name_dpo_base}' とトークナイザをロード中...\")\n",
    "tokenizer_dpo = AutoTokenizer.from_pretrained(model_name_dpo_base)\n",
    "if tokenizer_dpo.pad_token is None:\n",
    "    tokenizer_dpo.pad_token = tokenizer_dpo.eos_token # GPT系ではよくある設定\n",
    "    print(\"pad_token を eos_token に設定しました。\")\n",
    "\n",
    "# DPOでは通常、AutoModelForCausalLM を使う (生成モデルとして)\n",
    "# LoRAを使う場合は、ここでベースモデルをロードし、後でPeftModelでラップする\n",
    "# 量子化などを行う場合は、bitsandbytesの設定もここで行う\n",
    "model_dpo = AutoModelForCausalLM.from_pretrained(model_name_dpo_base)\n",
    "# ref_model = AutoModelForCausalLM.from_pretrained(model_name_dpo_base) # 参照モデル (DPOTrainerがNoneなら内部作成)\n",
    "ref_model = None # DPOTrainerに内部作成を任せる場合\n",
    "\n",
    "print(\"モデルとトークナイザのロード完了。\")\n",
    "\n",
    "# --- 2. 選好データセットの準備 ---\n",
    "print(\"\\n選好データセットを準備中...\")\n",
    "df_train_p99 = pd.read_csv(train_file_path_p99, sep='\\t')\n",
    "\n",
    "# データセットを大幅に削減してテスト (実際の学習では全件または十分な量を使用)\n",
    "# df_train_p99 = df_train_p99.sample(n=1000, random_state=42) # 例: 1000件に削減\n",
    "df_train_p99 = df_train_p99.head(100) # さらに小さくしてテスト\n",
    "\n",
    "preference_data = []\n",
    "prompt_template_dpo = \"以下の映画レビューの感情を「ポジティブ」または「ネガティブ」で答えてください。\\nレビュー: {review}\\n感情:\"\n",
    "\n",
    "for _, row in df_train_p99.iterrows():\n",
    "    review_text = str(row['sentence'])\n",
    "    true_label = int(row['label'])\n",
    "    \n",
    "    prompt_filled = prompt_template_dpo.format(review=review_text)\n",
    "    \n",
    "    if true_label == 1: # ポジティブが望ましい\n",
    "        chosen_completion = \" ポジティブ\" # 先頭にスペース\n",
    "        rejected_completion = \" ネガティブ\"\n",
    "    else: # ネガティブが望ましい\n",
    "        chosen_completion = \" ネガティブ\"\n",
    "        rejected_completion = \" ポジティブ\"\n",
    "        \n",
    "    preference_data.append({\n",
    "        \"prompt\": prompt_filled,\n",
    "        \"chosen\": chosen_completion, # モデルに生成させたい「良い」応答\n",
    "        \"rejected\": rejected_completion # モデルに生成してほしくない「悪い」応答\n",
    "    })\n",
    "\n",
    "# Hugging Face Dataset形式に変換\n",
    "# DPOTrainer は 'prompt', 'chosen', 'rejected' というキーを期待する\n",
    "# また、入力はトークナイズされていないテキストで良い (DPOTrainerが内部でトークナイズ)\n",
    "train_pref_dataset = Dataset.from_list(preference_data)\n",
    "print(f\"選好データセット準備完了。事例数: {len(train_pref_dataset)}\")\n",
    "print(\"選好データの例:\")\n",
    "print(train_pref_dataset[0])\n",
    "\n",
    "\n",
    "# --- 3. (任意) PEFT (LoRA) の設定 ---\n",
    "# LoRAを使うと、フルファインチューニングより少ない計算資源で済むことが多い\n",
    "use_lora = True # LoRAを使用するかどうか\n",
    "if use_lora:\n",
    "    peft_config = LoraConfig(\n",
    "        r=16,  # LoRAランク\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        # target_modules=['q_proj', 'v_proj'] # モデルによって対象モジュール名は異なる。自動検出も試みる。\n",
    "        # target_modules=\"all-linear\" # trlがサポートする便利な指定方法\n",
    "    )\n",
    "    print(\"\\nLoRA設定を準備しました。\")\n",
    "else:\n",
    "    peft_config = None\n",
    "\n",
    "\n",
    "# --- 4. DPOTrainer の設定と学習 ---\n",
    "# DPOConfig または TrainingArguments を使用\n",
    "# DPOConfig は DPOTrainer に特化した引数を持ち、内部で TrainingArguments をラップする\n",
    "output_dir_dpo = \"./dpo_sentiment_model\"\n",
    "\n",
    "# DPOTrainer 用の設定 (trl 0.8.0 以降など、バージョンによって引数が変わる可能性あり)\n",
    "# DPOConfigはTrainingArgumentsを継承しないので、両方設定するか、\n",
    "# DPOTrainerにTrainingArgumentsを直接渡す古いスタイルもある。\n",
    "# ここではDPOConfigを直接使う新しいスタイルを試みる（trlのバージョンに依存）\n",
    "\n",
    "# まずはTrainingArgumentsで基本的な学習設定\n",
    "# training_args = TrainingArguments(\n",
    "#     per_device_train_batch_size=2, # GPUメモリに応じて調整\n",
    "#     gradient_accumulation_steps=4, # 実質的なバッチサイズを増やす\n",
    "#     learning_rate=1e-5, # DPOではSFTより少し高めも試される\n",
    "#     num_train_epochs=1, # 選好学習は少ないエポックでも効果が出ることがある\n",
    "#     logging_steps=10,\n",
    "#     output_dir=output_dir_dpo,\n",
    "#     # optim=\"adamw_torch\", # 推奨\n",
    "#     remove_unused_columns=False, # DPOTrainerが'prompt','chosen','rejected'以外の列を扱うため\n",
    "#     # report_to=\"tensorboard\", # tensorboardなどでログを見る場合\n",
    "# )\n",
    "\n",
    "# DPOTrainerの初期化 (trl 0.8.0以降を想定)\n",
    "# 最新のtrlではDPOTrainerのコンストラクタ引数が変わっている可能性があるため、\n",
    "# 公式ドキュメントを参照するのが最も確実です。\n",
    "# beta, loss_type, max_prompt_length, max_length, max_target_length などが重要な引数。\n",
    "try:\n",
    "    dpo_trainer = DPOTrainer(\n",
    "        model=model_dpo,\n",
    "        ref_model=ref_model, # Noneにすると内部でコピーが作成される\n",
    "        args=TrainingArguments( # DPOTrainerにはTrainingArgumentsを渡す\n",
    "            output_dir=output_dir_dpo,\n",
    "            per_device_train_batch_size=1, # メモリに応じて調整 (LoRAならもう少し増やせるかも)\n",
    "            gradient_accumulation_steps=4,\n",
    "            learning_rate=1.0e-5, # DPOでは比較的小さな学習率が良いとされることが多い\n",
    "            num_train_epochs=1,   # DPOは少ないエポックで効果が出やすい\n",
    "            logging_steps=10,\n",
    "            remove_unused_columns=False, # 'prompt', 'chosen', 'rejected' を使うため\n",
    "            # bf16=True, # 対応GPUならbf16で高速化・省メモリ化\n",
    "            # report_to=\"none\", # tensorboardなど使わない場合\n",
    "        ),\n",
    "        beta=0.1, # DPO損失のβパラメータ (0.1 ~ 0.5程度が多い)\n",
    "        train_dataset=train_pref_dataset,\n",
    "        tokenizer=tokenizer_dpo,\n",
    "        peft_config=peft_config if use_lora else None, # LoRAを使う場合\n",
    "        max_prompt_length=128, # プロンプトの最大長\n",
    "        max_length=256,        # プロンプト＋生成の最大長 (max_prompt_length + max_target_length)\n",
    "        # max_target_length=128 # 生成部分の最大長 (max_lengthからmax_prompt_lengthを引いたもの)\n",
    "    )\n",
    "    print(\"\\nDPOTrainerを初期化しました。学習を開始します...\")\n",
    "    # 学習の実行\n",
    "    dpo_trainer.train()\n",
    "    print(\"DPO学習が完了しました。\")\n",
    "\n",
    "    # モデルの保存 (LoRAを使っている場合はアダプタのみ保存されることが多い)\n",
    "    dpo_trainer.save_model(os.path.join(output_dir_dpo, \"final_checkpoint\"))\n",
    "    tokenizer_dpo.save_pretrained(os.path.join(output_dir_dpo, \"final_checkpoint\"))\n",
    "    print(f\"学習済みモデルとトークナイザを '{os.path.join(output_dir_dpo, 'final_checkpoint')}' に保存しました。\")\n",
    "    \n",
    "    # 学習後のモデルで評価 (問題96と同様のやり方で)\n",
    "    # model_dpo (または dpo_trainer.model) を使って検証データで正解率を測定\n",
    "    # (この部分は長くなるので骨子のみ)\n",
    "    print(\"\\n--- DPO学習後のモデルで検証データ評価 ---\")\n",
    "    # df_dev_p99 = pd.read_csv(dev_file_path_p99, sep='\\t')\n",
    "    # (問題96の get_sentiment_from_llm のような関数を、\n",
    "    #  Hugging Faceモデルでテキスト生成するように修正して評価する)\n",
    "    # 例:\n",
    "    # model_to_eval = dpo_trainer.model # または AutoModelForCausalLM.from_pretrained でロード\n",
    "    # model_to_eval.eval()\n",
    "    # model_to_eval.to(device) # GPU使うなら\n",
    "    # ... (devデータの各文でプロンプト作成 -> model.generate() -> 応答抽出 -> ラベル比較 -> 正解率計算) ...\n",
    "    print(\"（評価部分は別途実装が必要です。問題96のプロンプトベース評価や、\")\n",
    "    print(\" 問題98のファインチューニング後評価のロジックを参考に、\")\n",
    "    print(\" Hugging Faceモデルでのテキスト生成と応答解釈を行ってください。）\")\n",
    "\n",
    "\n",
    "except ImportError as e:\n",
    "    print(f\"ImportError: {e}. 必要なライブラリ (trl, peft, accelerateなど) がインストールされているか確認してください。\")\n",
    "    print(\"`pip install trl peft accelerate bitsandbytes` を試してください。\")\n",
    "except Exception as e:\n",
    "    print(f\"DPOTrainerの初期化または学習中にエラーが発生しました: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0a56790cc58c4155bb4fb62352fe6853": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0c4e978f772946f293451de4484720ed": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_da96a5f6901649958e682be81f939149",
      "placeholder": "​",
      "style": "IPY_MODEL_3f9c4fcaa07b42a0971b99205dd05458",
      "value": " 1/1 [00:00&lt;00:00, 25.67it/s]"
     }
    },
    "0f1172658036407993eb46da17554501": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bd75431e52b54d9bb2fd7c06737c28f5",
      "placeholder": "​",
      "style": "IPY_MODEL_12c4ec93b2fc4b239b775d5b947b253f",
      "value": " 67349/0 [00:00&lt;00:00, 674165.72 examples/s]"
     }
    },
    "12c4ec93b2fc4b239b775d5b947b253f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1411c82bf2f3475a826a145fb9f89626": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "145b34f60df1429e8685bcc9b2f05be2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6c8a470363f246d692c48ab4a502c4c7",
      "max": 872,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ae5b03ae867f4ffba662a8888b319926",
      "value": 872
     }
    },
    "14c91ad60bc84fb0a9c6cc1eb9365425": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2b91de93f8ec46c2ac153f40f50ae2e0",
      "placeholder": "​",
      "style": "IPY_MODEL_f4367d9584aa42e0b4249b33af0435d9",
      "value": "Generating train split: "
     }
    },
    "16763c09bfa34f03821316fe7c9902e7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c57bd417124c4faa8652b13e826405db",
       "IPY_MODEL_3248b0100e614e2a89b0e2dc13be0ad5",
       "IPY_MODEL_936c040e05bc4b2aadd82245d0c2f3b3"
      ],
      "layout": "IPY_MODEL_a1aac38df0bd43bf9a48a55be029f499"
     }
    },
    "1860eb6e26ca45aeab8ab4ab41334a25": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "29245bfd86f0460ea2bfccaca75690a1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8e33f53fccce4875bbb245afe5b87cf3",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5cbbe2769e024a92a5aa718d8f889d0f",
      "value": 1
     }
    },
    "2b91de93f8ec46c2ac153f40f50ae2e0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3248b0100e614e2a89b0e2dc13be0ad5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_93d07d6d16da45b2a248ad37e51c3180",
      "max": 67349,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a03ab49faf2a4027906a0e9f806bd2db",
      "value": 67349
     }
    },
    "3c41fe7af38f49f581ae359b5d970d23": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3f95aef5e2c74acd8ffbfad2543b2102",
      "placeholder": "​",
      "style": "IPY_MODEL_9a20391390dd4f96aaea37321b4dfb53",
      "value": " 872/0 [00:00&lt;00:00, 66375.69 examples/s]"
     }
    },
    "3f6a346c2abd42ea9fb5af9441376472": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_97484a182f634ec8a711d516fdeb0c63",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d7453ed4be064ab586858e8dec61ffae",
      "value": 1
     }
    },
    "3f95aef5e2c74acd8ffbfad2543b2102": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3f9c4fcaa07b42a0971b99205dd05458": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4b8bf221e7974b65b4dde96219ce0f47": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4bc6646a088f4bd7b58b33ae1bbab1ae": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4d1eef416f5944c2911154fb770d869f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "54587b2141984fd495bf55ea6df0d1a5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d603aabb8b6443d2a66b94701858f523",
      "placeholder": "​",
      "style": "IPY_MODEL_1411c82bf2f3475a826a145fb9f89626",
      "value": "Generating dev split: "
     }
    },
    "5cbbe2769e024a92a5aa718d8f889d0f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "64ebcabb9c26423f8cda28966d9bbedb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "678df530cc994774899eb213581f737b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6c8a470363f246d692c48ab4a502c4c7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "75a29e01568b4272832f3e00dd92a707": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "80837417e00b4fd3814524786898697c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "81b067d9308141c786bc70f709681ce7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_54587b2141984fd495bf55ea6df0d1a5",
       "IPY_MODEL_29245bfd86f0460ea2bfccaca75690a1",
       "IPY_MODEL_3c41fe7af38f49f581ae359b5d970d23"
      ],
      "layout": "IPY_MODEL_4bc6646a088f4bd7b58b33ae1bbab1ae"
     }
    },
    "8dec4107d6a44cdaae9020345e2c25a4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8e33f53fccce4875bbb245afe5b87cf3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "936c040e05bc4b2aadd82245d0c2f3b3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_64ebcabb9c26423f8cda28966d9bbedb",
      "placeholder": "​",
      "style": "IPY_MODEL_678df530cc994774899eb213581f737b",
      "value": " 67349/67349 [00:03&lt;00:00, 20324.02 examples/s]"
     }
    },
    "93d07d6d16da45b2a248ad37e51c3180": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "97484a182f634ec8a711d516fdeb0c63": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "9a20391390dd4f96aaea37321b4dfb53": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a03ab49faf2a4027906a0e9f806bd2db": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a1aac38df0bd43bf9a48a55be029f499": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a3164d72e2d5431dbd7921b74290193a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a3cd48beab0741b99a7885f89ebe1181": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a8eb7b58bb3a4bfc9d4dad2bc0857dbb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f4da859c892b42cfa6c909e7dfe2df7d",
       "IPY_MODEL_df8979acad044b40bb55c3e00c418e05",
       "IPY_MODEL_0c4e978f772946f293451de4484720ed"
      ],
      "layout": "IPY_MODEL_b913e33aebe642cca6fa8a6ade682988"
     }
    },
    "ae5b03ae867f4ffba662a8888b319926": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b1763f6b7fc9430ab2f773279fdd954d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_75a29e01568b4272832f3e00dd92a707",
      "placeholder": "​",
      "style": "IPY_MODEL_8dec4107d6a44cdaae9020345e2c25a4",
      "value": "Map: 100%"
     }
    },
    "b913e33aebe642cca6fa8a6ade682988": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bd75431e52b54d9bb2fd7c06737c28f5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c0929c9ec77a48df8a7fd3e2f4539a51": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c57bd417124c4faa8652b13e826405db": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1860eb6e26ca45aeab8ab4ab41334a25",
      "placeholder": "​",
      "style": "IPY_MODEL_4b8bf221e7974b65b4dde96219ce0f47",
      "value": "Map: 100%"
     }
    },
    "d5e5c6c318cc41c4ac40647e6d5a5106": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_14c91ad60bc84fb0a9c6cc1eb9365425",
       "IPY_MODEL_3f6a346c2abd42ea9fb5af9441376472",
       "IPY_MODEL_0f1172658036407993eb46da17554501"
      ],
      "layout": "IPY_MODEL_0a56790cc58c4155bb4fb62352fe6853"
     }
    },
    "d603aabb8b6443d2a66b94701858f523": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d7453ed4be064ab586858e8dec61ffae": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "da96a5f6901649958e682be81f939149": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "deee5780a392426096c2ff4f76aa1547": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "df8979acad044b40bb55c3e00c418e05": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ec97a156272345ba8d456c5f090d9412",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_deee5780a392426096c2ff4f76aa1547",
      "value": 1
     }
    },
    "e92b3187acda44e9b30051a9e410f43f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b1763f6b7fc9430ab2f773279fdd954d",
       "IPY_MODEL_145b34f60df1429e8685bcc9b2f05be2",
       "IPY_MODEL_fc589f87f65d4a3196cdd790bbdf7fa0"
      ],
      "layout": "IPY_MODEL_c0929c9ec77a48df8a7fd3e2f4539a51"
     }
    },
    "ec97a156272345ba8d456c5f090d9412": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f4367d9584aa42e0b4249b33af0435d9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f4da859c892b42cfa6c909e7dfe2df7d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4d1eef416f5944c2911154fb770d869f",
      "placeholder": "​",
      "style": "IPY_MODEL_a3164d72e2d5431dbd7921b74290193a",
      "value": "100%"
     }
    },
    "fc589f87f65d4a3196cdd790bbdf7fa0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a3cd48beab0741b99a7885f89ebe1181",
      "placeholder": "​",
      "style": "IPY_MODEL_80837417e00b4fd3814524786898697c",
      "value": " 872/872 [00:00&lt;00:00, 9939.81 examples/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
