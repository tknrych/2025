{"cells":[{"cell_type":"markdown","source":["## 事前準備"],"metadata":{"id":"8r-Pl3ZtIlRL"},"id":"8r-Pl3ZtIlRL"},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"I3Ag9AgXIlvc"},"id":"I3Ag9AgXIlvc","execution_count":null,"outputs":[]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/nlp100_2025/ja/\n","\n","import os\n","\n","current_directory = os.getcwd()\n","print(f\"The Current Directory: {current_directory}\")"],"metadata":{"id":"drOq31oNIldT"},"id":"drOq31oNIldT","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"8df29512-a5c0-4e06-8c53-91eaab143762","metadata":{"editable":true,"id":"8df29512-a5c0-4e06-8c53-91eaab143762","tags":[]},"source":["# 第6章: 単語ベクトル"]},{"cell_type":"markdown","id":"b05585fe-de49-4474-b2f9-57888e067319","metadata":{"id":"b05585fe-de49-4474-b2f9-57888e067319"},"source":["単語の意味を実ベクトルで表現する単語ベクトル（単語埋め込み）に関して、以下の処理を行うプログラムを作成せよ。"]},{"cell_type":"markdown","id":"c0afdc8e-57f5-4388-a914-1150d846d5da","metadata":{"id":"c0afdc8e-57f5-4388-a914-1150d846d5da"},"source":["## 50. 単語ベクトルの読み込みと表示\n","\n","Google Newsデータセット（約1,000億単語）での[学習済み単語ベクトル](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing)（300万単語・フレーズ、300次元）をダウンロードし、\"United States\"の単語ベクトルを表示せよ。ただし、\"United States\"は内部的には\"United_States\"と表現されていることに注意せよ。"]},{"cell_type":"code","execution_count":null,"id":"662cd1a8-e2d6-4260-9265-cffd152c9d36","metadata":{"id":"662cd1a8-e2d6-4260-9265-cffd152c9d36"},"outputs":[],"source":["!uv pip install numpy==1.26.4\n","!uv pip install gensim"]},{"cell_type":"code","execution_count":null,"id":"19984e30-41f9-4e50-86ea-dd58c166bb1c","metadata":{"id":"19984e30-41f9-4e50-86ea-dd58c166bb1c"},"outputs":[],"source":["from gensim.models import KeyedVectors\n","\n","model_path = '../data/GoogleNews-vectors-negative300.bin.gz'\n","\n","try:\n","    # モデルの読み込み (時間がかかることがあります)\n","    print(\"単語ベクトルモデルを読み込んでいます... (数分かかる場合があります)\")\n","    word_vectors = KeyedVectors.load_word2vec_format(model_path, binary=True)\n","    print(\"モデルの読み込みが完了しました。\")\n","\n","    # \"United States\" (内部表現 \"United_States\") の単語ベクトルを取得\n","    target_word = \"United_States\"\n","\n","    if target_word in word_vectors:\n","        vector_united_states = word_vectors[target_word]\n","        print(f\"\\n単語 '{target_word}' のベクトル:\")\n","        print(vector_united_states)\n","        print(f\"\\nベクトルの次元数: {len(vector_united_states)}\")\n","    else:\n","        print(f\"エラー: 単語 '{target_word}' はボキャブラリ内に見つかりませんでした。\")\n","        print(f\"代わりに 'United States' で試してみます...\")\n","        target_word_alt = \"United States\" # スペース区切りも試す (通常は _ 区切り)\n","        if target_word_alt in word_vectors:\n","            vector_united_states = word_vectors[target_word_alt]\n","            print(f\"\\n単語 '{target_word_alt}' のベクトル:\")\n","            print(vector_united_states)\n","            print(f\"\\nベクトルの次元数: {len(vector_united_states)}\")\n","        else:\n","            print(f\"エラー: 単語 '{target_word_alt}' もボキャブラリ内に見つかりませんでした。\")\n","\n","\n","except FileNotFoundError:\n","    print(f\"エラー: 指定されたパスにファイルが見つかりません: {model_path}\")\n","    print(\"Google Newsの単語ベクトルファイルをダウンロードし、正しいパスを指定してください。\")\n","except Exception as e:\n","    print(f\"モデルの読み込み中またはベクトル取得中にエラーが発生しました: {e}\")"]},{"cell_type":"markdown","id":"c4e3bb10-c37f-4395-9397-ac71e36bf4ed","metadata":{"id":"c4e3bb10-c37f-4395-9397-ac71e36bf4ed"},"source":["## 51. 単語の類似度\n","\n","\"United States\"と\"U.S.\"のコサイン類似度を計算せよ。"]},{"cell_type":"code","execution_count":null,"id":"f8d4fff1-0947-462d-99df-4e6441381f4b","metadata":{"id":"f8d4fff1-0947-462d-99df-4e6441381f4b"},"outputs":[],"source":["if 'word_vectors' not in locals() or word_vectors is None:\n","    print(\"エラー: 単語ベクトルモデル 'word_vectors' が読み込まれていません。\")\n","    print(\"問題50のコードを実行して、先にモデルをロードしてください。\")\n","else:\n","    word1 = \"United_States\"  # 問題50で確認した表現\n","    word2 = \"U.S.\"          # \"U.S.\" の表現\n","\n","    try:\n","        # まず、各単語がボキャブラリに存在するか確認\n","        if word1 not in word_vectors:\n","            print(f\"警告: 単語 '{word1}' がボキャブラリにありません。\")\n","\n","        # 両方の単語がボキャブラリに存在する場合のみ類似度を計算\n","        if word1 in word_vectors and word2 in word_vectors:\n","            similarity = word_vectors.similarity(word1, word2)\n","            print(f\"'{word1}' と '{word2}' のコサイン類似度: {similarity:.4f}\")\n","        else:\n","            print(f\"必要な単語の少なくとも一方がボキャブラリにないため、類似度を計算できませんでした。\")\n","\n","    except KeyError as e:\n","        print(f\"エラー: 単語 '{e}' がボキャブラリ内に見つかりませんでした。\")\n","    except Exception as e:\n","        print(f\"類似度の計算中にエラーが発生しました: {e}\")"]},{"cell_type":"markdown","id":"36314a02-d3d8-4121-b310-e800b2d1ce3e","metadata":{"id":"36314a02-d3d8-4121-b310-e800b2d1ce3e"},"source":["## 52. 類似度の高い単語10件\n","\n","\"United States\"とコサイン類似度が高い10語と、その類似度を出力せよ。"]},{"cell_type":"code","execution_count":null,"id":"a0d7221c-e4fb-49b5-9f0c-e224650c3e6b","metadata":{"id":"a0d7221c-e4fb-49b5-9f0c-e224650c3e6b"},"outputs":[],"source":["if 'word_vectors' not in locals() or word_vectors is None:\n","    print(\"エラー: 単語ベクトルモデル 'word_vectors' が読み込まれていません。\")\n","    print(\"問題50のコードを実行して、先にモデルをロードしてください。\")\n","else:\n","    target_word = \"United_States\"\n","\n","    try:\n","        if target_word in word_vectors:\n","            # 類似度の高い単語トップ10を取得\n","            similar_words = word_vectors.most_similar(positive=[target_word], topn=10)\n","\n","            print(f\"'{target_word}' とコサイン類似度が高い単語トップ10:\")\n","            for word, similarity in similar_words:\n","                print(f\"- {word}: {similarity:.4f}\")\n","        else:\n","            print(f\"エラー: 単語 '{target_word}' はボキャブラリ内に見つかりませんでした。\")\n","\n","    except KeyError as e:\n","        # most_similar内で単語が見つからない場合もKeyErrorになることがある\n","        print(f\"エラー: 単語 '{e}' がボキャブラリ内に見つかりませんでした。\")\n","    except Exception as e:\n","        print(f\"類似度の高い単語の取得中にエラーが発生しました: {e}\")"]},{"cell_type":"markdown","id":"6de3ff5c-5285-4e96-b8d7-4f995635d146","metadata":{"id":"6de3ff5c-5285-4e96-b8d7-4f995635d146"},"source":["## 53. 加法構成性によるアナロジー\n","\n","\"Spain\"の単語ベクトルから\"Madrid\"のベクトルを引き、\"Athens\"のベクトルを足したベクトルを計算し、そのベクトルと類似度の高い10語とその類似度を出力せよ。"]},{"cell_type":"code","execution_count":null,"id":"48ab430b-eeb5-4e4d-a455-17a5ef188a88","metadata":{"id":"48ab430b-eeb5-4e4d-a455-17a5ef188a88"},"outputs":[],"source":["if 'word_vectors' not in locals() or word_vectors is None:\n","    print(\"エラー: 単語ベクトルモデル 'word_vectors' が読み込まれていません。\")\n","    print(\"問題50のコードを実行して、先にモデルをロードしてください。\")\n","else:\n","    word_spain = \"Spain\"\n","    word_madrid = \"Madrid\"\n","    word_athens = \"Athens\"\n","\n","    try:\n","        # 必要な単語がボキャブラリに存在するか確認\n","        required_words = [word_spain, word_madrid, word_athens]\n","        missing_words = [word for word in required_words if word not in word_vectors]\n","\n","        if missing_words:\n","            print(f\"エラー: ボキャブラリに存在しない単語があります: {', '.join(missing_words)}\")\n","        else:\n","            # most_similar を使ってアナロジー計算と類似単語の取得を一度に行う\n","            # calculation: vec(Spain) - vec(Madrid) + vec(Athens)\n","            # most_similar(positive=['Spain', 'Athens'], negative=['Madrid'])\n","\n","            similar_words_analogy = word_vectors.most_similar(\n","                positive=[word_spain, word_athens],\n","                negative=[word_madrid],\n","                topn=10\n","            )\n","\n","            print(f\"vec('{word_spain}') - vec('{word_madrid}') + vec('{word_athens}') に類似する単語トップ10:\")\n","            for word, similarity in similar_words_analogy:\n","                print(f\"- {word}: {similarity:.4f}\")\n","\n","    except KeyError as e:\n","        print(f\"エラー: 単語 '{e}' がボキャブラリ内に見つかりませんでした（most_similarの内部で発生）。\")\n","    except Exception as e:\n","        print(f\"アナロジー計算または類似単語の取得中にエラーが発生しました: {e}\")"]},{"cell_type":"markdown","id":"d5db38ad-b7f4-40b2-a068-0dea071d23e7","metadata":{"id":"d5db38ad-b7f4-40b2-a068-0dea071d23e7"},"source":["## 54. アナロジーデータでの実験\n","\n","[単語アナロジーの評価データ](http://download.tensorflow.org/data/questions-words.txt)をダウンロードし、国と首都に関する事例（`: capital-common-countries`セクション）に対して、vec(2列目の単語) - vec(1列目の単語) + vec(3列目の単語)を計算し、そのベクトルと類似度が最も高い単語と、その類似度を求めよ。求めた単語と類似度は、各事例と一緒に記録せよ。"]},{"cell_type":"code","execution_count":null,"id":"db0dbdd6-1186-47ed-bab0-5672ee8f91c5","metadata":{"id":"db0dbdd6-1186-47ed-bab0-5672ee8f91c5"},"outputs":[],"source":["import requests # ファイルダウンロード用\n","\n","# word_vectors は問題50で読み込まれた KeyedVectors オブジェクトとします。\n","if 'word_vectors' not in locals() or word_vectors is None:\n","    print(\"エラー: 単語ベクトルモデル 'word_vectors' が読み込まれていません。\")\n","    print(\"問題50のコードを実行して、先にモデルをロードしてください。\")\n","    # このセル以降の処理を中断するために、ここで None を代入するなどの処置をしてもよい\n","    # word_vectors = None\n","else:\n","    # 1. 評価データのダウンロード\n","    url = \"http://download.tensorflow.org/data/questions-words.txt\"\n","    file_name = \"questions-words.txt\"\n","\n","    try:\n","        response = requests.get(url)\n","        response.raise_for_status() # エラーがあればここで例外発生\n","        with open(file_name, 'wb') as f:\n","            f.write(response.content)\n","        print(f\"'{file_name}' をダウンロードしました。\")\n","    except requests.exceptions.RequestException as e:\n","        print(f\"ファイルのダウンロード中にエラーが発生しました: {e}\")\n","        file_name = None # エラーの場合はファイル名をNoneに\n","\n","    if file_name:\n","        analogy_results = []\n","        target_section = \": capital-common-countries\"\n","        in_section = False\n","\n","        try:\n","            with open(file_name, 'r', encoding='utf-8') as f:\n","                for line in f:\n","                    line = line.strip()\n","                    if line.startswith(\":\"):\n","                        in_section = (line == target_section)\n","                        if in_section:\n","                            print(f\"\\nセクション '{target_section}' の処理を開始します...\")\n","                        elif line.startswith(\":\") and in_section: # 他のセクションが始まったら終了\n","                            print(f\"セクション '{target_section}' の処理を終了します。\")\n","                            break\n","                        continue\n","\n","                    if in_section:\n","                        words = line.split()\n","                        if len(words) == 4:\n","                            w1, w2, w3, w4_true = words[0], words[1], words[2], words[3]\n","\n","                            # 使用する単語がボキャブラリに存在するか確認\n","                            # most_similar に渡す単語 (w1, w2, w3)\n","                            analogy_input_words = [w2, w3, w1]\n","                            missing_input_words = [word for word in analogy_input_words if word not in word_vectors]\n","\n","                            if missing_input_words:\n","                                # print(f\"  スキップ (入力単語不足): {line} - 不足: {', '.join(missing_input_words)}\")\n","                                continue\n","\n","                            try:\n","                                # vec(w2) - vec(w1) + vec(w3) に最も類似する単語を取得\n","                                # most_similar は [(単語, 類似度), ...] のリストを返す\n","                                result = word_vectors.most_similar(positive=[w2, w3], negative=[w1], topn=1)\n","                                w4_pred, similarity = result[0]\n","\n","                                analogy_results.append({\n","                                    \"w1\": w1, \"w2\": w2, \"w3\": w3, \"w4_true\": w4_true,\n","                                    \"w4_pred\": w4_pred, \"similarity\": similarity\n","                                })\n","                                # 結果を一部表示 (大量に出力されるのを避けるため、ここでは表示しない)\n","                                # print(f\"  {w1} : {w2} :: {w3} : {w4_true}  =>  予測: {w4_pred} ({similarity:.4f})\")\n","\n","                            except KeyError as e:\n","                                # most_similar の中で内部的にKeyErrorが発生する場合もある\n","                                # print(f\"  スキップ (KeyError in most_similar): {line} - 単語: {e}\")\n","                                continue\n","                        else:\n","                            # print(f\"  スキップ (フォーマット不正): {line}\")\n","                            pass # フォーマットが不正な行は無視\n","\n","        except FileNotFoundError:\n","            print(f\"エラー: ファイル '{file_name}' が見つかりません。\")\n","        except Exception as e:\n","            print(f\"ファイル処理またはアナロジー計算中にエラー: {e}\")\n","\n","        if analogy_results:\n","            print(f\"\\n'{target_section}' セクションから {len(analogy_results)} 件のアナロジー結果を記録しました。\")\n","            print(\"最初の5件の結果:\")\n","            for i, res in enumerate(analogy_results[:5]):\n","                print(f\"  {res['w1']} : {res['w2']} :: {res['w3']} : {res['w4_true']}  =>  予測: {res['w4_pred']} ({res['similarity']:.4f})\")\n","\n","            # (オプション) 結果をファイルに保存する場合\n","            # import json\n","            # with open(\"analogy_capital-common-countries_results.json\", \"w\", encoding=\"utf-8\") as outfile:\n","            #    json.dump(analogy_results, outfile, ensure_ascii=False, indent=2)\n","            # print(\"\\n結果を analogy_capital-common-countries_results.json に保存しました。\")\n","        else:\n","            if target_section and not in_section: # target_section自体が見つからなかった場合\n","                 print(f\"エラー: 指定されたセクション '{target_section}' がファイル内に見つかりませんでした。\")\n","            elif file_name: # ファイルはあったが、結果が0件だった場合\n","                 print(\"処理可能なアナロジー事例がありませんでした（ボキャブラリ不足、またはセクション内容なし）。\")"]},{"cell_type":"markdown","id":"dbc1fcff-5555-44da-8b4c-12d97ede10e8","metadata":{"id":"dbc1fcff-5555-44da-8b4c-12d97ede10e8"},"source":["## 55. アナロジータスクでの正解率\n","\n","54の実行結果を用い、意味的アナロジー（semantic analogy）と文法的アナロジー（syntactic analogy）の正解率を測定せよ。"]},{"cell_type":"code","execution_count":null,"id":"968ddc2a-ac16-4133-9c30-168d64f82287","metadata":{"id":"968ddc2a-ac16-4133-9c30-168d64f82287"},"outputs":[],"source":["# analogy_results は問題54で作成されたアナロジー結果のリストとします。\n","# このリストの各要素は以下のような辞書であることを想定しています:\n","# {\n","#     \"w1\": w1, \"w2\": w2, \"w3\": w3, \"w4_true\": w4_true,\n","#     \"w4_pred\": w4_pred, \"similarity\": similarity\n","# }\n","\n","if 'analogy_results' not in locals() or not analogy_results:\n","    print(\"エラー: 'analogy_results' が存在しないか空です。\")\n","    print(\"問題54を先に実行して、アナロジー結果を生成・記録してください。\")\n","else:\n","    correct_predictions = 0\n","    total_cases = len(analogy_results)\n","\n","    for item in analogy_results:\n","        # 予測された単語と正解の単語を比較\n","        # 大文字・小文字を区別して比較する場合 (通常、このデータセットではそのように評価します)\n","        if item['w4_pred'] == item['w4_true']:\n","            correct_predictions += 1\n","        # (オプション) 大文字・小文字を区別しない場合は .lower() を使う\n","        # if item['w4_pred'].lower() == item['w4_true'].lower():\n","        #     correct_predictions += 1\n","\n","    if total_cases > 0:\n","        accuracy = (correct_predictions / total_cases) * 100\n","        print(f\"アナロジーデータセクション: : capital-common-countries (意味的アナロジー)\")\n","        print(f\"処理した事例数: {total_cases}\")\n","        print(f\"正解数: {correct_predictions}\")\n","        print(f\"正解率: {accuracy:.2f}%\")\n","    else:\n","        print(\"評価対象の事例がありませんでした。\")\n"]},{"cell_type":"markdown","id":"b62a9077-45e9-4cf2-8546-5e5b226b4cb6","metadata":{"id":"b62a9077-45e9-4cf2-8546-5e5b226b4cb6"},"source":["## 56. WordSimilarity-353での評価\n","\n","[The WordSimilarity-353 Test Collection](http://www.gabrilovich.com/resources/data/wordsim353/wordsim353.html)の評価データをダウンロードし、単語ベクトルにより計算される類似度のランキングと、人間の類似度判定のランキングの間のスピアマン相関係数を計算せよ。"]},{"cell_type":"code","execution_count":null,"id":"92513c23-d630-41e0-a9be-e029fde7b5e3","metadata":{"id":"92513c23-d630-41e0-a9be-e029fde7b5e3"},"outputs":[],"source":["!uv pip install pandas"]},{"cell_type":"code","execution_count":null,"id":"568b788a-ea9c-4dad-9d19-8eb3bd748fb1","metadata":{"id":"568b788a-ea9c-4dad-9d19-8eb3bd748fb1"},"outputs":[],"source":["import pandas as pd\n","from scipy.stats import spearmanr\n","import requests\n","import zipfile\n","import io\n","\n","# word_vectors は問題50で読み込まれた KeyedVectors オブジェクトとします。\n","if 'word_vectors' not in locals() or word_vectors is None:\n","    print(\"エラー: 単語ベクトルモデル 'word_vectors' が読み込まれていません。\")\n","    print(\"問題50のコードを実行して、先にモデルをロードしてください。\")\n","    # word_vectors = None # 以降の処理を止めるためにNoneを代入\n","else:\n","    # 1. WordSimilarity-353 データセットのダウンロードと展開\n","    ws353_url = \"http://www.gabrilovich.com/resources/data/wordsim353/wordsim353.zip\"\n","    ws353_csv_filename = \"combined.csv\"\n","    df_ws353 = None\n","\n","    try:\n","        print(f\"'{ws353_url}' からWordSimilarity-353データセットをダウンロードしています...\")\n","        response = requests.get(ws353_url)\n","        response.raise_for_status()\n","\n","        with zipfile.ZipFile(io.BytesIO(response.content)) as z:\n","            # zipファイル内の combined.csv を直接pandasで読み込む\n","            if ws353_csv_filename in z.namelist():\n","                with z.open(ws353_csv_filename) as csv_file:\n","                    df_ws353 = pd.read_csv(csv_file)\n","                print(f\"'{ws353_csv_filename}' を読み込みました。\")\n","            else:\n","                print(f\"エラー: zipファイル内に '{ws353_csv_filename}' が見つかりません。\")\n","                # 代替として、もし 'wordsim353.csv' があればそれを使う試み (ファイル名が異なる場合があるため)\n","                alt_csv_filename = \"wordsim353.csv\"\n","                if alt_csv_filename in z.namelist():\n","                    print(f\"代替ファイル '{alt_csv_filename}' を試します...\")\n","                    with z.open(alt_csv_filename) as csv_file:\n","                         df_ws353 = pd.read_csv(csv_file)\n","                    print(f\"'{alt_csv_filename}' を読み込みました。\")\n","                else:\n","                    print(f\"エラー: zipファイル内に '{alt_csv_filename}' も見つかりません。\")\n","\n","    except requests.exceptions.RequestException as e:\n","        print(f\"データセットのダウンロード中にエラーが発生しました: {e}\")\n","    except zipfile.BadZipFile:\n","        print(\"エラー: ダウンロードしたファイルが正しいzipファイルではありません。\")\n","    except Exception as e:\n","        print(f\"データセットの処理中に予期せぬエラーが発生しました: {e}\")\n","\n","    if df_ws353 is not None:\n","        print(\"\\nWordSimilarity-353 データセットの最初の5行:\")\n","        print(df_ws353.head())\n","\n","        human_scores = []\n","        model_scores = []\n","        pairs_processed_count = 0\n","        pairs_skipped_count = 0\n","\n","        # 4. 類似度スコアの収集\n","        # 列名が 'Word 1', 'Word 2', 'Human (mean)' であることを想定\n","        # 実際の列名に合わせて調整が必要な場合があります。\n","        col_word1 = 'Word 1'\n","        col_word2 = 'Word 2'\n","        col_human_score = 'Human (mean)'\n","\n","        # df_ws353.columns を確認して、実際の列名に合わせる\n","        if not {col_word1, col_word2, col_human_score}.issubset(df_ws353.columns):\n","            print(f\"\\nエラー: CSVファイルに必要な列 ('{col_word1}', '{col_word2}', '{col_human_score}') が見つかりません。\")\n","            print(f\"実際の列名: {df_ws353.columns.tolist()}\")\n","            print(\"コード内の col_word1, col_word2, col_human_score 変数を実際の列名に修正してください。\")\n","        else:\n","            for index, row in df_ws353.iterrows():\n","                word1 = row[col_word1]\n","                word2 = row[col_word2]\n","                human_score = row[col_human_score]\n","\n","                # 単語がボキャブラリに存在するか確認\n","                if word1 in word_vectors and word2 in word_vectors:\n","                    model_similarity = word_vectors.similarity(word1, word2)\n","                    human_scores.append(float(human_score)) # human_scoreが文字列の場合があるのでfloatに変換\n","                    model_scores.append(model_similarity)\n","                    pairs_processed_count += 1\n","                else:\n","                    pairs_skipped_count += 1\n","                    # print(f\"スキップ: '{word1}' または '{word2}' がボキャブラリにありません。\")\n","\n","            print(f\"\\n処理した単語ペア数: {pairs_processed_count}\")\n","            print(f\"スキップした単語ペア数 (ボキャブラリ不足): {pairs_skipped_count}\")\n","\n","            if len(human_scores) > 1 and len(model_scores) > 1 : # スピアマン相関は少なくとも2つのデータポイントが必要\n","                # 5. スピアマン相関係数の計算\n","                correlation, p_value = spearmanr(human_scores, model_scores)\n","                print(f\"\\nスピアマン相関係数: {correlation:.4f}\")\n","                print(f\"P値: {p_value:.4g}\") # p_valueは相関の有意性を示す\n","            else:\n","                print(\"\\nスピアマン相関係数を計算するための十分なデータポイントがありません。\")\n","    else:\n","        if df_ws353 is None: # df_ws353がNoneのままなら（読み込み失敗など）\n","            print(\"WordSimilarity-353 データセットを読み込めませんでした。処理を中断します。\")"]},{"cell_type":"markdown","id":"4e45f26a-5896-453b-a11b-a97ef3b3900c","metadata":{"id":"4e45f26a-5896-453b-a11b-a97ef3b3900c"},"source":["## 57. k-meansクラスタリング\n","\n","国名に関する単語ベクトルを抽出し、k-meansクラスタリングをクラスタ数k=5として実行せよ。"]},{"cell_type":"code","execution_count":null,"id":"822b12e4-dc8d-4a30-8b35-46a4b892067e","metadata":{"id":"822b12e4-dc8d-4a30-8b35-46a4b892067e"},"outputs":[],"source":["!uv pip install scikit-learn"]},{"cell_type":"code","execution_count":null,"id":"03bdfca2-7896-49df-9a6a-d521befcddcf","metadata":{"id":"03bdfca2-7896-49df-9a6a-d521befcddcf"},"outputs":[],"source":["import numpy as np\n","from sklearn.cluster import KMeans\n","# pandas は国名リストの取得に使うかもしれない (問題54の結果を利用する場合)\n","# import pandas as pd\n","\n","# word_vectors は問題50で読み込まれた KeyedVectors オブジェクト、\n","# analogy_results は問題54で記録されたアナロジー結果のリストとします。\n","\n","if 'word_vectors' not in locals() or word_vectors is None:\n","    print(\"エラー: 単語ベクトルモデル 'word_vectors' が読み込まれていません。\")\n","    print(\"問題50のコードを実行して、先にモデルをロードしてください。\")\n","elif 'analogy_results' not in locals() or not analogy_results:\n","    print(\"エラー: 'analogy_results' が存在しないか空です。\")\n","    print(\"問題54を先に実行して、アナロジー結果を生成・記録してください。\")\n","else:\n","    # 1. 国名リストの準備 (問題54の結果から抽出)\n","    country_names_from_analogy = set()\n","    for item in analogy_results:\n","        # w2 と w4_true が国名であると仮定 (capital-common-countries セクションの場合)\n","        country_names_from_analogy.add(item['w2'])\n","        country_names_from_analogy.add(item['w4_true'])\n","\n","    print(f\"問題54の結果から抽出したユニークな国名（候補）の数: {len(country_names_from_analogy)}\")\n","\n","    # 2. 単語ベクトルの抽出\n","    country_vectors = []\n","    valid_country_names = [] # ボキャブラリに存在し、ベクトルを抽出できた国名\n","\n","    for country_name in sorted(list(country_names_from_analogy)): # ソートして処理順を一定に\n","        if country_name in word_vectors:\n","            country_vectors.append(word_vectors[country_name])\n","            valid_country_names.append(country_name)\n","        # else:\n","            # print(f\"単語 '{country_name}' はボキャブラリにないためスキップします。\")\n","\n","    if not valid_country_names or not country_vectors:\n","        print(\"エラー: ボキャブラリに存在する国名が見つからなかったか、ベクトルを抽出できませんでした。\")\n","    else:\n","        print(f\"ボキャブラリに存在し、ベクトルを抽出した国名の数: {len(valid_country_names)}\")\n","\n","        # リストをnumpy配列に変換\n","        X_countries = np.array(country_vectors)\n","\n","        # 3. k-meansクラスタリングの実行\n","        k = 5\n","        kmeans = KMeans(n_clusters=k, random_state=0, n_init='auto') # random_stateで結果を固定, n_initで警告抑制\n","        clusters = kmeans.fit_predict(X_countries)\n","\n","        print(f\"\\nk-meansクラスタリング (k={k}) を実行しました。\")\n","\n","        # 4. 結果の確認 (各クラスタに含まれる国名を表示)\n","        print(\"\\n各クラスタに含まれる国名 (最初の数件):\")\n","        for i in range(k):\n","            cluster_members = [valid_country_names[j] for j, label in enumerate(clusters) if label == i]\n","            print(f\"クラスタ {i}:\")\n","            if cluster_members:\n","                # あまりに多い場合は一部だけ表示\n","                print(f\"  メンバー数: {len(cluster_members)}\")\n","                print(f\"  例: {', '.join(cluster_members[:10])}{'...' if len(cluster_members) > 10 else ''}\")\n","            else:\n","                print(\"  メンバーなし\")"]},{"cell_type":"markdown","id":"c63cb16e-7cbe-49e8-8f66-d703600128fa","metadata":{"id":"c63cb16e-7cbe-49e8-8f66-d703600128fa"},"source":["## 58. Ward法によるクラスタリング\n","\n","国名に関する単語ベクトルに対し、Ward法による階層型クラスタリングを実行せよ。さらに、クラスタリング結果をデンドログラムとして可視化せよ。"]},{"cell_type":"code","execution_count":null,"id":"4c1ef856-cc29-469b-b8d7-42f4873e5b77","metadata":{"id":"4c1ef856-cc29-469b-b8d7-42f4873e5b77"},"outputs":[],"source":["!brew install --cask font-ipafont"]},{"cell_type":"code","execution_count":null,"id":"cd16fb78-0c2a-4b4d-ba7a-b423d87422ea","metadata":{"id":"cd16fb78-0c2a-4b4d-ba7a-b423d87422ea"},"outputs":[],"source":["import numpy as np\n","from scipy.cluster.hierarchy import linkage, dendrogram\n","import matplotlib.pyplot as plt\n","import matplotlib.font_manager # 日本語フォント設定用 (環境によっては不要/設定方法が異なる)\n","\n","# X_countries (国名ベクトルのnumpy配列) と\n","# valid_country_names (国名のリスト) は問題57から引き継がれると仮定します。\n","\n","if 'X_countries' not in locals() or 'valid_country_names' not in locals() \\\n","   or X_countries is None or valid_country_names is None:\n","    print(\"エラー: 'X_countries' または 'valid_country_names' が定義されていません。\")\n","    print(\"問題57のコードを先に実行して、これらの変数を準備してください。\")\n","else:\n","    if len(valid_country_names) < 2:\n","        print(\"エラー: クラスタリングを行うには少なくとも2つの国名が必要です。\")\n","    else:\n","        print(\"Ward法による階層型クラスタリングを実行し、デンドログラムを可視化します...\")\n","\n","        # 3. Ward法による階層型クラスタリングの実行\n","        # linkage_matrix はクラスタリングの連結情報（リンケージ行列）\n","        linkage_matrix = linkage(X_countries, method='ward', metric='euclidean')\n","\n","        # 4. デンドログラムの可視化\n","        plt.figure(figsize=(15, 8)) # プロットのサイズを調整\n","\n","        try:\n","            font_path_found = None\n","            font_successfully_set = False # フォント設定が成功したかのフラグ\n","\n","            # まず 'IPAPGothic' (プロポーショナルIPA Pゴシック) を試す\n","            font_name_to_try = 'IPAPGothic'\n","            try:\n","                jp_font_path = matplotlib.font_manager.findfont(font_name_to_try, fallback_to_default=False)\n","                plt.rcParams['font.family'] = font_name_to_try\n","                print(f\"フォント '{font_name_to_try}' を使用します。 Path: {jp_font_path}\")\n","                font_successfully_set = True\n","            except ValueError: # findfont がフォントを見つけられない場合の主なエラー\n","                print(f\"警告: フォント '{font_name_to_try}' が見つかりません (ValueError)。\")\n","\n","            if not font_successfully_set:\n","                # 次に 'IPAGothic' (等幅IPAゴシック) を試す\n","                font_name_to_try_alt = 'IPAGothic'\n","                try:\n","                    jp_font_path_alt = matplotlib.font_manager.findfont(font_name_to_try_alt, fallback_to_default=False)\n","                    plt.rcParams['font.family'] = font_name_to_try_alt\n","                    print(f\"フォント '{font_name_to_try_alt}' を使用します。 Path: {jp_font_path_alt}\")\n","                    font_successfully_set = True\n","                except ValueError:\n","                    print(f\"警告: フォント '{font_name_to_try_alt}' も見つかりません (ValueError)。\")\n","\n","            if not font_successfully_set:\n","                print(\"macOSの標準的な日本語フォント 'Hiragino Sans' を試します...\")\n","                # macOS標準のヒラギノを試す\n","                font_name_hiragino = 'Hiragino Sans'\n","                try:\n","                    jp_font_path_hira = matplotlib.font_manager.findfont(font_name_hiragino, fallback_to_default=False)\n","                    plt.rcParams['font.family'] = font_name_hiragino\n","                    print(f\"フォント '{font_name_hiragino}' を使用します。 Path: {jp_font_path_hira}\")\n","                    font_successfully_set = True\n","                except ValueError:\n","                    print(f\"警告: '{font_name_hiragino}' も見つかりません (ValueError)。\")\n","\n","            if not font_successfully_set:\n","                print(\"警告: 指定した日本語フォントのいずれも見つからなかったため、デフォルトの 'sans-serif' フォントを使用します。日本語ラベルが文字化けする可能性があります。\")\n","                plt.rcParams['font.family'] = 'sans-serif'\n","\n","        except Exception as e: # 上記以外の予期せぬエラー\n","            print(f\"日本語フォント設定の試行中に予期せぬエラー: {e}。日本語ラベルが文字化けする可能性があります。\")\n","            plt.rcParams['font.family'] = 'sans-serif' # 念のためフォールバック\n","            print(\"デフォルトの 'sans-serif' フォントを使用します（エラー発生のため）。\")\n","\n","\n","        except Exception as e: # 上記以外の予期せぬエラー\n","            print(f\"日本語フォント設定の試行中に予期せぬエラー: {e}。日本語ラベルが文字化けする可能性があります。\")\n","            plt.rcParams['font.family'] = 'sans-serif' # 念のためフォールバック\n","            print(\"デフォルトの 'sans-serif' フォントを使用します（エラー発生のため）。\")\n","\n","        dendrogram(\n","            linkage_matrix,\n","            labels=valid_country_names,\n","            leaf_rotation=90.,  # ラベルを90度回転して読みやすくする\n","            leaf_font_size=10.  # ラベルのフォントサイズ\n","        )\n","\n","        plt.title('Hierarchical Clustering Dendrogram (Ward, Countries)')\n","        plt.xlabel('Country')\n","        plt.ylabel('Distance (Ward)')\n","        plt.grid(axis='y') # 横軸のグリッド線\n","        plt.tight_layout() # レイアウトを調整してラベルがはみ出ないようにする\n","        plt.show()"]},{"cell_type":"markdown","id":"epeDPNhuFoEY","metadata":{"id":"epeDPNhuFoEY"},"source":["## 59. t-SNEによる可視化\n","\n","ベクトル空間上の国名に関する単語ベクトルをt-SNEで可視化せよ。"]},{"cell_type":"code","execution_count":null,"id":"2fd6521c-6edd-44e1-a06e-96d1df3ccda5","metadata":{"id":"2fd6521c-6edd-44e1-a06e-96d1df3ccda5"},"outputs":[],"source":["import numpy as np\n","from sklearn.manifold import TSNE\n","import matplotlib.pyplot as plt\n","# import matplotlib.font_manager # 問題58でフォント設定済みであれば再度は不要かも\n","\n","# X_countries (国名ベクトルのnumpy配列) と\n","# valid_country_names (国名のリスト) は問題57/58から引き継がれると仮定します。\n","\n","if 'X_countries' not in locals() or 'valid_country_names' not in locals() \\\n","   or X_countries is None or valid_country_names is None:\n","    print(\"エラー: 'X_countries' または 'valid_country_names' が定義されていません。\")\n","    print(\"問題57のコードを先に実行して、これらの変数を準備してください。\")\n","else:\n","    if len(valid_country_names) < 2:\n","        print(\"エラー: 可視化を行うには少なくとも2つの国名が必要です。\")\n","    # perplexity はサンプル数より小さい必要がある。 len(valid_country_names)-1 以下。\n","    # サンプル数が23なので、perplexityは5～20程度が適切か。\n","    elif len(valid_country_names) <= 5: # perplexityの推奨範囲より小さい場合\n","        print(f\"エラー: サンプル数 ({len(valid_country_names)}) がt-SNEのperplexityの典型的な下限よりも少ないため、意味のある結果を得るのが難しいかもしれません。\")\n","    else:\n","        print(\"t-SNEによる次元削減と可視化を実行します...\")\n","\n","        # perplexityの値を調整 (サンプル数23に対して適切な値)\n","        # サンプル数より小さい値である必要がある。一般的に5-50。\n","        # len(valid_country_names) が23なので、perplexity は 5 や 10、15 などが良い。\n","        # デフォルトの30だと大きすぎる。\n","        perplexity_value = min(20.0, float(len(valid_country_names) - 1))\n","        if perplexity_value < 5.0 and len(valid_country_names) > 1: # 極端に少ない場合も調整\n","             perplexity_value = float(len(valid_country_names) -1)\n","\n","\n","        # 3. t-SNEの実行\n","        tsne = TSNE(n_components=2, perplexity=perplexity_value, random_state=0, n_iter=1000, learning_rate='auto', init='pca')\n","        X_embedded = tsne.fit_transform(X_countries)\n","\n","        print(\"t-SNEによる2次元への次元削減が完了しました。\")\n","\n","        # 4. 結果の可視化\n","        plt.figure(figsize=(12, 10)) # プロットのサイズを調整\n","\n","        # 問題58で設定した日本語フォントが引き続き有効であることを期待\n","        # もし文字化けする場合は、再度フォント設定の確認が必要\n","        # plt.rcParams['font.family'] = 'Hiragino Sans' # macOSの場合の例 (再設定が必要なら)\n","\n","        plt.scatter(X_embedded[:, 0], X_embedded[:, 1], marker='o')\n","\n","        for i, country_name in enumerate(valid_country_names):\n","            plt.annotate(country_name, (X_embedded[i, 0], X_embedded[i, 1]), fontsize=9)\n","\n","        plt.title(f't-SNE visualization of Country Name Vectors (Perplexity={perplexity_value:.1f})')\n","        plt.xlabel('t-SNE Dimension 1')\n","        plt.ylabel('t-SNE Dimension 2')\n","        plt.grid(True)\n","        plt.tight_layout()\n","        plt.show()"]},{"cell_type":"code","execution_count":null,"id":"b4a5a28a-e059-404d-8440-508c38baea55","metadata":{"id":"b4a5a28a-e059-404d-8440-508c38baea55"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.12"}},"nbformat":4,"nbformat_minor":5}