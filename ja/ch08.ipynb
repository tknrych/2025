{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e1YwuFtZd1t",
   "metadata": {
    "editable": true,
    "id": "1e1YwuFtZd1t",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# 第8章: ニューラルネット\n",
    "\n",
    "第7章で取り組んだポジネガ分類を題材として、ニューラルネットワークで分類モデルを実装する。なお、この章ではPyTorchやTensorFlow、JAXなどの深層学習フレームワークを活用せよ。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03603ee-a54b-4b93-97a2-888f5e3feeff",
   "metadata": {
    "id": "b03603ee-a54b-4b93-97a2-888f5e3feeff"
   },
   "source": [
    "## 70. 単語埋め込みの読み込み\n",
    "\n",
    "事前学習済み単語埋め込みを活用し、$|V| \\times d_\\rm{emb}$ の単語埋め込み行列$\\pmb{E}$を作成せよ。ここで、$|V|$は単語埋め込みの語彙数、$d_\\rm{emb}$は単語埋め込みの次元数である。ただし、単語埋め込み行列の先頭の行ベクトル$\\pmb{E}_{0,:}$は、将来的にパディング（`<PAD>`）トークンの埋め込みベクトルとして用いたいので、ゼロベクトルとして予約せよ。ゆえに、$\\pmb{E}$の2行目以降に事前学習済み単語埋め込みを読み込むことになる。\n",
    "\n",
    "もし、Google Newsデータセットの[学習済み単語ベクトル](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing)（300万単語・フレーズ、300次元）を全て読み込んだ場合、$|V|=3000001, d_\\rm{emb}=300$になるはずである（ただ、300万単語の中には、殆ど用いられない稀な単語も含まれるので、語彙を削減した方がメモリの節約になる）。\n",
    "\n",
    "また、単語埋め込み行列の構築と同時に、単語埋め込み行列の各行のインデックス番号（トークンID）と、単語（トークン）への双方向の対応付けを保持せよ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0c9c853-f346-4c68-9a7f-b92bbd8c65dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.11.10 environment at: /Users/ryuichi/.venv\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 14ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "935ee606-9fe7-4ce1-83a6-3530d432a446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "単語ベクトルモデルを読み込んでいます... (数分かかる場合があります)\n",
      "モデルの読み込みが完了しました。\n",
      "\n",
      "単語 'United_States' のベクトル:\n",
      "[-3.61328125e-02 -4.83398438e-02  2.35351562e-01  1.74804688e-01\n",
      " -1.46484375e-01 -7.42187500e-02 -1.01562500e-01 -7.71484375e-02\n",
      "  1.09375000e-01 -5.71289062e-02 -1.48437500e-01 -6.00585938e-02\n",
      "  1.74804688e-01 -7.71484375e-02  2.58789062e-02 -7.66601562e-02\n",
      " -3.80859375e-02  1.35742188e-01  3.75976562e-02 -4.19921875e-02\n",
      " -3.56445312e-02  5.34667969e-02  3.68118286e-04 -1.66992188e-01\n",
      " -1.17187500e-01  1.41601562e-01 -1.69921875e-01 -6.49414062e-02\n",
      " -1.66992188e-01  1.00585938e-01  1.15722656e-01 -2.18750000e-01\n",
      " -9.86328125e-02 -2.56347656e-02  1.23046875e-01 -3.54003906e-02\n",
      " -1.58203125e-01 -1.60156250e-01  2.94189453e-02  8.15429688e-02\n",
      "  6.88476562e-02  1.87500000e-01  6.49414062e-02  1.15234375e-01\n",
      " -2.27050781e-02  3.32031250e-01 -3.27148438e-02  1.77734375e-01\n",
      " -2.08007812e-01  4.54101562e-02 -1.23901367e-02  1.19628906e-01\n",
      "  7.44628906e-03 -9.03320312e-03  1.14257812e-01  1.69921875e-01\n",
      " -2.38281250e-01 -2.79541016e-02 -1.21093750e-01  2.47802734e-02\n",
      "  7.71484375e-02 -2.81982422e-02 -4.71191406e-02  1.78222656e-02\n",
      " -1.23046875e-01 -5.32226562e-02  2.68554688e-02 -3.11279297e-02\n",
      " -5.59082031e-02 -5.00488281e-02 -3.73535156e-02  1.25976562e-01\n",
      "  5.61523438e-02  1.51367188e-01  4.29687500e-02 -2.08007812e-01\n",
      " -4.78515625e-02  2.78320312e-02  1.81640625e-01  2.20703125e-01\n",
      " -3.61328125e-02 -8.39843750e-02 -3.69548798e-05 -9.52148438e-02\n",
      " -1.25000000e-01 -1.95312500e-01 -1.50390625e-01 -4.15039062e-02\n",
      "  1.31835938e-01  1.17675781e-01  1.91650391e-02  5.51757812e-02\n",
      " -9.42382812e-02 -1.08886719e-01  7.32421875e-02 -1.15234375e-01\n",
      "  8.93554688e-02 -1.40625000e-01  1.45507812e-01  4.49218750e-02\n",
      " -1.10473633e-02 -1.62353516e-02  4.05883789e-03  3.75976562e-02\n",
      " -6.98242188e-02 -5.46875000e-02  2.17285156e-02 -9.47265625e-02\n",
      "  4.24804688e-02  1.81884766e-02 -1.73339844e-02  4.63867188e-02\n",
      " -1.42578125e-01  1.99218750e-01  1.10839844e-01  2.58789062e-02\n",
      " -7.08007812e-02 -5.54199219e-02  3.45703125e-01  1.61132812e-01\n",
      " -2.44140625e-01 -2.59765625e-01 -9.71679688e-02  8.00781250e-02\n",
      " -8.78906250e-02 -7.22656250e-02  1.42578125e-01 -8.54492188e-02\n",
      " -3.18359375e-01  8.30078125e-02  6.34765625e-02  1.64062500e-01\n",
      " -1.92382812e-01 -1.17675781e-01 -5.41992188e-02 -1.56250000e-01\n",
      " -1.21582031e-01 -4.95605469e-02  1.20117188e-01 -3.83300781e-02\n",
      "  5.51757812e-02 -8.97216797e-03  4.32128906e-02  6.93359375e-02\n",
      "  8.93554688e-02  2.53906250e-01  1.65039062e-01  1.64062500e-01\n",
      " -1.41601562e-01  4.58984375e-02  1.97265625e-01 -8.98437500e-02\n",
      "  3.90625000e-02 -1.51367188e-01 -8.60595703e-03 -1.17675781e-01\n",
      " -1.97265625e-01 -1.12792969e-01  1.29882812e-01  1.96289062e-01\n",
      "  1.56402588e-03  3.93066406e-02  2.17773438e-01 -1.43554688e-01\n",
      "  6.03027344e-02 -1.35742188e-01  1.16210938e-01 -1.59912109e-02\n",
      "  2.79296875e-01  1.46484375e-01 -1.19628906e-01  1.76757812e-01\n",
      "  1.28906250e-01 -1.49414062e-01  6.93359375e-02 -1.72851562e-01\n",
      "  9.22851562e-02  1.33056641e-02 -2.00195312e-01 -9.76562500e-02\n",
      " -1.65039062e-01 -2.46093750e-01 -2.35595703e-02 -2.11914062e-01\n",
      "  1.84570312e-01 -1.85546875e-02  2.16796875e-01  5.05371094e-02\n",
      "  2.02636719e-02  4.25781250e-01  1.28906250e-01 -2.77099609e-02\n",
      "  1.29882812e-01 -1.15722656e-01 -2.05078125e-02  1.49414062e-01\n",
      "  7.81250000e-03 -2.05078125e-01 -8.05664062e-02 -2.67578125e-01\n",
      " -2.29492188e-02 -8.20312500e-02  8.64257812e-02  7.61718750e-02\n",
      " -3.66210938e-02  5.22460938e-02 -1.22070312e-01 -1.44042969e-02\n",
      " -2.69531250e-01  8.44726562e-02 -2.52685547e-02 -2.96630859e-02\n",
      " -1.68945312e-01  1.93359375e-01 -1.08398438e-01  1.94091797e-02\n",
      " -1.80664062e-01  1.93359375e-01 -7.08007812e-02  5.85937500e-02\n",
      " -1.01562500e-01 -1.31835938e-01  7.51953125e-02 -7.66601562e-02\n",
      "  3.37219238e-03 -8.59375000e-02  1.25000000e-01  2.92968750e-02\n",
      "  1.70898438e-01 -9.37500000e-02 -1.09375000e-01 -2.50244141e-02\n",
      "  2.11914062e-01 -4.44335938e-02  6.12792969e-02  2.62451172e-02\n",
      " -1.77734375e-01  1.23046875e-01 -7.42187500e-02 -1.67968750e-01\n",
      " -1.08886719e-01 -9.04083252e-04 -7.37304688e-02  5.49316406e-02\n",
      "  6.03027344e-02  8.39843750e-02  9.17968750e-02 -1.32812500e-01\n",
      "  1.22070312e-01 -8.78906250e-03  1.19140625e-01 -1.94335938e-01\n",
      " -6.64062500e-02 -2.07031250e-01  7.37304688e-02  8.93554688e-02\n",
      "  1.81884766e-02 -1.20605469e-01 -2.61230469e-02  2.67333984e-02\n",
      "  7.76367188e-02 -8.30078125e-02  6.78710938e-02 -3.54003906e-02\n",
      "  3.10546875e-01 -2.42919922e-02 -1.41601562e-01 -2.08007812e-01\n",
      " -4.57763672e-03 -6.54296875e-02 -4.95605469e-02  2.22656250e-01\n",
      "  1.53320312e-01 -1.38671875e-01 -5.24902344e-02  4.24804688e-02\n",
      " -2.38281250e-01  1.56250000e-01  5.83648682e-04 -1.20605469e-01\n",
      " -9.22851562e-02 -4.44335938e-02  3.61328125e-02 -1.86767578e-02\n",
      " -8.25195312e-02 -8.25195312e-02 -4.05273438e-02  1.19018555e-02\n",
      "  1.69921875e-01 -2.80761719e-02  3.03649902e-03  9.32617188e-02\n",
      " -8.49609375e-02  1.57470703e-02  7.03125000e-02  1.62353516e-02\n",
      " -2.27050781e-02  3.51562500e-02  2.47070312e-01 -2.67333984e-02]\n",
      "\n",
      "ベクトルの次元数: 300\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "model_path = '../data/GoogleNews-vectors-negative300.bin.gz'\n",
    "\n",
    "try:\n",
    "    # モデルの読み込み (時間がかかることがあります)\n",
    "    print(\"単語ベクトルモデルを読み込んでいます... (数分かかる場合があります)\")\n",
    "    word_vectors = KeyedVectors.load_word2vec_format(model_path, binary=True)\n",
    "    print(\"モデルの読み込みが完了しました。\")\n",
    "\n",
    "    # \"United States\" (内部表現 \"United_States\") の単語ベクトルを取得\n",
    "    target_word = \"United_States\"\n",
    "\n",
    "    if target_word in word_vectors:\n",
    "        vector_united_states = word_vectors[target_word]\n",
    "        print(f\"\\n単語 '{target_word}' のベクトル:\")\n",
    "        print(vector_united_states)\n",
    "        print(f\"\\nベクトルの次元数: {len(vector_united_states)}\")\n",
    "    else:\n",
    "        print(f\"エラー: 単語 '{target_word}' はボキャブラリ内に見つかりませんでした。\")\n",
    "        print(f\"代わりに 'United States' で試してみます...\")\n",
    "        target_word_alt = \"United States\" # スペース区切りも試す (通常は _ 区切り)\n",
    "        if target_word_alt in word_vectors:\n",
    "            vector_united_states = word_vectors[target_word_alt]\n",
    "            print(f\"\\n単語 '{target_word_alt}' のベクトル:\")\n",
    "            print(vector_united_states)\n",
    "            print(f\"\\nベクトルの次元数: {len(vector_united_states)}\")\n",
    "        else:\n",
    "            print(f\"エラー: 単語 '{target_word_alt}' もボキャブラリ内に見つかりませんでした。\")\n",
    "\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"エラー: 指定されたパスにファイルが見つかりません: {model_path}\")\n",
    "    print(\"Google Newsの単語ベクトルファイルをダウンロードし、正しいパスを指定してください。\")\n",
    "except Exception as e:\n",
    "    print(f\"モデルの読み込み中またはベクトル取得中にエラーが発生しました: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "224f340b-33dd-46e5-9a39-e0e19a3f1dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "読み込み済みの単語ベクトルモデルを使用します。\n",
      "単語ベクトルの次元数 (d_emb): 300\n",
      "元モデルの語彙数: 3000000\n",
      "パディングトークンを含む総語彙数 (|V|): 3000001\n",
      "単語埋め込み行列 E の形状: (3000001, 300)\n",
      "\n",
      "単語埋め込み行列とIDマッピングの構築が完了しました。\n",
      "\n",
      "<PAD> のID: 0\n",
      "<PAD> のベクトル (embedding_matrix[0]): [0. 0. 0. 0. 0.]... (最初の5次元)\n",
      "\n",
      "単語 'king' のID: 6148\n",
      "ID 6148 に対応する単語: king\n",
      "'king' のベクトル (word_vectors['king'][:5]): [ 0.12597656  0.02978516  0.00860596  0.13964844 -0.02563477]...\n",
      "'king' のベクトル (embedding_matrix[6148][:5]): [ 0.12597656  0.02978516  0.00860596  0.13964844 -0.02563477]...\n",
      "'king' のベクトルは正しくコピーされました。\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# word_vectors は問題50で読み込まれた KeyedVectors オブジェクトとします。\n",
    "# もし word_vectors が未定義の場合は、問題50のコードを先に実行してモデルをロードしてください。\n",
    "\n",
    "if 'word_vectors' not in locals() or word_vectors is None:\n",
    "    print(\"エラー: 単語ベクトルモデル 'word_vectors' が読み込まれていません。\")\n",
    "    print(\"問題50のコードを実行して、先にモデルをロードしてください。\")\n",
    "    # この後の処理に進めないため、ここで処理を中断\n",
    "    # raise NameError(\"word_vectors is not defined\")\n",
    "else:\n",
    "    print(\"読み込み済みの単語ベクトルモデルを使用します。\")\n",
    "\n",
    "    # 単語ベクトルの次元数を取得\n",
    "    embedding_dim = word_vectors.vector_size\n",
    "    print(f\"単語ベクトルの次元数 (d_emb): {embedding_dim}\")\n",
    "\n",
    "    # 語彙リストとIDマッピングの初期化\n",
    "    # <PAD> トークンを追加\n",
    "    PAD_TOKEN = \"<PAD>\"\n",
    "    PAD_ID = 0\n",
    "    word_to_id = {PAD_TOKEN: 0}\n",
    "    id_to_word = {0: PAD_TOKEN}\n",
    "    \n",
    "    # 語彙リスト (gensimモデルのキーをそのまま使う。順序も保持される)\n",
    "    # word_vectors.index_to_key で単語リストを取得可能\n",
    "    vocabulary_words = word_vectors.index_to_key\n",
    "    \n",
    "    # 埋め込み行列の語彙数 (|V|) は、元の語彙数 + 1 (<PAD>トークン分)\n",
    "    vocab_size = len(vocabulary_words) + 1\n",
    "    print(f\"元モデルの語彙数: {len(vocabulary_words)}\")\n",
    "    print(f\"パディングトークンを含む総語彙数 (|V|): {vocab_size}\")\n",
    "\n",
    "    # 単語埋め込み行列Eを初期化 (0行目はパディング用にゼロベクトル、残りは実際のベクトル)\n",
    "    # 0行目は既にゼロで初期化される\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim), dtype=np.float32)\n",
    "\n",
    "    print(f\"単語埋め込み行列 E の形状: {embedding_matrix.shape}\")\n",
    "\n",
    "    # word_vectorsから単語とベクトルを読み込み、word_to_id, id_to_word, embedding_matrix を構築\n",
    "    # ID=0 は <PAD> なので、実際の単語は ID=1 から開始\n",
    "    for i, word in enumerate(vocabulary_words):\n",
    "        token_id = i + 1 # IDは1から開始\n",
    "        word_to_id[word] = token_id\n",
    "        id_to_word[token_id] = word\n",
    "        embedding_matrix[token_id] = word_vectors[word]\n",
    "        \n",
    "    print(\"\\n単語埋め込み行列とIDマッピングの構築が完了しました。\")\n",
    "\n",
    "    # 簡単なテスト\n",
    "    print(f\"\\n<PAD> のID: {word_to_id[PAD_TOKEN]}\")\n",
    "    print(f\"<PAD> のベクトル (embedding_matrix[0]): {embedding_matrix[word_to_id[PAD_TOKEN]][:5]}... (最初の5次元)\") # 全て0のはず\n",
    "\n",
    "    # 何か適当な単語でテスト\n",
    "    sample_word = \"king\" # word_vectors に含まれる単語を選ぶ\n",
    "    if sample_word in word_to_id:\n",
    "        sample_word_id = word_to_id[sample_word]\n",
    "        print(f\"\\n単語 '{sample_word}' のID: {sample_word_id}\")\n",
    "        print(f\"ID {sample_word_id} に対応する単語: {id_to_word[sample_word_id]}\")\n",
    "        print(f\"'{sample_word}' のベクトル (word_vectors['{sample_word}'][:5]): {word_vectors[sample_word][:5]}...\")\n",
    "        print(f\"'{sample_word}' のベクトル (embedding_matrix[{sample_word_id}][:5]): {embedding_matrix[sample_word_id][:5]}...\")\n",
    "        # 上の2つのベクトルが一致することを確認\n",
    "        if np.allclose(word_vectors[sample_word], embedding_matrix[sample_word_id]):\n",
    "            print(f\"'{sample_word}' のベクトルは正しくコピーされました。\")\n",
    "        else:\n",
    "            print(f\"警告: '{sample_word}' のベクトルが正しくコピーされていません。\")\n",
    "    else:\n",
    "        print(f\"テスト単語 '{sample_word}' は語彙に含まれていません。\")\n",
    "        # もし 'king' がなければ、word_vectors.index_to_key[0] など、確実に存在する単語で試してください。\n",
    "        # 例えば: test_word_for_check = id_to_word[1] (ID=1の単語)\n",
    "        # print(f\"代わりにID=1の単語 '{test_word_for_check}' で確認します。\")\n",
    "        # print(f\"  ベクトル (word_vectors): {word_vectors[test_word_for_check][:5]}\")\n",
    "        # print(f\"  ベクトル (embedding_matrix[1]): {embedding_matrix[1][:5]}\")\n",
    "\n",
    "    # これで、embedding_matrix, word_to_id, id_to_word が準備できました。\n",
    "    # これらは後の問題で使用しますので、変数として保持しておいてください。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45bc5ba-4a83-493a-a78e-04aa48f3db2e",
   "metadata": {
    "id": "c45bc5ba-4a83-493a-a78e-04aa48f3db2e"
   },
   "source": [
    "## 71. データセットの読み込み\n",
    "\n",
    "[General Language Understanding Evaluation (GLUE)](https://gluebenchmark.com/) ベンチマークで配布されている[Stanford Sentiment Treebank (SST)](https://dl.fbaipublicfiles.com/glue/data/SST-2.zip) をダウンロードし、訓練セット（train.tsv）と開発セット（dev.tsv）のテキストと極性ラベルと読み込み、全てのテキストをトークンID列に変換せよ。このとき、単語埋め込みの語彙でカバーされていない単語は無視し、トークン列に含めないことにせよ。また、テキストの全トークンが単語埋め込みの語彙に含まれておらず、空のトークン列となってしまう事例は、訓練セットおよび開発セットから削除せよ（このため、第7章の実験で得られた正解率と比較できなくなることに注意せよ）。\n",
    "\n",
    "事例の表現方法は任意でよいが、例えば\"contains no wit , only labored gags\"がネガティブに分類される事例は、次のような辞書オブジェクトで表現すればよい。\n",
    "\n",
    "```\n",
    "{'text': 'contains no wit , only labored gags',\n",
    " 'label': tensor([0.]),\n",
    " 'input_ids': tensor([ 3475,    87, 15888,    90, 27695, 42637])}\n",
    "```\n",
    "\n",
    "この例では、`text`はテキスト、`label`は分類ラベル（ポジティブなら`tensor([1.])`、ネガティブなら`tensor([0.])`）、`input_ids`はテキストのトークン列をID列で表現している。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f9b5fc8-c738-4d71-a0b4-70caebfbdad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.11.10 environment at: /Users/ryuichi/.venv\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 23ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1ae7ea1-34ce-4b01-8a1c-96923d238dda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "訓練データの処理を開始します...\n",
      "\n",
      "--- 訓練データ (train.tsv) ---\n",
      "元の事例数: 67349\n",
      "語彙外単語無視後の処理済み事例数: 66650\n",
      "空のトークン列のため削除された事例数: 699\n",
      "\n",
      "開発（検証）データの処理を開始します...\n",
      "\n",
      "--- 開発データ (dev.tsv) ---\n",
      "元の事例数: 872\n",
      "語彙外単語無視後の処理済み事例数: 872\n",
      "空のトークン列のため削除された事例数: 0\n",
      "\n",
      "訓練データの最初の3事例（処理後）:\n",
      "{'text': 'hide new secretions from the parental units ', 'label': tensor([0.]), 'input_ids': tensor([  5785,     66, 113845,     18,     12,  15095,   1594])}\n",
      "{'text': 'contains no wit , only labored gags ', 'label': tensor([0.]), 'input_ids': tensor([ 3475,    87, 15888,    90, 27695, 42637])}\n",
      "{'text': 'that loves its characters and communicates something rather beautiful about human nature ', 'label': tensor([1.]), 'input_ids': tensor([    4,  5053,    45,  3305, 31647,   348,   904,  2815,    47,  1276,\n",
      "         1964])}\n",
      "\n",
      "開発データの最初の3事例（処理後）:\n",
      "{'text': \"it 's a charming and often affecting journey . \", 'label': tensor([1.]), 'input_ids': tensor([   16, 13259,   640,  5199,  3900])}\n",
      "{'text': 'unflinchingly bleak and desperate ', 'label': tensor([0.]), 'input_ids': tensor([136642,  12607,   4984])}\n",
      "{'text': 'allows us to hope that nolan is poised to embark a major career as a commercial yet inventive filmmaker . ', 'label': tensor([1.]), 'input_ids': tensor([  1488,    165,    684,      4, 953829,      5,   6091,  14671,    339,\n",
      "           513,     15,   1073,    507,  24346,  11212])}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch # PyTorchを使用\n",
    "import os\n",
    "\n",
    "# --- 前提となる変数 (問題70および問題60から) ---\n",
    "# word_to_id: 単語からトークンIDへの辞書 (問題70で作成済みとします)\n",
    "# train_file_path: train.tsvへのパス (問題60で定義済みとします)\n",
    "# dev_file_path: dev.tsvへのパス (問題60で定義済みとします)\n",
    "\n",
    "# もし word_to_id やファイルパスが現在のセッションにない場合は、\n",
    "# 問題70や問題60を再実行して準備してください。\n",
    "# 例:\n",
    "# word_to_id = {PAD_TOKEN: 0, 'word1': 1, ...} # 問題70の word_to_id\n",
    "# train_file_path = \"../data/SST-2_data/SST-2/train.tsv\" # 問題60のパス\n",
    "# dev_file_path = \"../data/SST-2_data/SST-2/dev.tsv\"   # 問題60のパス\n",
    "\n",
    "if 'word_to_id' not in locals():\n",
    "    print(\"エラー: 'word_to_id' が定義されていません。問題70を先に実行してください。\")\n",
    "    # この後の処理に進めないため、ここで処理を中断\n",
    "    raise NameError(\"word_to_id is not defined.\")\n",
    "\n",
    "# 問題60でSST-2データを展開した親ディレクトリへのパスを想定\n",
    "# (ユーザー様が問題61で確認・設定したパスを参考にしてください)\n",
    "base_data_dir_ch7 = '../data/SST-2_data' \n",
    "train_file_path = os.path.join(base_data_dir_ch7, \"SST-2/train.tsv\")\n",
    "dev_file_path = os.path.join(base_data_dir_ch7, \"SST-2/dev.tsv\")\n",
    "\n",
    "# --- ここから問題71の処理 ---\n",
    "\n",
    "def text_to_token_ids(text, word_to_id_map):\n",
    "    \"\"\"テキストを単語に分割し、word_to_id_map を使ってトークンIDのリストに変換する。\n",
    "       語彙にない単語は無視する。\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str): # 万が一テキストが文字列でない場合\n",
    "        text = str(text)\n",
    "        \n",
    "    tokens = text.split(' ') # スペースで分割 (問題61と同様)\n",
    "    token_ids = [word_to_id_map[word] for word in tokens if word in word_to_id_map]\n",
    "    return token_ids\n",
    "\n",
    "def load_and_process_sst2_dataset(file_path, word_to_id_map, dataset_name=\"データセット\"):\n",
    "    \"\"\"SST-2データセットのTSVファイルを読み込み、指定の形式に処理する。\"\"\"\n",
    "    processed_data = []\n",
    "    skipped_empty_count = 0\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(file_path, sep='\\t')\n",
    "        print(f\"\\n--- {dataset_name} ({os.path.basename(file_path)}) ---\")\n",
    "        print(f\"元の事例数: {len(df)}\")\n",
    "        \n",
    "        for index, row in df.iterrows():\n",
    "            text = str(row['sentence'])\n",
    "            label = int(row['label']) # ラベルを整数 (0 or 1) に\n",
    "            \n",
    "            token_ids = text_to_token_ids(text, word_to_id_map)\n",
    "            \n",
    "            # トークンID列が空になった事例は削除 (無視)\n",
    "            if not token_ids:\n",
    "                skipped_empty_count += 1\n",
    "                continue\n",
    "                \n",
    "            processed_data.append({\n",
    "                'text': text,\n",
    "                'label': torch.tensor([float(label)]), # ラベルをfloatのテンソルに\n",
    "                'input_ids': torch.tensor(token_ids, dtype=torch.long) # トークンID列をlongのテンソルに\n",
    "            })\n",
    "            \n",
    "        print(f\"語彙外単語無視後の処理済み事例数: {len(processed_data)}\")\n",
    "        print(f\"空のトークン列のため削除された事例数: {skipped_empty_count}\")\n",
    "        return processed_data\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"エラー: ファイル '{file_path}' が見つかりません。\")\n",
    "        return None\n",
    "    except KeyError:\n",
    "        print(f\"エラー: ファイル '{file_path}' に必要な列 ('sentence' or 'label') が見つかりません。\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"ファイル '{file_path}' の処理中にエラーが発生しました: {e}\")\n",
    "        return None\n",
    "\n",
    "# word_to_id が問題70で正しく作成されていることを前提とする\n",
    "# (もし word_to_id が巨大すぎる場合は、問題70で語彙削減版を作成しておくことを推奨)\n",
    "\n",
    "print(\"訓練データの処理を開始します...\")\n",
    "train_dataset_processed = load_and_process_sst2_dataset(train_file_path, word_to_id, \"訓練データ\")\n",
    "\n",
    "print(\"\\n開発（検証）データの処理を開始します...\")\n",
    "dev_dataset_processed = load_and_process_sst2_dataset(dev_file_path, word_to_id, \"開発データ\")\n",
    "\n",
    "# 処理結果の最初の数件を表示して確認\n",
    "if train_dataset_processed:\n",
    "    print(\"\\n訓練データの最初の3事例（処理後）:\")\n",
    "    for i in range(min(3, len(train_dataset_processed))):\n",
    "        print(train_dataset_processed[i])\n",
    "\n",
    "if dev_dataset_processed:\n",
    "    print(\"\\n開発データの最初の3事例（処理後）:\")\n",
    "    for i in range(min(3, len(dev_dataset_processed))):\n",
    "        print(dev_dataset_processed[i])\n",
    "\n",
    "# これで train_dataset_processed と dev_dataset_processed に\n",
    "# 処理済みのデータが格納されました。\n",
    "# これらは後の問題で使用します。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29dfe527-a08c-48fa-b9b4-0acebea36bca",
   "metadata": {
    "id": "29dfe527-a08c-48fa-b9b4-0acebea36bca"
   },
   "source": [
    "## 72. Bag of wordsモデルの構築\n",
    "\n",
    "単語埋め込みの平均ベクトルでテキストの特徴ベクトルを表現し、重みベクトルとの内積でポジティブ及びネガティブを分類するニューラルネットワーク（ロジスティック回帰モデル）を設計せよ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5625a19-76b3-4a7b-996d-4987af567bb0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "設計したBag of Wordsモデルの構造:\n",
      "SimpleBOWClassifier(\n",
      "  (embedding): Embedding(3000001, 300, padding_idx=0)\n",
      "  (fc): Linear(in_features=300, out_features=1, bias=True)\n",
      ")\n",
      "\n",
      "ダミー入力でフォワードパスをテストします...\n",
      "ダミー出力 (ロジット):\n",
      "tensor([[-0.0211],\n",
      "        [-0.0887]], grad_fn=<AddmmBackward0>)\n",
      "出力形状: torch.Size([2, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np # embedding_matrix を PyTorchテンソルに変換するために使用\n",
    "\n",
    "# --- 前提となる変数 (問題70から) ---\n",
    "# embedding_matrix: 単語埋め込み行列 (NumPy配列)\n",
    "# vocab_size: 総語彙数\n",
    "# embedding_dim: 単語ベクトルの次元数\n",
    "# PAD_ID = 0 (パディングトークンのID)\n",
    "\n",
    "# これらの変数が現在のセッションに存在することを確認してください。\n",
    "# もし存在しない場合は、問題70を再実行して準備してください。\n",
    "if 'embedding_matrix' not in locals() or \\\n",
    "   'vocab_size' not in locals() or \\\n",
    "   'embedding_dim' not in locals() or \\\n",
    "   'PAD_ID' not in locals(): # PAD_ID は直接コード内で0として使われることもありますが、念のため\n",
    "    print(\"エラー: 'embedding_matrix', 'vocab_size', 'embedding_dim', または 'PAD_ID' が定義されていません。\")\n",
    "    print(\"問題70を先に実行して、これらの変数を準備してください。\")\n",
    "    raise NameError(\"Required variables from Problem 70 are not defined.\")\n",
    "\n",
    "class SimpleBOWClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, pretrained_embeddings, padding_idx):\n",
    "        super(SimpleBOWClassifier, self).__init__()\n",
    "        \n",
    "        # 1. 単語埋め込み層\n",
    "        # 事前学習済み重みをロードし、学習中は更新しない (freeze)\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=padding_idx)\n",
    "        self.embedding.weight = nn.Parameter(pretrained_embeddings, requires_grad=False)\n",
    "        \n",
    "        # 2. 線形層 (平均化された埋め込みベクトルを入力とし、1つの値を出力)\n",
    "        # 出力はロジット (シグモイド関数を適用する前の値) とする\n",
    "        self.fc = nn.Linear(embedding_dim, 1)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        # input_ids: (バッチサイズ, シーケンス長) のテンソル\n",
    "        \n",
    "        # 埋め込みベクトルを取得\n",
    "        # embedded: (バッチサイズ, シーケンス長, embedding_dim)\n",
    "        embedded = self.embedding(input_ids)\n",
    "        \n",
    "        # パディングを考慮した平均プーリング\n",
    "        # マスクを作成 (パディング部分は0、非パディング部分は1)\n",
    "        # input_idsがpadding_idxでない箇所が1、padding_idxの箇所が0になるマスク\n",
    "        mask = (input_ids != self.embedding.padding_idx).unsqueeze(-1).float()\n",
    "        # mask: (バッチサイズ, シーケンス長, 1)\n",
    "        \n",
    "        # マスクを適用してパディング部分のベクトルをゼロにする (元々ゼロベクトルだが念のため)\n",
    "        # embedded = embedded * mask # embedding_matrixのpadding_idx行がゼロなら不要な場合も\n",
    "        \n",
    "        # 各シーケンスの実際の長さ（非パディングトークンの数）を計算\n",
    "        # lengths: (バッチサイズ)\n",
    "        lengths = mask.sum(dim=1)\n",
    "        lengths = lengths.clamp(min=1) # ゼロ除算を避けるため、最小値を1に（全てパディングの稀なケース対策）\n",
    "        \n",
    "        # マスクされた埋め込みベクトルの合計を計算\n",
    "        # sum_embedded: (バッチサイズ, embedding_dim)\n",
    "        sum_embedded = (embedded * mask).sum(dim=1) # パディング部分を除外して合計\n",
    "        \n",
    "        # 平均を計算\n",
    "        # mean_embedded: (バッチサイズ, embedding_dim)\n",
    "        mean_embedded = sum_embedded / lengths\n",
    "        \n",
    "        # 線形層に入力\n",
    "        # logits: (バッチサイズ, 1)\n",
    "        logits = self.fc(mean_embedded)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "# --- モデルのインスタンス化と簡単なテスト ---\n",
    "# PyTorchテンソルに変換\n",
    "embedding_tensor = torch.tensor(embedding_matrix, dtype=torch.float)\n",
    "\n",
    "# モデルのインスタンスを作成\n",
    "model_bow = SimpleBOWClassifier(vocab_size, embedding_dim, embedding_tensor, PAD_ID)\n",
    "\n",
    "# モデルの構造を表示して確認\n",
    "print(\"設計したBag of Wordsモデルの構造:\")\n",
    "print(model_bow)\n",
    "\n",
    "# 簡単なダミー入力でフォワードパスをテスト\n",
    "# バッチサイズ2, シーケンス長5 のダミー入力 (トークンID)\n",
    "dummy_input_ids = torch.tensor([[10, 20, 30, 0, 0],  # 最初の事例は3トークン + 2パディング\n",
    "                                [40, 50,  0, 0, 0]], # 2番目の事例は2トークン + 3パディング\n",
    "                               dtype=torch.long)\n",
    "\n",
    "# モデルが学習モードか評価モードか (dropoutなどがある場合は影響)\n",
    "# model_bow.eval() # 評価時\n",
    "# model_bow.train() # 学習時\n",
    "\n",
    "if dummy_input_ids.max().item() < vocab_size: # ダミーIDが語彙サイズ内か確認\n",
    "    try:\n",
    "        print(\"\\nダミー入力でフォワードパスをテストします...\")\n",
    "        dummy_output = model_bow(dummy_input_ids)\n",
    "        print(\"ダミー出力 (ロジット):\")\n",
    "        print(dummy_output)\n",
    "        print(f\"出力形状: {dummy_output.shape}\") # 期待: (バッチサイズ, 1)\n",
    "    except Exception as e:\n",
    "        print(f\"ダミー入力でのフォワードパステスト中にエラー: {e}\")\n",
    "else:\n",
    "    print(\"\\nダミー入力のIDが語彙サイズを超えています。テストをスキップします。\")\n",
    "    print(f\"ダミー入力の最大ID: {dummy_input_ids.max().item()}, 語彙サイズ: {vocab_size}\")\n",
    "\n",
    "# この model_bow が問題72で設計したモデルとなります。\n",
    "# 次の問題73で、このモデルの学習を行います。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72385c44-ceab-4d62-a4df-3023e15a37e2",
   "metadata": {
    "id": "72385c44-ceab-4d62-a4df-3023e15a37e2"
   },
   "source": [
    "## 73. モデルの学習\n",
    "\n",
    "問題72で設計したモデルの重みベクトルを訓練セット上で学習せよ。ただし、学習中は単語埋め込み行列の値を固定せよ（単語埋め込み行列のファインチューニングは行わない）。また、学習時に損失値を表示するなど、学習の進捗状況をモニタリングできるようにせよ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e786085-aae9-46c9-bd40-5ad782abb049",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "モデルの学習を開始します...\n",
      "  Epoch [1/5], Batch [1000/66650], Avg Loss: 0.6365\n",
      "  Epoch [1/5], Batch [2000/66650], Avg Loss: 0.5996\n",
      "  Epoch [1/5], Batch [3000/66650], Avg Loss: 0.5743\n",
      "  Epoch [1/5], Batch [4000/66650], Avg Loss: 0.5554\n",
      "  Epoch [1/5], Batch [5000/66650], Avg Loss: 0.5413\n",
      "  Epoch [1/5], Batch [6000/66650], Avg Loss: 0.5269\n",
      "  Epoch [1/5], Batch [7000/66650], Avg Loss: 0.5152\n",
      "  Epoch [1/5], Batch [8000/66650], Avg Loss: 0.5062\n",
      "  Epoch [1/5], Batch [9000/66650], Avg Loss: 0.4996\n",
      "  Epoch [1/5], Batch [10000/66650], Avg Loss: 0.4935\n",
      "  Epoch [1/5], Batch [11000/66650], Avg Loss: 0.4857\n",
      "  Epoch [1/5], Batch [12000/66650], Avg Loss: 0.4809\n",
      "  Epoch [1/5], Batch [13000/66650], Avg Loss: 0.4762\n",
      "  Epoch [1/5], Batch [14000/66650], Avg Loss: 0.4720\n",
      "  Epoch [1/5], Batch [15000/66650], Avg Loss: 0.4680\n",
      "  Epoch [1/5], Batch [16000/66650], Avg Loss: 0.4632\n",
      "  Epoch [1/5], Batch [17000/66650], Avg Loss: 0.4605\n",
      "  Epoch [1/5], Batch [18000/66650], Avg Loss: 0.4573\n",
      "  Epoch [1/5], Batch [19000/66650], Avg Loss: 0.4529\n",
      "  Epoch [1/5], Batch [20000/66650], Avg Loss: 0.4506\n",
      "  Epoch [1/5], Batch [21000/66650], Avg Loss: 0.4485\n",
      "  Epoch [1/5], Batch [22000/66650], Avg Loss: 0.4456\n",
      "  Epoch [1/5], Batch [23000/66650], Avg Loss: 0.4435\n",
      "  Epoch [1/5], Batch [24000/66650], Avg Loss: 0.4410\n",
      "  Epoch [1/5], Batch [25000/66650], Avg Loss: 0.4392\n",
      "  Epoch [1/5], Batch [26000/66650], Avg Loss: 0.4377\n",
      "  Epoch [1/5], Batch [27000/66650], Avg Loss: 0.4358\n",
      "  Epoch [1/5], Batch [28000/66650], Avg Loss: 0.4335\n",
      "  Epoch [1/5], Batch [29000/66650], Avg Loss: 0.4322\n",
      "  Epoch [1/5], Batch [30000/66650], Avg Loss: 0.4306\n",
      "  Epoch [1/5], Batch [31000/66650], Avg Loss: 0.4289\n",
      "  Epoch [1/5], Batch [32000/66650], Avg Loss: 0.4281\n",
      "  Epoch [1/5], Batch [33000/66650], Avg Loss: 0.4268\n",
      "  Epoch [1/5], Batch [34000/66650], Avg Loss: 0.4250\n",
      "  Epoch [1/5], Batch [35000/66650], Avg Loss: 0.4238\n",
      "  Epoch [1/5], Batch [36000/66650], Avg Loss: 0.4231\n",
      "  Epoch [1/5], Batch [37000/66650], Avg Loss: 0.4213\n",
      "  Epoch [1/5], Batch [38000/66650], Avg Loss: 0.4209\n",
      "  Epoch [1/5], Batch [39000/66650], Avg Loss: 0.4206\n",
      "  Epoch [1/5], Batch [40000/66650], Avg Loss: 0.4203\n",
      "  Epoch [1/5], Batch [41000/66650], Avg Loss: 0.4188\n",
      "  Epoch [1/5], Batch [42000/66650], Avg Loss: 0.4180\n",
      "  Epoch [1/5], Batch [43000/66650], Avg Loss: 0.4168\n",
      "  Epoch [1/5], Batch [44000/66650], Avg Loss: 0.4161\n",
      "  Epoch [1/5], Batch [45000/66650], Avg Loss: 0.4150\n",
      "  Epoch [1/5], Batch [46000/66650], Avg Loss: 0.4138\n",
      "  Epoch [1/5], Batch [47000/66650], Avg Loss: 0.4130\n",
      "  Epoch [1/5], Batch [48000/66650], Avg Loss: 0.4121\n",
      "  Epoch [1/5], Batch [49000/66650], Avg Loss: 0.4116\n",
      "  Epoch [1/5], Batch [50000/66650], Avg Loss: 0.4108\n",
      "  Epoch [1/5], Batch [51000/66650], Avg Loss: 0.4098\n",
      "  Epoch [1/5], Batch [52000/66650], Avg Loss: 0.4094\n",
      "  Epoch [1/5], Batch [53000/66650], Avg Loss: 0.4084\n",
      "  Epoch [1/5], Batch [54000/66650], Avg Loss: 0.4083\n",
      "  Epoch [1/5], Batch [55000/66650], Avg Loss: 0.4077\n",
      "  Epoch [1/5], Batch [56000/66650], Avg Loss: 0.4071\n",
      "  Epoch [1/5], Batch [57000/66650], Avg Loss: 0.4067\n",
      "  Epoch [1/5], Batch [58000/66650], Avg Loss: 0.4064\n",
      "  Epoch [1/5], Batch [59000/66650], Avg Loss: 0.4059\n",
      "  Epoch [1/5], Batch [60000/66650], Avg Loss: 0.4054\n",
      "  Epoch [1/5], Batch [61000/66650], Avg Loss: 0.4048\n",
      "  Epoch [1/5], Batch [62000/66650], Avg Loss: 0.4048\n",
      "  Epoch [1/5], Batch [63000/66650], Avg Loss: 0.4046\n",
      "  Epoch [1/5], Batch [64000/66650], Avg Loss: 0.4044\n",
      "  Epoch [1/5], Batch [65000/66650], Avg Loss: 0.4033\n",
      "  Epoch [1/5], Batch [66000/66650], Avg Loss: 0.4030\n",
      "  Epoch [1/5], Batch [66650/66650], Avg Loss: 0.4029\n",
      "Epoch [1/5] 完了, 平均損失: 0.4029\n",
      "  Epoch [1/5], 検証データ正解率: 80.39%\n",
      "  Epoch [2/5], Batch [1000/66650], Avg Loss: 0.3672\n",
      "  Epoch [2/5], Batch [2000/66650], Avg Loss: 0.3720\n",
      "  Epoch [2/5], Batch [3000/66650], Avg Loss: 0.3761\n",
      "  Epoch [2/5], Batch [4000/66650], Avg Loss: 0.3753\n",
      "  Epoch [2/5], Batch [5000/66650], Avg Loss: 0.3750\n",
      "  Epoch [2/5], Batch [6000/66650], Avg Loss: 0.3775\n",
      "  Epoch [2/5], Batch [7000/66650], Avg Loss: 0.3779\n",
      "  Epoch [2/5], Batch [8000/66650], Avg Loss: 0.3761\n",
      "  Epoch [2/5], Batch [9000/66650], Avg Loss: 0.3753\n",
      "  Epoch [2/5], Batch [10000/66650], Avg Loss: 0.3725\n",
      "  Epoch [2/5], Batch [11000/66650], Avg Loss: 0.3716\n",
      "  Epoch [2/5], Batch [12000/66650], Avg Loss: 0.3721\n",
      "  Epoch [2/5], Batch [13000/66650], Avg Loss: 0.3737\n",
      "  Epoch [2/5], Batch [14000/66650], Avg Loss: 0.3723\n",
      "  Epoch [2/5], Batch [15000/66650], Avg Loss: 0.3728\n",
      "  Epoch [2/5], Batch [16000/66650], Avg Loss: 0.3735\n",
      "  Epoch [2/5], Batch [17000/66650], Avg Loss: 0.3735\n",
      "  Epoch [2/5], Batch [18000/66650], Avg Loss: 0.3734\n",
      "  Epoch [2/5], Batch [19000/66650], Avg Loss: 0.3731\n",
      "  Epoch [2/5], Batch [20000/66650], Avg Loss: 0.3732\n",
      "  Epoch [2/5], Batch [21000/66650], Avg Loss: 0.3740\n",
      "  Epoch [2/5], Batch [22000/66650], Avg Loss: 0.3725\n",
      "  Epoch [2/5], Batch [23000/66650], Avg Loss: 0.3744\n",
      "  Epoch [2/5], Batch [24000/66650], Avg Loss: 0.3743\n",
      "  Epoch [2/5], Batch [25000/66650], Avg Loss: 0.3741\n",
      "  Epoch [2/5], Batch [26000/66650], Avg Loss: 0.3747\n",
      "  Epoch [2/5], Batch [27000/66650], Avg Loss: 0.3744\n",
      "  Epoch [2/5], Batch [28000/66650], Avg Loss: 0.3733\n",
      "  Epoch [2/5], Batch [29000/66650], Avg Loss: 0.3731\n",
      "  Epoch [2/5], Batch [30000/66650], Avg Loss: 0.3726\n",
      "  Epoch [2/5], Batch [31000/66650], Avg Loss: 0.3716\n",
      "  Epoch [2/5], Batch [32000/66650], Avg Loss: 0.3719\n",
      "  Epoch [2/5], Batch [33000/66650], Avg Loss: 0.3715\n",
      "  Epoch [2/5], Batch [34000/66650], Avg Loss: 0.3710\n",
      "  Epoch [2/5], Batch [35000/66650], Avg Loss: 0.3705\n",
      "  Epoch [2/5], Batch [36000/66650], Avg Loss: 0.3714\n",
      "  Epoch [2/5], Batch [37000/66650], Avg Loss: 0.3719\n",
      "  Epoch [2/5], Batch [38000/66650], Avg Loss: 0.3718\n",
      "  Epoch [2/5], Batch [39000/66650], Avg Loss: 0.3719\n",
      "  Epoch [2/5], Batch [40000/66650], Avg Loss: 0.3724\n",
      "  Epoch [2/5], Batch [41000/66650], Avg Loss: 0.3717\n",
      "  Epoch [2/5], Batch [42000/66650], Avg Loss: 0.3714\n",
      "  Epoch [2/5], Batch [43000/66650], Avg Loss: 0.3711\n",
      "  Epoch [2/5], Batch [44000/66650], Avg Loss: 0.3712\n",
      "  Epoch [2/5], Batch [45000/66650], Avg Loss: 0.3710\n",
      "  Epoch [2/5], Batch [46000/66650], Avg Loss: 0.3707\n",
      "  Epoch [2/5], Batch [47000/66650], Avg Loss: 0.3701\n",
      "  Epoch [2/5], Batch [48000/66650], Avg Loss: 0.3700\n",
      "  Epoch [2/5], Batch [49000/66650], Avg Loss: 0.3702\n",
      "  Epoch [2/5], Batch [50000/66650], Avg Loss: 0.3701\n",
      "  Epoch [2/5], Batch [51000/66650], Avg Loss: 0.3703\n",
      "  Epoch [2/5], Batch [52000/66650], Avg Loss: 0.3700\n",
      "  Epoch [2/5], Batch [53000/66650], Avg Loss: 0.3704\n",
      "  Epoch [2/5], Batch [54000/66650], Avg Loss: 0.3704\n",
      "  Epoch [2/5], Batch [55000/66650], Avg Loss: 0.3702\n",
      "  Epoch [2/5], Batch [56000/66650], Avg Loss: 0.3701\n",
      "  Epoch [2/5], Batch [57000/66650], Avg Loss: 0.3707\n",
      "  Epoch [2/5], Batch [58000/66650], Avg Loss: 0.3711\n",
      "  Epoch [2/5], Batch [59000/66650], Avg Loss: 0.3712\n",
      "  Epoch [2/5], Batch [60000/66650], Avg Loss: 0.3718\n",
      "  Epoch [2/5], Batch [61000/66650], Avg Loss: 0.3716\n",
      "  Epoch [2/5], Batch [62000/66650], Avg Loss: 0.3717\n",
      "  Epoch [2/5], Batch [63000/66650], Avg Loss: 0.3717\n",
      "  Epoch [2/5], Batch [64000/66650], Avg Loss: 0.3720\n",
      "  Epoch [2/5], Batch [65000/66650], Avg Loss: 0.3722\n",
      "  Epoch [2/5], Batch [66000/66650], Avg Loss: 0.3722\n",
      "  Epoch [2/5], Batch [66650/66650], Avg Loss: 0.3722\n",
      "Epoch [2/5] 完了, 平均損失: 0.3722\n",
      "  Epoch [2/5], 検証データ正解率: 80.85%\n",
      "  Epoch [3/5], Batch [1000/66650], Avg Loss: 0.3877\n",
      "  Epoch [3/5], Batch [2000/66650], Avg Loss: 0.3772\n",
      "  Epoch [3/5], Batch [3000/66650], Avg Loss: 0.3716\n",
      "  Epoch [3/5], Batch [4000/66650], Avg Loss: 0.3695\n",
      "  Epoch [3/5], Batch [5000/66650], Avg Loss: 0.3704\n",
      "  Epoch [3/5], Batch [6000/66650], Avg Loss: 0.3711\n",
      "  Epoch [3/5], Batch [7000/66650], Avg Loss: 0.3694\n",
      "  Epoch [3/5], Batch [8000/66650], Avg Loss: 0.3720\n",
      "  Epoch [3/5], Batch [9000/66650], Avg Loss: 0.3692\n",
      "  Epoch [3/5], Batch [10000/66650], Avg Loss: 0.3716\n",
      "  Epoch [3/5], Batch [11000/66650], Avg Loss: 0.3717\n",
      "  Epoch [3/5], Batch [12000/66650], Avg Loss: 0.3699\n",
      "  Epoch [3/5], Batch [13000/66650], Avg Loss: 0.3697\n",
      "  Epoch [3/5], Batch [14000/66650], Avg Loss: 0.3696\n",
      "  Epoch [3/5], Batch [15000/66650], Avg Loss: 0.3712\n",
      "  Epoch [3/5], Batch [16000/66650], Avg Loss: 0.3700\n",
      "  Epoch [3/5], Batch [17000/66650], Avg Loss: 0.3711\n",
      "  Epoch [3/5], Batch [18000/66650], Avg Loss: 0.3713\n",
      "  Epoch [3/5], Batch [19000/66650], Avg Loss: 0.3712\n",
      "  Epoch [3/5], Batch [20000/66650], Avg Loss: 0.3713\n",
      "  Epoch [3/5], Batch [21000/66650], Avg Loss: 0.3716\n",
      "  Epoch [3/5], Batch [22000/66650], Avg Loss: 0.3708\n",
      "  Epoch [3/5], Batch [23000/66650], Avg Loss: 0.3708\n",
      "  Epoch [3/5], Batch [24000/66650], Avg Loss: 0.3711\n",
      "  Epoch [3/5], Batch [25000/66650], Avg Loss: 0.3712\n",
      "  Epoch [3/5], Batch [26000/66650], Avg Loss: 0.3695\n",
      "  Epoch [3/5], Batch [27000/66650], Avg Loss: 0.3696\n",
      "  Epoch [3/5], Batch [28000/66650], Avg Loss: 0.3702\n",
      "  Epoch [3/5], Batch [29000/66650], Avg Loss: 0.3696\n",
      "  Epoch [3/5], Batch [30000/66650], Avg Loss: 0.3696\n",
      "  Epoch [3/5], Batch [31000/66650], Avg Loss: 0.3695\n",
      "  Epoch [3/5], Batch [32000/66650], Avg Loss: 0.3697\n",
      "  Epoch [3/5], Batch [33000/66650], Avg Loss: 0.3697\n",
      "  Epoch [3/5], Batch [34000/66650], Avg Loss: 0.3694\n",
      "  Epoch [3/5], Batch [35000/66650], Avg Loss: 0.3695\n",
      "  Epoch [3/5], Batch [36000/66650], Avg Loss: 0.3694\n",
      "  Epoch [3/5], Batch [37000/66650], Avg Loss: 0.3694\n",
      "  Epoch [3/5], Batch [38000/66650], Avg Loss: 0.3691\n",
      "  Epoch [3/5], Batch [39000/66650], Avg Loss: 0.3691\n",
      "  Epoch [3/5], Batch [40000/66650], Avg Loss: 0.3695\n",
      "  Epoch [3/5], Batch [41000/66650], Avg Loss: 0.3688\n",
      "  Epoch [3/5], Batch [42000/66650], Avg Loss: 0.3692\n",
      "  Epoch [3/5], Batch [43000/66650], Avg Loss: 0.3696\n",
      "  Epoch [3/5], Batch [44000/66650], Avg Loss: 0.3689\n",
      "  Epoch [3/5], Batch [45000/66650], Avg Loss: 0.3691\n",
      "  Epoch [3/5], Batch [46000/66650], Avg Loss: 0.3692\n",
      "  Epoch [3/5], Batch [47000/66650], Avg Loss: 0.3689\n",
      "  Epoch [3/5], Batch [48000/66650], Avg Loss: 0.3693\n",
      "  Epoch [3/5], Batch [49000/66650], Avg Loss: 0.3690\n",
      "  Epoch [3/5], Batch [50000/66650], Avg Loss: 0.3687\n",
      "  Epoch [3/5], Batch [51000/66650], Avg Loss: 0.3687\n",
      "  Epoch [3/5], Batch [52000/66650], Avg Loss: 0.3685\n",
      "  Epoch [3/5], Batch [53000/66650], Avg Loss: 0.3684\n",
      "  Epoch [3/5], Batch [54000/66650], Avg Loss: 0.3689\n",
      "  Epoch [3/5], Batch [55000/66650], Avg Loss: 0.3686\n",
      "  Epoch [3/5], Batch [56000/66650], Avg Loss: 0.3689\n",
      "  Epoch [3/5], Batch [57000/66650], Avg Loss: 0.3695\n",
      "  Epoch [3/5], Batch [58000/66650], Avg Loss: 0.3692\n",
      "  Epoch [3/5], Batch [59000/66650], Avg Loss: 0.3694\n",
      "  Epoch [3/5], Batch [60000/66650], Avg Loss: 0.3693\n",
      "  Epoch [3/5], Batch [61000/66650], Avg Loss: 0.3692\n",
      "  Epoch [3/5], Batch [62000/66650], Avg Loss: 0.3694\n",
      "  Epoch [3/5], Batch [63000/66650], Avg Loss: 0.3695\n",
      "  Epoch [3/5], Batch [64000/66650], Avg Loss: 0.3699\n",
      "  Epoch [3/5], Batch [65000/66650], Avg Loss: 0.3698\n",
      "  Epoch [3/5], Batch [66000/66650], Avg Loss: 0.3697\n",
      "  Epoch [3/5], Batch [66650/66650], Avg Loss: 0.3699\n",
      "Epoch [3/5] 完了, 平均損失: 0.3699\n",
      "  Epoch [3/5], 検証データ正解率: 80.96%\n",
      "  Epoch [4/5], Batch [1000/66650], Avg Loss: 0.3594\n",
      "  Epoch [4/5], Batch [2000/66650], Avg Loss: 0.3553\n",
      "  Epoch [4/5], Batch [3000/66650], Avg Loss: 0.3597\n",
      "  Epoch [4/5], Batch [4000/66650], Avg Loss: 0.3591\n",
      "  Epoch [4/5], Batch [5000/66650], Avg Loss: 0.3619\n",
      "  Epoch [4/5], Batch [6000/66650], Avg Loss: 0.3607\n",
      "  Epoch [4/5], Batch [7000/66650], Avg Loss: 0.3598\n",
      "  Epoch [4/5], Batch [8000/66650], Avg Loss: 0.3633\n",
      "  Epoch [4/5], Batch [9000/66650], Avg Loss: 0.3667\n",
      "  Epoch [4/5], Batch [10000/66650], Avg Loss: 0.3661\n",
      "  Epoch [4/5], Batch [11000/66650], Avg Loss: 0.3665\n",
      "  Epoch [4/5], Batch [12000/66650], Avg Loss: 0.3654\n",
      "  Epoch [4/5], Batch [13000/66650], Avg Loss: 0.3656\n",
      "  Epoch [4/5], Batch [14000/66650], Avg Loss: 0.3674\n",
      "  Epoch [4/5], Batch [15000/66650], Avg Loss: 0.3659\n",
      "  Epoch [4/5], Batch [16000/66650], Avg Loss: 0.3668\n",
      "  Epoch [4/5], Batch [17000/66650], Avg Loss: 0.3675\n",
      "  Epoch [4/5], Batch [18000/66650], Avg Loss: 0.3676\n",
      "  Epoch [4/5], Batch [19000/66650], Avg Loss: 0.3676\n",
      "  Epoch [4/5], Batch [20000/66650], Avg Loss: 0.3671\n",
      "  Epoch [4/5], Batch [21000/66650], Avg Loss: 0.3670\n",
      "  Epoch [4/5], Batch [22000/66650], Avg Loss: 0.3667\n",
      "  Epoch [4/5], Batch [23000/66650], Avg Loss: 0.3667\n",
      "  Epoch [4/5], Batch [24000/66650], Avg Loss: 0.3664\n",
      "  Epoch [4/5], Batch [25000/66650], Avg Loss: 0.3672\n",
      "  Epoch [4/5], Batch [26000/66650], Avg Loss: 0.3677\n",
      "  Epoch [4/5], Batch [27000/66650], Avg Loss: 0.3686\n",
      "  Epoch [4/5], Batch [28000/66650], Avg Loss: 0.3687\n",
      "  Epoch [4/5], Batch [29000/66650], Avg Loss: 0.3682\n",
      "  Epoch [4/5], Batch [30000/66650], Avg Loss: 0.3691\n",
      "  Epoch [4/5], Batch [31000/66650], Avg Loss: 0.3690\n",
      "  Epoch [4/5], Batch [32000/66650], Avg Loss: 0.3688\n",
      "  Epoch [4/5], Batch [33000/66650], Avg Loss: 0.3681\n",
      "  Epoch [4/5], Batch [34000/66650], Avg Loss: 0.3681\n",
      "  Epoch [4/5], Batch [35000/66650], Avg Loss: 0.3687\n",
      "  Epoch [4/5], Batch [36000/66650], Avg Loss: 0.3689\n",
      "  Epoch [4/5], Batch [37000/66650], Avg Loss: 0.3679\n",
      "  Epoch [4/5], Batch [38000/66650], Avg Loss: 0.3684\n",
      "  Epoch [4/5], Batch [39000/66650], Avg Loss: 0.3682\n",
      "  Epoch [4/5], Batch [40000/66650], Avg Loss: 0.3687\n",
      "  Epoch [4/5], Batch [41000/66650], Avg Loss: 0.3685\n",
      "  Epoch [4/5], Batch [42000/66650], Avg Loss: 0.3686\n",
      "  Epoch [4/5], Batch [43000/66650], Avg Loss: 0.3687\n",
      "  Epoch [4/5], Batch [44000/66650], Avg Loss: 0.3692\n",
      "  Epoch [4/5], Batch [45000/66650], Avg Loss: 0.3694\n",
      "  Epoch [4/5], Batch [46000/66650], Avg Loss: 0.3701\n",
      "  Epoch [4/5], Batch [47000/66650], Avg Loss: 0.3693\n",
      "  Epoch [4/5], Batch [48000/66650], Avg Loss: 0.3695\n",
      "  Epoch [4/5], Batch [49000/66650], Avg Loss: 0.3697\n",
      "  Epoch [4/5], Batch [50000/66650], Avg Loss: 0.3698\n",
      "  Epoch [4/5], Batch [51000/66650], Avg Loss: 0.3696\n",
      "  Epoch [4/5], Batch [52000/66650], Avg Loss: 0.3697\n",
      "  Epoch [4/5], Batch [53000/66650], Avg Loss: 0.3697\n",
      "  Epoch [4/5], Batch [54000/66650], Avg Loss: 0.3700\n",
      "  Epoch [4/5], Batch [55000/66650], Avg Loss: 0.3696\n",
      "  Epoch [4/5], Batch [56000/66650], Avg Loss: 0.3700\n",
      "  Epoch [4/5], Batch [57000/66650], Avg Loss: 0.3694\n",
      "  Epoch [4/5], Batch [58000/66650], Avg Loss: 0.3695\n",
      "  Epoch [4/5], Batch [59000/66650], Avg Loss: 0.3698\n",
      "  Epoch [4/5], Batch [60000/66650], Avg Loss: 0.3706\n",
      "  Epoch [4/5], Batch [61000/66650], Avg Loss: 0.3705\n",
      "  Epoch [4/5], Batch [62000/66650], Avg Loss: 0.3703\n",
      "  Epoch [4/5], Batch [63000/66650], Avg Loss: 0.3704\n",
      "  Epoch [4/5], Batch [64000/66650], Avg Loss: 0.3701\n",
      "  Epoch [4/5], Batch [65000/66650], Avg Loss: 0.3702\n",
      "  Epoch [4/5], Batch [66000/66650], Avg Loss: 0.3697\n",
      "  Epoch [4/5], Batch [66650/66650], Avg Loss: 0.3692\n",
      "Epoch [4/5] 完了, 平均損失: 0.3692\n",
      "  Epoch [4/5], 検証データ正解率: 80.16%\n",
      "  Epoch [5/5], Batch [1000/66650], Avg Loss: 0.3626\n",
      "  Epoch [5/5], Batch [2000/66650], Avg Loss: 0.3784\n",
      "  Epoch [5/5], Batch [3000/66650], Avg Loss: 0.3637\n",
      "  Epoch [5/5], Batch [4000/66650], Avg Loss: 0.3627\n",
      "  Epoch [5/5], Batch [5000/66650], Avg Loss: 0.3634\n",
      "  Epoch [5/5], Batch [6000/66650], Avg Loss: 0.3608\n",
      "  Epoch [5/5], Batch [7000/66650], Avg Loss: 0.3649\n",
      "  Epoch [5/5], Batch [8000/66650], Avg Loss: 0.3631\n",
      "  Epoch [5/5], Batch [9000/66650], Avg Loss: 0.3636\n",
      "  Epoch [5/5], Batch [10000/66650], Avg Loss: 0.3625\n",
      "  Epoch [5/5], Batch [11000/66650], Avg Loss: 0.3622\n",
      "  Epoch [5/5], Batch [12000/66650], Avg Loss: 0.3625\n",
      "  Epoch [5/5], Batch [13000/66650], Avg Loss: 0.3627\n",
      "  Epoch [5/5], Batch [14000/66650], Avg Loss: 0.3649\n",
      "  Epoch [5/5], Batch [15000/66650], Avg Loss: 0.3663\n",
      "  Epoch [5/5], Batch [16000/66650], Avg Loss: 0.3684\n",
      "  Epoch [5/5], Batch [17000/66650], Avg Loss: 0.3669\n",
      "  Epoch [5/5], Batch [18000/66650], Avg Loss: 0.3676\n",
      "  Epoch [5/5], Batch [19000/66650], Avg Loss: 0.3675\n",
      "  Epoch [5/5], Batch [20000/66650], Avg Loss: 0.3677\n",
      "  Epoch [5/5], Batch [21000/66650], Avg Loss: 0.3669\n",
      "  Epoch [5/5], Batch [22000/66650], Avg Loss: 0.3682\n",
      "  Epoch [5/5], Batch [23000/66650], Avg Loss: 0.3677\n",
      "  Epoch [5/5], Batch [24000/66650], Avg Loss: 0.3672\n",
      "  Epoch [5/5], Batch [25000/66650], Avg Loss: 0.3672\n",
      "  Epoch [5/5], Batch [26000/66650], Avg Loss: 0.3672\n",
      "  Epoch [5/5], Batch [27000/66650], Avg Loss: 0.3677\n",
      "  Epoch [5/5], Batch [28000/66650], Avg Loss: 0.3677\n",
      "  Epoch [5/5], Batch [29000/66650], Avg Loss: 0.3675\n",
      "  Epoch [5/5], Batch [30000/66650], Avg Loss: 0.3682\n",
      "  Epoch [5/5], Batch [31000/66650], Avg Loss: 0.3678\n",
      "  Epoch [5/5], Batch [32000/66650], Avg Loss: 0.3666\n",
      "  Epoch [5/5], Batch [33000/66650], Avg Loss: 0.3666\n",
      "  Epoch [5/5], Batch [34000/66650], Avg Loss: 0.3662\n",
      "  Epoch [5/5], Batch [35000/66650], Avg Loss: 0.3656\n",
      "  Epoch [5/5], Batch [36000/66650], Avg Loss: 0.3663\n",
      "  Epoch [5/5], Batch [37000/66650], Avg Loss: 0.3665\n",
      "  Epoch [5/5], Batch [38000/66650], Avg Loss: 0.3664\n",
      "  Epoch [5/5], Batch [39000/66650], Avg Loss: 0.3656\n",
      "  Epoch [5/5], Batch [40000/66650], Avg Loss: 0.3662\n",
      "  Epoch [5/5], Batch [41000/66650], Avg Loss: 0.3662\n",
      "  Epoch [5/5], Batch [42000/66650], Avg Loss: 0.3665\n",
      "  Epoch [5/5], Batch [43000/66650], Avg Loss: 0.3662\n",
      "  Epoch [5/5], Batch [44000/66650], Avg Loss: 0.3664\n",
      "  Epoch [5/5], Batch [45000/66650], Avg Loss: 0.3666\n",
      "  Epoch [5/5], Batch [46000/66650], Avg Loss: 0.3669\n",
      "  Epoch [5/5], Batch [47000/66650], Avg Loss: 0.3669\n",
      "  Epoch [5/5], Batch [48000/66650], Avg Loss: 0.3662\n",
      "  Epoch [5/5], Batch [49000/66650], Avg Loss: 0.3663\n",
      "  Epoch [5/5], Batch [50000/66650], Avg Loss: 0.3666\n",
      "  Epoch [5/5], Batch [51000/66650], Avg Loss: 0.3669\n",
      "  Epoch [5/5], Batch [52000/66650], Avg Loss: 0.3664\n",
      "  Epoch [5/5], Batch [53000/66650], Avg Loss: 0.3665\n",
      "  Epoch [5/5], Batch [54000/66650], Avg Loss: 0.3656\n",
      "  Epoch [5/5], Batch [55000/66650], Avg Loss: 0.3660\n",
      "  Epoch [5/5], Batch [56000/66650], Avg Loss: 0.3666\n",
      "  Epoch [5/5], Batch [57000/66650], Avg Loss: 0.3665\n",
      "  Epoch [5/5], Batch [58000/66650], Avg Loss: 0.3673\n",
      "  Epoch [5/5], Batch [59000/66650], Avg Loss: 0.3675\n",
      "  Epoch [5/5], Batch [60000/66650], Avg Loss: 0.3676\n",
      "  Epoch [5/5], Batch [61000/66650], Avg Loss: 0.3684\n",
      "  Epoch [5/5], Batch [62000/66650], Avg Loss: 0.3682\n",
      "  Epoch [5/5], Batch [63000/66650], Avg Loss: 0.3680\n",
      "  Epoch [5/5], Batch [64000/66650], Avg Loss: 0.3683\n",
      "  Epoch [5/5], Batch [65000/66650], Avg Loss: 0.3687\n",
      "  Epoch [5/5], Batch [66000/66650], Avg Loss: 0.3688\n",
      "  Epoch [5/5], Batch [66650/66650], Avg Loss: 0.3686\n",
      "Epoch [5/5] 完了, 平均損失: 0.3686\n",
      "  Epoch [5/5], 検証データ正解率: 80.28%\n",
      "\n",
      "モデルの学習が完了しました。\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset # DataLoaderを使用する場合\n",
    "\n",
    "# --- 前提となる変数 ---\n",
    "# model_bow: 問題72で作成した SimpleBOWClassifier のインスタンス\n",
    "# train_dataset_processed: 問題71で作成した訓練データのリスト\n",
    "# dev_dataset_processed: 問題71で作成した開発データのリスト (エポックごとの評価用)\n",
    "\n",
    "# これらの変数が現在のセッションに存在することを確認してください。\n",
    "if 'model_bow' not in locals() or \\\n",
    "   'train_dataset_processed' not in locals() or not train_dataset_processed:\n",
    "    print(\"エラー: 'model_bow' または 'train_dataset_processed' が定義されていないか、データが空です。\")\n",
    "    print(\"問題71および72を先に実行して、これらの変数を準備してください。\")\n",
    "    raise NameError(\"Required variables/data not defined or empty.\")\n",
    "\n",
    "# PyTorchのDatasetクラスを作成 (DataLoaderで使いやすくするため)\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, processed_data):\n",
    "        self.data = processed_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]['input_ids'], self.data[idx]['label']\n",
    "\n",
    "# データセットとデータローダーの準備\n",
    "# ここではバッチサイズ1で、各事例を個別に処理する形をまず示します。\n",
    "# 問題75, 76で本格的なパディングとミニバッチを導入します。\n",
    "batch_size = 1 # まずは1で（問題76で本格的なバッチ処理）\n",
    "\n",
    "train_torch_dataset = SentimentDataset(train_dataset_processed)\n",
    "train_dataloader = DataLoader(train_torch_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# (オプション) 検証用データローダー\n",
    "if 'dev_dataset_processed' in locals() and dev_dataset_processed:\n",
    "    dev_torch_dataset = SentimentDataset(dev_dataset_processed)\n",
    "    dev_dataloader = DataLoader(dev_torch_dataset, batch_size=batch_size) # シャッフルは不要\n",
    "\n",
    "# 学習パラメータ\n",
    "learning_rate = 1e-3\n",
    "num_epochs = 5 # 学習エポック数 (適宜調整)\n",
    "\n",
    "# 損失関数と最適化アルゴリズム\n",
    "# モデルの出力がロジットなので BCEWithLogitsLoss を使用\n",
    "criterion = nn.BCEWithLogitsLoss() \n",
    "# 最適化対象はモデルの全パラメータのうち requires_grad=True のもの\n",
    "# SimpleBOWClassifierでは埋め込み層の重みは requires_grad=False に設定済み\n",
    "optimizer = optim.Adam(model_bow.parameters(), lr=learning_rate) \n",
    "# もしfc層のパラメータのみを更新対象とするなら:\n",
    "# optimizer = optim.Adam(model_bow.fc.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "print(\"モデルの学習を開始します...\")\n",
    "# --- 学習ループ ---\n",
    "for epoch in range(num_epochs):\n",
    "    model_bow.train() # モデルを学習モードに設定\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    num_processed_samples = 0\n",
    "    \n",
    "    for i, (input_ids_batch, labels_batch) in enumerate(train_dataloader):\n",
    "        # input_ids_batch: (batch_size, seq_len)\n",
    "        # labels_batch: (batch_size, 1)\n",
    "        \n",
    "        # 勾配を初期化\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 順伝播\n",
    "        outputs = model_bow(input_ids_batch) # outputs: (batch_size, 1)\n",
    "        \n",
    "        # 損失計算\n",
    "        loss = criterion(outputs, labels_batch) # labels_batchもoutputsと同じ形状・型である必要がある\n",
    "        \n",
    "        # 誤差逆伝播\n",
    "        loss.backward()\n",
    "        \n",
    "        # パラメータ更新\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * input_ids_batch.size(0)\n",
    "        num_processed_samples += input_ids_batch.size(0)\n",
    "        \n",
    "        # 学習の進捗を表示 (例: 1000バッチごと)\n",
    "        if (i + 1) % 1000 == 0 or batch_size * (i + 1) >= len(train_torch_dataset) :\n",
    "            avg_loss_so_far = running_loss / num_processed_samples\n",
    "            print(f\"  Epoch [{epoch+1}/{num_epochs}], Batch [{i+1}/{len(train_dataloader)}], Avg Loss: {avg_loss_so_far:.4f}\")\n",
    "\n",
    "    epoch_loss = running_loss / len(train_torch_dataset)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] 完了, 平均損失: {epoch_loss:.4f}\")\n",
    "\n",
    "    # (オプション) 各エポックの終わりに検証データで性能を軽く評価\n",
    "    if 'dev_dataloader' in locals():\n",
    "        model_bow.eval() # モデルを評価モードに\n",
    "        correct_dev = 0\n",
    "        total_dev = 0\n",
    "        with torch.no_grad(): # 勾配計算をしない\n",
    "            for input_ids_dev, labels_dev in dev_dataloader:\n",
    "                outputs_dev = model_bow(input_ids_dev)\n",
    "                predicted_probs = torch.sigmoid(outputs_dev) # ロジットを確率に変換\n",
    "                predicted_labels = (predicted_probs > 0.5).float() # 0.5を閾値として0 or 1に\n",
    "                total_dev += labels_dev.size(0)\n",
    "                correct_dev += (predicted_labels == labels_dev).sum().item()\n",
    "        dev_accuracy = 100 * correct_dev / total_dev if total_dev > 0 else 0\n",
    "        print(f\"  Epoch [{epoch+1}/{num_epochs}], 検証データ正解率: {dev_accuracy:.2f}%\")\n",
    "\n",
    "\n",
    "print(\"\\nモデルの学習が完了しました。\")\n",
    "# 学習済みの model_bow は次の問題74で使用します。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b25b5b-0ed2-4bf0-8350-601812eb057f",
   "metadata": {
    "id": "26b25b5b-0ed2-4bf0-8350-601812eb057f"
   },
   "source": [
    "## 74. モデルの評価\n",
    "\n",
    "問題73で学習したモデルの開発セットにおける正解率を求めよ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae550b3f-91fb-4587-96ca-d2d54877d268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "学習済みモデルを開発データで評価します...\n",
      "\n",
      "開発データにおける正解率: 80.28%\n",
      "  正解した事例数: 700\n",
      "  総事例数: 872\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset # DataLoaderを再度使う場合\n",
    "\n",
    "# --- 前提となる変数 ---\n",
    "# model_bow: 問題73で学習済みの SimpleBOWClassifier のインスタンス\n",
    "# dev_dataset_processed: 問題71で作成した開発データのリスト\n",
    "# SentimentDataset: 問題73で定義したカスタムDatasetクラス (DataLoaderを使う場合)\n",
    "\n",
    "# これらの変数が現在のセッションに存在することを確認してください。\n",
    "if 'model_bow' not in locals() or \\\n",
    "   'dev_dataset_processed' not in locals() or not dev_dataset_processed:\n",
    "    print(\"エラー: 'model_bow' または 'dev_dataset_processed' が定義されていないか、データが空です。\")\n",
    "    print(\"問題71および73を先に実行して、これらの変数とデータを準備してください。\")\n",
    "    raise NameError(\"Required variables/data not defined or empty.\")\n",
    "\n",
    "if 'SentimentDataset' not in locals(): # もし SentimentDatasetクラスが未定義なら問題73からコピー\n",
    "    class SentimentDataset(Dataset):\n",
    "        def __init__(self, processed_data):\n",
    "            self.data = processed_data\n",
    "        def __len__(self):\n",
    "            return len(self.data)\n",
    "        def __getitem__(self, idx):\n",
    "            return self.data[idx]['input_ids'], self.data[idx]['label']\n",
    "\n",
    "# 開発（検証）データローダーの準備 (問題73で作成していればそれを再利用可)\n",
    "# ここでは改めて作成する例を示します。バッチサイズは評価時には大きくても問題ないことが多いです。\n",
    "dev_batch_size = 1 # 評価時のバッチサイズ (適宜調整)\n",
    "dev_torch_dataset_eval = SentimentDataset(dev_dataset_processed)\n",
    "dev_dataloader_eval = DataLoader(dev_torch_dataset_eval, batch_size=dev_batch_size)\n",
    "\n",
    "print(\"\\n学習済みモデルを開発データで評価します...\")\n",
    "\n",
    "# モデルを評価モードに設定\n",
    "model_bow.eval()\n",
    "\n",
    "total_correct_dev = 0\n",
    "total_samples_dev = 0\n",
    "\n",
    "# 勾配計算を無効にするコンテキスト\n",
    "with torch.no_grad():\n",
    "    for input_ids_batch, labels_batch in dev_dataloader_eval:\n",
    "        # input_ids_batch: (batch_size, seq_len)\n",
    "        # labels_batch: (batch_size, 1)\n",
    "        \n",
    "        # 順伝播\n",
    "        outputs = model_bow(input_ids_batch) # outputs: (batch_size, 1) - ロジット\n",
    "        \n",
    "        # ロジットを確率に変換 (0から1の範囲)\n",
    "        predicted_probs = torch.sigmoid(outputs)\n",
    "        \n",
    "        # 確率を0.5を閾値として0または1のラベルに変換\n",
    "        predicted_labels = (predicted_probs > 0.5).float() # .float() でラベルと同じ型に\n",
    "        \n",
    "        # 正解数をカウント\n",
    "        total_correct_dev += (predicted_labels == labels_batch).sum().item()\n",
    "        total_samples_dev += labels_batch.size(0)\n",
    "\n",
    "# 正解率の計算\n",
    "if total_samples_dev > 0:\n",
    "    accuracy_dev = 100 * total_correct_dev / total_samples_dev\n",
    "    print(f\"\\n開発データにおける正解率: {accuracy_dev:.2f}%\")\n",
    "    print(f\"  正解した事例数: {total_correct_dev}\")\n",
    "    print(f\"  総事例数: {total_samples_dev}\")\n",
    "else:\n",
    "    print(\"開発データが空であるか、処理できませんでした。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "O08V9g0mcJwe",
   "metadata": {
    "id": "O08V9g0mcJwe"
   },
   "source": [
    "## 75. パディング\n",
    "\n",
    "複数の事例が与えられたとき、これらをまとめて一つのテンソル・オブジェクトで表現する関数`collate`を実装せよ。与えられた複数の事例のトークン列の長さが異なるときは、トークン列の長さが最も長いものに揃え、0番のトークンIDでパディングをせよ。さらに、トークン列の長さが長いものから順に、事例を並び替えよ。\n",
    "\n",
    "例えば、訓練データセットの冒頭の4事例が次のように表されているとき、\n",
    "\n",
    "```\n",
    "[{'text': 'hide new secretions from the parental units',\n",
    "  'label': tensor([0.]),\n",
    "  'input_ids': tensor([  5785,     66, 113845,     18,     12,  15095,   1594])},\n",
    " {'text': 'contains no wit , only labored gags',\n",
    "  'label': tensor([0.]),\n",
    "  'input_ids': tensor([ 3475,    87, 15888,    90, 27695, 42637])},\n",
    " {'text': 'that loves its characters and communicates something rather beautiful about human nature',\n",
    "  'label': tensor([1.]),\n",
    "  'input_ids': tensor([    4,  5053,    45,  3305, 31647,   348,   904,  2815,    47,  1276,  1964])},\n",
    " {'text': 'remains utterly satisfied to remain the same throughout',\n",
    "  'label': tensor([0.]),\n",
    "  'input_ids': tensor([  987, 14528,  4941,   873,    12,   208,   898])}]\n",
    "```\n",
    "\n",
    "`collate`関数を通した結果は以下のようになることが想定される。\n",
    "\n",
    "```\n",
    "{'input_ids': tensor([\n",
    "    [     4,   5053,     45,   3305,  31647,    348,    904,   2815,     47,   1276,   1964],\n",
    "    [  5785,     66, 113845,     18,     12,  15095,   1594,      0,      0,      0,      0],\n",
    "    [   987,  14528,   4941,    873,     12,    208,    898,      0,      0,      0,      0],\n",
    "    [  3475,     87,  15888,     90,  27695,  42637,      0,      0,      0,      0,      0]]),\n",
    " 'label': tensor([\n",
    "    [1.],\n",
    "    [0.],\n",
    "    [0.],\n",
    "    [0.]])}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "026c23cf-a113-4cdf-aabf-720454559e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collate関数の動作テストを行います。\n",
      "\n",
      "訓練データの最初の4事例を使ってテストします。\n",
      "\n",
      "collate関数を通した結果:\n",
      "input_ids (パディング・ソート済み):\n",
      "tensor([[     4,   5053,     45,   3305,  31647,    348,    904,   2815,     47,\n",
      "           1276,   1964],\n",
      "        [  5785,     66, 113845,     18,     12,  15095,   1594,      0,      0,\n",
      "              0,      0],\n",
      "        [   987,  14528,   4941,    873,     12,    208,    898,      0,      0,\n",
      "              0,      0],\n",
      "        [  3475,     87,  15888,     90,  27695,  42637,      0,      0,      0,\n",
      "              0,      0]])\n",
      "input_ids の形状: torch.Size([4, 11])\n",
      "\n",
      "label (ソート済み):\n",
      "tensor([[1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]])\n",
      "label の形状: torch.Size([4, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence # パディングに便利\n",
    "from torch.utils.data import DataLoader, Dataset # DataLoaderのテスト用\n",
    "\n",
    "# --- 前提となる変数・クラス ---\n",
    "# PAD_ID = 0 (問題70で定義)\n",
    "# SentimentDataset クラス (問題73または74で定義)\n",
    "# train_dataset_processed (問題71で作成)\n",
    "\n",
    "# これらの変数が現在のセッションに存在することを確認してください。\n",
    "if 'PAD_ID' not in locals():\n",
    "    print(\"エラー: 'PAD_ID' が定義されていません。問題70を先に実行してください。\")\n",
    "    raise NameError(\"PAD_ID is not defined.\")\n",
    "if 'SentimentDataset' not in locals():\n",
    "    print(\"エラー: 'SentimentDataset' クラスが定義されていません。問題73または74のコードを確認してください。\")\n",
    "    # もし未定義なら、ここで再度定義するか、該当セルを実行\n",
    "    class SentimentDataset(Dataset): # 再定義の例\n",
    "        def __init__(self, processed_data):\n",
    "            self.data = processed_data\n",
    "        def __len__(self):\n",
    "            return len(self.data)\n",
    "        def __getitem__(self, idx):\n",
    "            # 各要素は {'input_ids': tensor, 'label': tensor} の辞書\n",
    "            # collate_fn が (input_ids, label) のタプルのリストを期待する場合\n",
    "            return self.data[idx]['input_ids'], self.data[idx]['label']\n",
    "if 'train_dataset_processed' not in locals() or not train_dataset_processed:\n",
    "    print(\"エラー: 'train_dataset_processed' が定義されていないか空です。問題71を実行してください。\")\n",
    "    raise NameError(\"train_dataset_processed is not defined or empty.\")\n",
    "\n",
    "# --- ここから問題75の collate 関数の実装 ---\n",
    "\n",
    "def collate_fn_custom_pad_sort(batch, padding_value=PAD_ID):\n",
    "    \"\"\"\n",
    "    バッチ内の事例を処理し、パディングとソートを行うcollate関数。\n",
    "    Args:\n",
    "        batch (list of tuples): Datasetの__getitem__が返す要素のリスト。\n",
    "                                 各タプルは (input_ids_tensor, label_tensor)。\n",
    "        padding_value (int): パディングに使用するトークンID。\n",
    "    Returns:\n",
    "        dict: {'input_ids': パディング・ソート済みinput_idsバッチ (tensor),\n",
    "               'label': ソート済みlabelバッチ (tensor)}\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. input_ids と label をそれぞれのリストに分離\n",
    "    input_ids_list = [item[0] for item in batch]\n",
    "    labels_list = [item[1] for item in batch]\n",
    "    \n",
    "    # 2. input_ids の長さを取得し、それに基づいて事例を降順にソート\n",
    "    #    (input_ids, label, length) のタプルリストを作成\n",
    "    lengths = [len(ids) for ids in input_ids_list]\n",
    "    # ソートキーとなる長さと共に元のデータを保持\n",
    "    # (元のインデックスも保持しておくと、後でソートを戻す場合に役立つこともあるが、今回は不要)\n",
    "    batch_with_lengths = sorted(zip(input_ids_list, labels_list, lengths), \n",
    "                                key=lambda x: x[2], \n",
    "                                reverse=True)\n",
    "    \n",
    "    # ソートされた input_ids と label を再度取り出す\n",
    "    sorted_input_ids = [item[0] for item in batch_with_lengths]\n",
    "    sorted_labels = [item[1] for item in batch_with_lengths]\n",
    "    # sorted_lengths = [item[2] for item in batch_with_lengths] # 必要であれば長さも保持\n",
    "\n",
    "    # 3. パディング (ソート済みの input_ids_list に対して)\n",
    "    # torch.nn.utils.rnn.pad_sequence はテンソルのリストを受け取る\n",
    "    # batch_first=True で出力形状が (バッチサイズ, 最大シーケンス長) になる\n",
    "    input_ids_padded = pad_sequence(sorted_input_ids, batch_first=True, padding_value=padding_value)\n",
    "    \n",
    "    # 4. ラベルをテンソルにスタック (ソート済みの labels_list に対して)\n",
    "    # labels_list の各要素は既にテンソル (例: tensor([0.])) なので、torch.stack でまとめる\n",
    "    labels_batched = torch.stack(sorted_labels)\n",
    "    \n",
    "    return {'input_ids': input_ids_padded, 'label': labels_batched}\n",
    "\n",
    "\n",
    "# --- collate関数の動作テスト ---\n",
    "print(\"collate関数の動作テストを行います。\")\n",
    "\n",
    "# 問題文の例に近いダミーデータを作成 (問題71の train_dataset_processed の形式を模倣)\n",
    "# train_dataset_processed の最初の数件を使うのがより実践的\n",
    "if len(train_dataset_processed) >= 4:\n",
    "    print(\"\\n訓練データの最初の4事例を使ってテストします。\")\n",
    "    sample_batch_from_dataset = [train_torch_dataset_item for train_torch_dataset_item in SentimentDataset(train_dataset_processed[:4])]\n",
    "    # SentimentDatasetの__getitem__は(input_ids, label)のタプルを返すので、それがリストになったものがsample_batch_from_dataset\n",
    "else:\n",
    "    print(\"\\n訓練データが4件未満のため、手動でダミーデータを作成します。\")\n",
    "    # 手動ダミーデータ (input_ids は様々な長さのテンソル、label もテンソル)\n",
    "    sample_batch_from_dataset = [\n",
    "        (torch.tensor([5785, 66, 113845, 18, 12, 15095, 1594]), torch.tensor([0.])), # len 7\n",
    "        (torch.tensor([3475, 87, 15888, 90, 27695, 42637]), torch.tensor([0.])),    # len 6\n",
    "        (torch.tensor([4, 5053, 45, 3305, 31647, 348, 904, 2815, 47, 1276, 1964]), torch.tensor([1.])), # len 11\n",
    "        (torch.tensor([987, 14528, 4941, 873, 12, 208, 898]), torch.tensor([0.]))  # len 7\n",
    "    ]\n",
    "\n",
    "# collate関数をテスト\n",
    "# PAD_ID は問題70で定義されているはず (通常は0)\n",
    "collated_batch = collate_fn_custom_pad_sort(sample_batch_from_dataset, padding_value=PAD_ID)\n",
    "\n",
    "print(\"\\ncollate関数を通した結果:\")\n",
    "print(\"input_ids (パディング・ソート済み):\")\n",
    "print(collated_batch['input_ids'])\n",
    "print(\"input_ids の形状:\", collated_batch['input_ids'].shape)\n",
    "print(\"\\nlabel (ソート済み):\")\n",
    "print(collated_batch['label'])\n",
    "print(\"label の形状:\", collated_batch['label'].shape)\n",
    "\n",
    "# 問題文の期待する出力形式と一致しているか確認\n",
    "# 期待される input_ids (長さ11が最長、降順ソート):\n",
    "# tensor([[    4,  5053,    45,  3305, 31647,   348,   904,  2815,    47,  1276,  1964],\n",
    "#         [ 5785,    66,113845,    18,    12, 15095,  1594,     0,     0,     0,     0],\n",
    "#         [  987, 14528,  4941,   873,    12,   208,   898,     0,     0,     0,     0],\n",
    "#         [ 3475,    87, 15888,    90, 27695, 42637,     0,     0,     0,     0,     0]])\n",
    "# 期待される label (対応してソート):\n",
    "# tensor([[1.],\n",
    "#         [0.],\n",
    "#         [0.],\n",
    "#         [0.]])\n",
    "\n",
    "# この collate_fn_custom_pad_sort 関数を、次の問題76で DataLoader の collate_fn として指定します。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9NzvuZ-5ebDU",
   "metadata": {
    "id": "9NzvuZ-5ebDU"
   },
   "source": [
    "## 76. ミニバッチ学習\n",
    "\n",
    "問題75のパディングの処理を活用して、ミニバッチでモデルを学習せよ。また、学習したモデルの開発セットにおける正解率を求めよ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e20d5e5b-2102-442f-bb3a-990810bc871d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "モデルの重みを再初期化します...\n",
      "\n",
      "ミニバッチ学習 (バッチサイズ=32) を開始します...\n",
      "  Epoch [1/5], Batch [416/2083], Avg Loss: 0.6041\n",
      "  Epoch [1/5], Batch [832/2083], Avg Loss: 0.5591\n",
      "  Epoch [1/5], Batch [1248/2083], Avg Loss: 0.5298\n",
      "  Epoch [1/5], Batch [1664/2083], Avg Loss: 0.5091\n",
      "  Epoch [1/5], Batch [2080/2083], Avg Loss: 0.4941\n",
      "  Epoch [1/5], Batch [2083/2083], Avg Loss: 0.4940\n",
      "Epoch [1/5] 完了, 平均訓練損失: 0.4940\n",
      "  Epoch [1/5], 検証データ: 平均損失=0.5175, 正解率=77.29%\n",
      "  Epoch [2/5], Batch [416/2083], Avg Loss: 0.4190\n",
      "  Epoch [2/5], Batch [832/2083], Avg Loss: 0.4163\n",
      "  Epoch [2/5], Batch [1248/2083], Avg Loss: 0.4133\n",
      "  Epoch [2/5], Batch [1664/2083], Avg Loss: 0.4100\n",
      "  Epoch [2/5], Batch [2080/2083], Avg Loss: 0.4063\n",
      "  Epoch [2/5], Batch [2083/2083], Avg Loss: 0.4062\n",
      "Epoch [2/5] 完了, 平均訓練損失: 0.4062\n",
      "  Epoch [2/5], 検証データ: 平均損失=0.4863, 正解率=77.98%\n",
      "  Epoch [3/5], Batch [416/2083], Avg Loss: 0.3982\n",
      "  Epoch [3/5], Batch [832/2083], Avg Loss: 0.3944\n",
      "  Epoch [3/5], Batch [1248/2083], Avg Loss: 0.3923\n",
      "  Epoch [3/5], Batch [1664/2083], Avg Loss: 0.3913\n",
      "  Epoch [3/5], Batch [2080/2083], Avg Loss: 0.3879\n",
      "  Epoch [3/5], Batch [2083/2083], Avg Loss: 0.3879\n",
      "Epoch [3/5] 完了, 平均訓練損失: 0.3879\n",
      "  Epoch [3/5], 検証データ: 平均損失=0.4681, 正解率=79.59%\n",
      "  Epoch [4/5], Batch [416/2083], Avg Loss: 0.3859\n",
      "  Epoch [4/5], Batch [832/2083], Avg Loss: 0.3840\n",
      "  Epoch [4/5], Batch [1248/2083], Avg Loss: 0.3840\n",
      "  Epoch [4/5], Batch [1664/2083], Avg Loss: 0.3810\n",
      "  Epoch [4/5], Batch [2080/2083], Avg Loss: 0.3802\n",
      "  Epoch [4/5], Batch [2083/2083], Avg Loss: 0.3802\n",
      "Epoch [4/5] 完了, 平均訓練損失: 0.3802\n",
      "  Epoch [4/5], 検証データ: 平均損失=0.4610, 正解率=79.82%\n",
      "  Epoch [5/5], Batch [416/2083], Avg Loss: 0.3808\n",
      "  Epoch [5/5], Batch [832/2083], Avg Loss: 0.3747\n",
      "  Epoch [5/5], Batch [1248/2083], Avg Loss: 0.3764\n",
      "  Epoch [5/5], Batch [1664/2083], Avg Loss: 0.3758\n",
      "  Epoch [5/5], Batch [2080/2083], Avg Loss: 0.3763\n",
      "  Epoch [5/5], Batch [2083/2083], Avg Loss: 0.3762\n",
      "Epoch [5/5] 完了, 平均訓練損失: 0.3762\n",
      "  Epoch [5/5], 検証データ: 平均損失=0.4614, 正解率=79.59%\n",
      "\n",
      "ミニバッチ学習が完了しました。\n",
      "\n",
      "--- 最終評価 (開発セット) ---\n",
      "開発セットにおける最終正解率: 79.59%\n",
      "  正解した事例数: 694\n",
      "  総事例数: 872\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence # collate_fn_custom_pad_sort で使用\n",
    "\n",
    "# --- 前提となる変数・クラス・関数の定義 ---\n",
    "# PAD_ID (問題70)\n",
    "# embedding_matrix, vocab_size, embedding_dim (問題70)\n",
    "# SimpleBOWClassifier クラス (問題72)\n",
    "# model_bow (問題72でインスタンス化、問題73で一度学習させたが、再学習するので再インスタンス化も可)\n",
    "# train_dataset_processed, dev_dataset_processed (問題71)\n",
    "# SentimentDataset クラス (問題73または75で定義)\n",
    "# collate_fn_custom_pad_sort 関数 (問題75で定義)\n",
    "\n",
    "# --- 変数・クラスが現在のセッションに存在するか確認 ---\n",
    "required_vars = ['PAD_ID', 'embedding_matrix', 'vocab_size', 'embedding_dim', \n",
    "                 'model_bow', 'train_dataset_processed', 'dev_dataset_processed']\n",
    "for var_name in required_vars:\n",
    "    if var_name not in locals():\n",
    "        print(f\"エラー: 前提となる変数 '{var_name}' が定義されていません。\")\n",
    "        print(\"問題70, 71, 72, (73)を先に実行してください。\")\n",
    "        raise NameError(f\"Variable '{var_name}' is not defined.\")\n",
    "\n",
    "required_classes_funcs = ['SentimentDataset', 'collate_fn_custom_pad_sort', 'SimpleBOWClassifier']\n",
    "for item_name in required_classes_funcs:\n",
    "    if item_name not in locals():\n",
    "        print(f\"エラー: 前提となるクラス/関数 '{item_name}' が定義されていません。\")\n",
    "        print(\"問題72, 73, 75のコードを確認・実行してください。\")\n",
    "        raise NameError(f\"Class/Function '{item_name}' is not defined.\")\n",
    "\n",
    "# --- ここから問題76の処理 ---\n",
    "\n",
    "# モデルを再初期化 (問題73で学習した重みをリセットして、ミニバッチ学習の効果を新たに見るため)\n",
    "# もし問題73の学習結果を引き継ぎたい場合は、この行はコメントアウト\n",
    "print(\"モデルの重みを再初期化します...\")\n",
    "embedding_tensor = torch.tensor(embedding_matrix, dtype=torch.float) # 問題72と同様\n",
    "model_bow = SimpleBOWClassifier(vocab_size, embedding_dim, embedding_tensor, PAD_ID)\n",
    "\n",
    "\n",
    "# データローダーの準備 (ミニバッチ化とカスタムcollate_fnを使用)\n",
    "batch_size = 32 # ミニバッチのサイズ (64, 128なども試せる)\n",
    "\n",
    "train_torch_dataset_mb = SentimentDataset(train_dataset_processed)\n",
    "train_dataloader_mb = DataLoader(train_torch_dataset_mb, \n",
    "                                 batch_size=batch_size, \n",
    "                                 shuffle=True, \n",
    "                                 collate_fn=lambda b: collate_fn_custom_pad_sort(b, padding_value=PAD_ID))\n",
    "\n",
    "if dev_dataset_processed:\n",
    "    dev_torch_dataset_mb = SentimentDataset(dev_dataset_processed)\n",
    "    dev_dataloader_mb = DataLoader(dev_torch_dataset_mb, \n",
    "                                   batch_size=batch_size, \n",
    "                                   shuffle=False, # 検証時はシャッフル不要\n",
    "                                   collate_fn=lambda b: collate_fn_custom_pad_sort(b, padding_value=PAD_ID))\n",
    "else:\n",
    "    dev_dataloader_mb = None\n",
    "\n",
    "\n",
    "# 学習パラメータ\n",
    "learning_rate_mb = 1e-3 # 問題73と同じか、調整しても良い\n",
    "num_epochs_mb = 5     # 学習エポック数 (適宜調整)\n",
    "\n",
    "# 損失関数と最適化アルゴリズム\n",
    "criterion_mb = nn.BCEWithLogitsLoss()\n",
    "optimizer_mb = optim.Adam(model_bow.parameters(), lr=learning_rate_mb)\n",
    "\n",
    "print(f\"\\nミニバッチ学習 (バッチサイズ={batch_size}) を開始します...\")\n",
    "# --- 学習ループ ---\n",
    "for epoch in range(num_epochs_mb):\n",
    "    model_bow.train() # モデルを学習モードに\n",
    "    running_loss = 0.0\n",
    "    num_processed_samples = 0\n",
    "    \n",
    "    for i, batch_data in enumerate(train_dataloader_mb):\n",
    "        input_ids_batch = batch_data['input_ids'] # collate_fnの返り値に合わせてアクセス\n",
    "        labels_batch = batch_data['label']\n",
    "        \n",
    "        optimizer_mb.zero_grad()\n",
    "        outputs = model_bow(input_ids_batch)\n",
    "        loss = criterion_mb(outputs, labels_batch)\n",
    "        loss.backward()\n",
    "        optimizer_mb.step()\n",
    "        \n",
    "        running_loss += loss.item() * input_ids_batch.size(0)\n",
    "        num_processed_samples += input_ids_batch.size(0)\n",
    "        \n",
    "        if (i + 1) % (len(train_dataloader_mb) // 5) == 0 or (i + 1) == len(train_dataloader_mb): # 約20%ごとと最後に表示\n",
    "            avg_loss_so_far = running_loss / num_processed_samples\n",
    "            print(f\"  Epoch [{epoch+1}/{num_epochs_mb}], Batch [{i+1}/{len(train_dataloader_mb)}], Avg Loss: {avg_loss_so_far:.4f}\")\n",
    "\n",
    "    epoch_loss = running_loss / len(train_torch_dataset_mb)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs_mb}] 完了, 平均訓練損失: {epoch_loss:.4f}\")\n",
    "\n",
    "    # 各エポックの終わりに検証データで性能評価\n",
    "    if dev_dataloader_mb:\n",
    "        model_bow.eval() # モデルを評価モードに\n",
    "        correct_dev = 0\n",
    "        total_dev = 0\n",
    "        dev_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch_data_dev in dev_dataloader_mb:\n",
    "                input_ids_dev = batch_data_dev['input_ids']\n",
    "                labels_dev = batch_data_dev['label']\n",
    "                \n",
    "                outputs_dev = model_bow(input_ids_dev)\n",
    "                loss_dev_batch = criterion_mb(outputs_dev, labels_dev)\n",
    "                dev_loss += loss_dev_batch.item() * input_ids_dev.size(0)\n",
    "\n",
    "                predicted_probs = torch.sigmoid(outputs_dev)\n",
    "                predicted_labels = (predicted_probs > 0.5).float()\n",
    "                total_dev += labels_dev.size(0)\n",
    "                correct_dev += (predicted_labels == labels_dev).sum().item()\n",
    "        \n",
    "        avg_dev_loss = dev_loss / total_dev if total_dev > 0 else 0\n",
    "        dev_accuracy = 100 * correct_dev / total_dev if total_dev > 0 else 0\n",
    "        print(f\"  Epoch [{epoch+1}/{num_epochs_mb}], 検証データ: 平均損失={avg_dev_loss:.4f}, 正解率={dev_accuracy:.2f}%\")\n",
    "\n",
    "print(\"\\nミニバッチ学習が完了しました。\")\n",
    "\n",
    "# --- 学習したモデルの開発セットにおける最終的な正解率を求める ---\n",
    "if dev_dataloader_mb:\n",
    "    print(\"\\n--- 最終評価 (開発セット) ---\")\n",
    "    model_bow.eval()\n",
    "    final_correct_dev = 0\n",
    "    final_total_dev = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_data_dev in dev_dataloader_mb:\n",
    "            input_ids_dev = batch_data_dev['input_ids']\n",
    "            labels_dev = batch_data_dev['label']\n",
    "            outputs_dev = model_bow(input_ids_dev)\n",
    "            predicted_probs = torch.sigmoid(outputs_dev)\n",
    "            predicted_labels = (predicted_probs > 0.5).float()\n",
    "            final_total_dev += labels_dev.size(0)\n",
    "            final_correct_dev += (predicted_labels == labels_dev).sum().item()\n",
    "            \n",
    "    if final_total_dev > 0:\n",
    "        final_dev_accuracy = 100 * final_correct_dev / final_total_dev\n",
    "        print(f\"開発セットにおける最終正解率: {final_dev_accuracy:.2f}%\")\n",
    "        print(f\"  正解した事例数: {final_correct_dev}\")\n",
    "        print(f\"  総事例数: {final_total_dev}\")\n",
    "    else:\n",
    "        print(\"開発データでの評価ができませんでした。\")\n",
    "else:\n",
    "    print(\"開発データローダーが準備されていないため、最終評価をスキップします。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RUbjivUTejxn",
   "metadata": {
    "id": "RUbjivUTejxn"
   },
   "source": [
    "## 77. GPU上での学習\n",
    "\n",
    "問題76のモデル学習をGPU上で実行せよ。また、学習したモデルの開発セットにおける正解率を求めよ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0c499161-d86b-4298-807f-6dc568b73539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUが利用できません。CPUを使用します。\n",
      "\n",
      "モデルを再初期化し、デバイスに転送します...\n",
      "\n",
      "GPU上でのミニバッチ学習 (バッチサイズ=32) を開始します...\n",
      "  Epoch [1/5], Batch [416/2083], Avg Loss: 0.6096\n",
      "  Epoch [1/5], Batch [832/2083], Avg Loss: 0.5656\n",
      "  Epoch [1/5], Batch [1248/2083], Avg Loss: 0.5345\n",
      "  Epoch [1/5], Batch [1664/2083], Avg Loss: 0.5128\n",
      "  Epoch [1/5], Batch [2080/2083], Avg Loss: 0.4967\n",
      "  Epoch [1/5], Batch [2083/2083], Avg Loss: 0.4966\n",
      "Epoch [1/5] 完了, 平均訓練損失: 0.4966\n",
      "  Epoch [1/5], 検証データ: 平均損失=0.5178, 正解率=77.64%\n",
      "  Epoch [2/5], Batch [416/2083], Avg Loss: 0.4208\n",
      "  Epoch [2/5], Batch [832/2083], Avg Loss: 0.4188\n",
      "  Epoch [2/5], Batch [1248/2083], Avg Loss: 0.4127\n",
      "  Epoch [2/5], Batch [1664/2083], Avg Loss: 0.4104\n",
      "  Epoch [2/5], Batch [2080/2083], Avg Loss: 0.4069\n",
      "  Epoch [2/5], Batch [2083/2083], Avg Loss: 0.4069\n",
      "Epoch [2/5] 完了, 平均訓練損失: 0.4069\n",
      "  Epoch [2/5], 検証データ: 平均損失=0.4841, 正解率=78.21%\n",
      "  Epoch [3/5], Batch [416/2083], Avg Loss: 0.3885\n",
      "  Epoch [3/5], Batch [832/2083], Avg Loss: 0.3894\n",
      "  Epoch [3/5], Batch [1248/2083], Avg Loss: 0.3892\n",
      "  Epoch [3/5], Batch [1664/2083], Avg Loss: 0.3900\n",
      "  Epoch [3/5], Batch [2080/2083], Avg Loss: 0.3885\n",
      "  Epoch [3/5], Batch [2083/2083], Avg Loss: 0.3884\n",
      "Epoch [3/5] 完了, 平均訓練損失: 0.3884\n",
      "  Epoch [3/5], 検証データ: 平均損失=0.4699, 正解率=79.13%\n",
      "  Epoch [4/5], Batch [416/2083], Avg Loss: 0.3823\n",
      "  Epoch [4/5], Batch [832/2083], Avg Loss: 0.3793\n",
      "  Epoch [4/5], Batch [1248/2083], Avg Loss: 0.3795\n",
      "  Epoch [4/5], Batch [1664/2083], Avg Loss: 0.3807\n",
      "  Epoch [4/5], Batch [2080/2083], Avg Loss: 0.3806\n",
      "  Epoch [4/5], Batch [2083/2083], Avg Loss: 0.3806\n",
      "Epoch [4/5] 完了, 平均訓練損失: 0.3806\n",
      "  Epoch [4/5], 検証データ: 平均損失=0.4653, 正解率=79.47%\n",
      "  Epoch [5/5], Batch [416/2083], Avg Loss: 0.3718\n",
      "  Epoch [5/5], Batch [832/2083], Avg Loss: 0.3767\n",
      "  Epoch [5/5], Batch [1248/2083], Avg Loss: 0.3779\n",
      "  Epoch [5/5], Batch [1664/2083], Avg Loss: 0.3762\n",
      "  Epoch [5/5], Batch [2080/2083], Avg Loss: 0.3764\n",
      "  Epoch [5/5], Batch [2083/2083], Avg Loss: 0.3764\n",
      "Epoch [5/5] 完了, 平均訓練損失: 0.3764\n",
      "  Epoch [5/5], 検証データ: 平均損失=0.4617, 正解率=79.36%\n",
      "\n",
      "GPU上でのミニバッチ学習が完了しました。所要時間: 6.55 秒\n",
      "\n",
      "--- 最終評価 (開発セット) ---\n",
      "開発セットにおける最終正解率 (GPU学習): 79.36%\n",
      "  正解した事例数: 692\n",
      "  総事例数: 872\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence # collate_fn_custom_pad_sort で使用\n",
    "import time # 学習時間の計測用\n",
    "\n",
    "# --- 前提となる変数・クラス・関数の定義 (問題76と同様) ---\n",
    "# PAD_ID, embedding_matrix, vocab_size, embedding_dim\n",
    "# SimpleBOWClassifier クラス\n",
    "# train_dataset_processed, dev_dataset_processed\n",
    "# SentimentDataset クラス\n",
    "# collate_fn_custom_pad_sort 関数\n",
    "\n",
    "# --- 変数・クラスが現在のセッションに存在するか確認 ---\n",
    "required_vars_p77 = ['PAD_ID', 'embedding_matrix', 'vocab_size', 'embedding_dim', \n",
    "                     'train_dataset_processed', 'dev_dataset_processed']\n",
    "for var_name in required_vars_p77:\n",
    "    if var_name not in locals():\n",
    "        print(f\"エラー: 前提となる変数 '{var_name}' が定義されていません。\")\n",
    "        print(\"問題70, 71を先に実行してください。\")\n",
    "        raise NameError(f\"Variable '{var_name}' is not defined.\")\n",
    "\n",
    "required_classes_funcs_p77 = ['SentimentDataset', 'collate_fn_custom_pad_sort', 'SimpleBOWClassifier']\n",
    "for item_name in required_classes_funcs_p77:\n",
    "    if item_name not in locals():\n",
    "        print(f\"エラー: 前提となるクラス/関数 '{item_name}' が定義されていません。\")\n",
    "        print(\"問題72, 73, 75のコードを確認・実行してください。\")\n",
    "        raise NameError(f\"Class/Function '{item_name}' is not defined.\")\n",
    "\n",
    "# --- ここから問題77の処理 ---\n",
    "\n",
    "# 1. GPUの利用可能性確認とデバイス設定\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"GPU ({torch.cuda.get_device_name(0)}) を使用します。\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPUが利用できません。CPUを使用します。\")\n",
    "\n",
    "# モデルを再初期化し、指定デバイスへ転送\n",
    "print(\"\\nモデルを再初期化し、デバイスに転送します...\")\n",
    "embedding_tensor_p77 = torch.tensor(embedding_matrix, dtype=torch.float)\n",
    "model_bow_gpu = SimpleBOWClassifier(vocab_size, embedding_dim, embedding_tensor_p77, PAD_ID)\n",
    "model_bow_gpu.to(device) # ★★★ モデルをデバイスへ ★★★\n",
    "\n",
    "# データローダーの準備 (問題76と同様)\n",
    "batch_size_gpu = 32 \n",
    "\n",
    "train_torch_dataset_gpu = SentimentDataset(train_dataset_processed)\n",
    "train_dataloader_gpu = DataLoader(train_torch_dataset_gpu, \n",
    "                                  batch_size=batch_size_gpu, \n",
    "                                  shuffle=True, \n",
    "                                  collate_fn=lambda b: collate_fn_custom_pad_sort(b, padding_value=PAD_ID))\n",
    "\n",
    "if dev_dataset_processed:\n",
    "    dev_torch_dataset_gpu = SentimentDataset(dev_dataset_processed)\n",
    "    dev_dataloader_gpu = DataLoader(dev_torch_dataset_gpu, \n",
    "                                    batch_size=batch_size_gpu, \n",
    "                                    shuffle=False,\n",
    "                                    collate_fn=lambda b: collate_fn_custom_pad_sort(b, padding_value=PAD_ID))\n",
    "else:\n",
    "    dev_dataloader_gpu = None\n",
    "\n",
    "# 学習パラメータ\n",
    "learning_rate_gpu = 1e-3\n",
    "num_epochs_gpu = 5 # 問題76と同じエポック数で比較\n",
    "\n",
    "# 損失関数と最適化アルゴリズム\n",
    "criterion_gpu = nn.BCEWithLogitsLoss()\n",
    "optimizer_gpu = optim.Adam(model_bow_gpu.parameters(), lr=learning_rate_gpu)\n",
    "\n",
    "print(f\"\\nGPU上でのミニバッチ学習 (バッチサイズ={batch_size_gpu}) を開始します...\")\n",
    "start_time = time.time()\n",
    "# --- 学習ループ ---\n",
    "for epoch in range(num_epochs_gpu):\n",
    "    model_bow_gpu.train()\n",
    "    running_loss = 0.0\n",
    "    num_processed_samples = 0\n",
    "    \n",
    "    for i, batch_data in enumerate(train_dataloader_gpu):\n",
    "        # ★★★ データをデバイスへ転送 ★★★\n",
    "        input_ids_batch = batch_data['input_ids'].to(device)\n",
    "        labels_batch = batch_data['label'].to(device)\n",
    "        \n",
    "        optimizer_gpu.zero_grad()\n",
    "        outputs = model_bow_gpu(input_ids_batch)\n",
    "        loss = criterion_gpu(outputs, labels_batch)\n",
    "        loss.backward()\n",
    "        optimizer_gpu.step()\n",
    "        \n",
    "        running_loss += loss.item() * input_ids_batch.size(0)\n",
    "        num_processed_samples += input_ids_batch.size(0)\n",
    "        \n",
    "        if (i + 1) % (len(train_dataloader_gpu) // 5) == 0 or (i + 1) == len(train_dataloader_gpu):\n",
    "            avg_loss_so_far = running_loss / num_processed_samples\n",
    "            print(f\"  Epoch [{epoch+1}/{num_epochs_gpu}], Batch [{i+1}/{len(train_dataloader_gpu)}], Avg Loss: {avg_loss_so_far:.4f}\")\n",
    "\n",
    "    epoch_loss = running_loss / len(train_torch_dataset_gpu)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs_gpu}] 完了, 平均訓練損失: {epoch_loss:.4f}\")\n",
    "\n",
    "    if dev_dataloader_gpu:\n",
    "        model_bow_gpu.eval()\n",
    "        correct_dev = 0\n",
    "        total_dev = 0\n",
    "        dev_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch_data_dev in dev_dataloader_gpu:\n",
    "                # ★★★ データをデバイスへ転送 ★★★\n",
    "                input_ids_dev = batch_data_dev['input_ids'].to(device)\n",
    "                labels_dev = batch_data_dev['label'].to(device)\n",
    "                \n",
    "                outputs_dev = model_bow_gpu(input_ids_dev)\n",
    "                loss_dev_batch = criterion_gpu(outputs_dev, labels_dev) # 損失計算もデバイス上\n",
    "                dev_loss += loss_dev_batch.item() * input_ids_dev.size(0)\n",
    "\n",
    "                predicted_probs = torch.sigmoid(outputs_dev)\n",
    "                predicted_labels = (predicted_probs > 0.5).float()\n",
    "                total_dev += labels_dev.size(0)\n",
    "                correct_dev += (predicted_labels == labels_dev).sum().item()\n",
    "        \n",
    "        avg_dev_loss = dev_loss / total_dev if total_dev > 0 else 0\n",
    "        dev_accuracy = 100 * correct_dev / total_dev if total_dev > 0 else 0\n",
    "        print(f\"  Epoch [{epoch+1}/{num_epochs_gpu}], 検証データ: 平均損失={avg_dev_loss:.4f}, 正解率={dev_accuracy:.2f}%\")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"\\nGPU上でのミニバッチ学習が完了しました。所要時間: {end_time - start_time:.2f} 秒\")\n",
    "\n",
    "# --- 学習したモデルの開発セットにおける最終的な正解率を求める ---\n",
    "if dev_dataloader_gpu:\n",
    "    print(\"\\n--- 最終評価 (開発セット) ---\")\n",
    "    model_bow_gpu.eval()\n",
    "    final_correct_dev = 0\n",
    "    final_total_dev = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_data_dev in dev_dataloader_gpu:\n",
    "            # ★★★ データをデバイスへ転送 ★★★\n",
    "            input_ids_dev = batch_data_dev['input_ids'].to(device)\n",
    "            labels_dev = batch_data_dev['label'].to(device)\n",
    "            \n",
    "            outputs_dev = model_bow_gpu(input_ids_dev)\n",
    "            predicted_probs = torch.sigmoid(outputs_dev)\n",
    "            predicted_labels = (predicted_probs > 0.5).float()\n",
    "            final_total_dev += labels_dev.size(0)\n",
    "            final_correct_dev += (predicted_labels == labels_dev).sum().item()\n",
    "            \n",
    "    if final_total_dev > 0:\n",
    "        final_dev_accuracy_gpu = 100 * final_correct_dev / final_total_dev\n",
    "        print(f\"開発セットにおける最終正解率 (GPU学習): {final_dev_accuracy_gpu:.2f}%\")\n",
    "        print(f\"  正解した事例数: {final_correct_dev}\")\n",
    "        print(f\"  総事例数: {final_total_dev}\")\n",
    "    else:\n",
    "        print(\"開発データでの評価ができませんでした。\")\n",
    "else:\n",
    "    print(\"開発データローダーが準備されていないため、最終評価をスキップします。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZUY1PsD-eplq",
   "metadata": {
    "id": "ZUY1PsD-eplq"
   },
   "source": [
    "## 78. 単語埋め込みのファインチューニング\n",
    "\n",
    "問題77の学習において、単語埋め込みのパラメータも同時に更新するファインチューニングを導入せよ。また、学習したモデルの開発セットにおける正解率を求めよ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ea9fd0-f0a3-4e1f-a651-c5d0f70ba205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUが利用できません。CPUを使用します。\n",
      "\n",
      "モデルを初期化（埋め込みファインチューニング有効）し、デバイスに転送します...\n",
      "\n",
      "単語埋め込みファインチューニングありでのミニバッチ学習 (バッチサイズ=32) を開始します...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import time\n",
    "import numpy as np # embedding_matrix のため\n",
    "\n",
    "# --- 前提となる変数・クラス・関数の定義 (問題77と同様) ---\n",
    "# PAD_ID, embedding_matrix, vocab_size, embedding_dim\n",
    "# train_dataset_processed, dev_dataset_processed\n",
    "# SentimentDataset クラス\n",
    "# collate_fn_custom_pad_sort 関数\n",
    "\n",
    "# --- 変数・クラスが現在のセッションに存在するか確認 ---\n",
    "required_vars_p78 = ['PAD_ID', 'embedding_matrix', 'vocab_size', 'embedding_dim', \n",
    "                     'train_dataset_processed', 'dev_dataset_processed']\n",
    "for var_name in required_vars_p78:\n",
    "    if var_name not in locals():\n",
    "        print(f\"エラー: 前提となる変数 '{var_name}' が定義されていません。\")\n",
    "        raise NameError(f\"Variable '{var_name}' is not defined.\")\n",
    "\n",
    "required_classes_funcs_p78 = ['SentimentDataset', 'collate_fn_custom_pad_sort'] # SimpleBOWClassifierはここで再定義\n",
    "for item_name in required_classes_funcs_p78:\n",
    "    if item_name not in locals():\n",
    "        print(f\"エラー: 前提となるクラス/関数 '{item_name}' が定義されていません。\")\n",
    "        raise NameError(f\"Class/Function '{item_name}' is not defined.\")\n",
    "\n",
    "# --- ここから問題78の処理 ---\n",
    "\n",
    "# 1. モデル定義の変更 (埋め込み層をファインチューニング可能に)\n",
    "class SimpleBOWClassifierFinetune(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, pretrained_embeddings, padding_idx):\n",
    "        super(SimpleBOWClassifierFinetune, self).__init__()\n",
    "        \n",
    "        # 単語埋め込み層\n",
    "        # 事前学習済み重みをロードし、学習中に更新する (requires_grad=True がデフォルト)\n",
    "        # または nn.Embedding.from_pretrained を使う\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=padding_idx)\n",
    "        # from_pretrained を使う場合:\n",
    "        # self.embedding = nn.Embedding.from_pretrained(\n",
    "        #     embeddings=pretrained_embeddings, \n",
    "        #     freeze=False,  # ★★★ Falseにすることでファインチューニング可能 ★★★\n",
    "        #     padding_idx=padding_idx\n",
    "        # )\n",
    "        \n",
    "        # もし nn.Parameter で直接設定する場合:\n",
    "        self.embedding.weight = nn.Parameter(pretrained_embeddings, requires_grad=True) # ★★★ requires_grad=True ★★★\n",
    "        \n",
    "        self.fc = nn.Linear(embedding_dim, 1)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        embedded = self.embedding(input_ids)\n",
    "        mask = (input_ids != self.embedding.padding_idx).unsqueeze(-1).float()\n",
    "        lengths = mask.sum(dim=1).clamp(min=1)\n",
    "        sum_embedded = (embedded * mask).sum(dim=1)\n",
    "        mean_embedded = sum_embedded / lengths\n",
    "        logits = self.fc(mean_embedded)\n",
    "        return logits\n",
    "\n",
    "# デバイス設定 (問題77と同様)\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"GPU ({torch.cuda.get_device_name(0)}) を使用します。\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPUが利用できません。CPUを使用します。\")\n",
    "\n",
    "# モデルを初期化し、指定デバイスへ転送\n",
    "print(\"\\nモデルを初期化（埋め込みファインチューニング有効）し、デバイスに転送します...\")\n",
    "embedding_tensor_p78 = torch.tensor(embedding_matrix, dtype=torch.float)\n",
    "model_bow_finetune = SimpleBOWClassifierFinetune(vocab_size, embedding_dim, embedding_tensor_p78, PAD_ID)\n",
    "model_bow_finetune.to(device)\n",
    "\n",
    "# データローダーの準備 (問題77と同様)\n",
    "batch_size_finetune = 32\n",
    "train_torch_dataset_ft = SentimentDataset(train_dataset_processed)\n",
    "train_dataloader_ft = DataLoader(train_torch_dataset_ft, \n",
    "                                 batch_size=batch_size_finetune, \n",
    "                                 shuffle=True, \n",
    "                                 collate_fn=lambda b: collate_fn_custom_pad_sort(b, padding_value=PAD_ID))\n",
    "\n",
    "if dev_dataset_processed:\n",
    "    dev_torch_dataset_ft = SentimentDataset(dev_dataset_processed)\n",
    "    dev_dataloader_ft = DataLoader(dev_torch_dataset_ft, \n",
    "                                   batch_size=batch_size_finetune, \n",
    "                                   shuffle=False,\n",
    "                                   collate_fn=lambda b: collate_fn_custom_pad_sort(b, padding_value=PAD_ID))\n",
    "else:\n",
    "    dev_dataloader_ft = None\n",
    "\n",
    "# 学習パラメータ\n",
    "learning_rate_ft = 1e-4 # ファインチューニング時は学習率を少し小さめに設定することが多い\n",
    "num_epochs_ft = 5    # エポック数\n",
    "\n",
    "# 損失関数と最適化アルゴリズム\n",
    "criterion_ft = nn.BCEWithLogitsLoss()\n",
    "# オプティマイザはモデルの全ての requires_grad=True のパラメータを対象とする\n",
    "optimizer_ft = optim.Adam(model_bow_finetune.parameters(), lr=learning_rate_ft)\n",
    "\n",
    "print(f\"\\n単語埋め込みファインチューニングありでのミニバッチ学習 (バッチサイズ={batch_size_finetune}) を開始します...\")\n",
    "start_time_ft = time.time()\n",
    "# --- 学習ループ (問題77とほぼ同じ) ---\n",
    "for epoch in range(num_epochs_ft):\n",
    "    model_bow_finetune.train()\n",
    "    running_loss = 0.0\n",
    "    num_processed_samples = 0\n",
    "    \n",
    "    for i, batch_data in enumerate(train_dataloader_ft):\n",
    "        input_ids_batch = batch_data['input_ids'].to(device)\n",
    "        labels_batch = batch_data['label'].to(device)\n",
    "        \n",
    "        optimizer_ft.zero_grad()\n",
    "        outputs = model_bow_finetune(input_ids_batch)\n",
    "        loss = criterion_ft(outputs, labels_batch)\n",
    "        loss.backward()\n",
    "        optimizer_ft.step()\n",
    "        \n",
    "        running_loss += loss.item() * input_ids_batch.size(0)\n",
    "        num_processed_samples += input_ids_batch.size(0)\n",
    "        \n",
    "        if (i + 1) % (len(train_dataloader_ft) // 5) == 0 or (i + 1) == len(train_dataloader_ft):\n",
    "            avg_loss_so_far = running_loss / num_processed_samples\n",
    "            print(f\"  Epoch [{epoch+1}/{num_epochs_ft}], Batch [{i+1}/{len(train_dataloader_ft)}], Avg Loss: {avg_loss_so_far:.4f}\")\n",
    "\n",
    "    epoch_loss = running_loss / len(train_torch_dataset_ft)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs_ft}] 完了, 平均訓練損失: {epoch_loss:.4f}\")\n",
    "\n",
    "    if dev_dataloader_ft:\n",
    "        model_bow_finetune.eval()\n",
    "        correct_dev = 0\n",
    "        total_dev = 0\n",
    "        dev_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch_data_dev in dev_dataloader_ft:\n",
    "                input_ids_dev = batch_data_dev['input_ids'].to(device)\n",
    "                labels_dev = batch_data_dev['label'].to(device)\n",
    "                \n",
    "                outputs_dev = model_bow_finetune(input_ids_dev)\n",
    "                loss_dev_batch = criterion_ft(outputs_dev, labels_dev)\n",
    "                dev_loss += loss_dev_batch.item() * input_ids_dev.size(0)\n",
    "\n",
    "                predicted_probs = torch.sigmoid(outputs_dev)\n",
    "                predicted_labels = (predicted_probs > 0.5).float()\n",
    "                total_dev += labels_dev.size(0)\n",
    "                correct_dev += (predicted_labels == labels_dev).sum().item()\n",
    "        \n",
    "        avg_dev_loss = dev_loss / total_dev if total_dev > 0 else 0\n",
    "        dev_accuracy = 100 * correct_dev / total_dev if total_dev > 0 else 0\n",
    "        print(f\"  Epoch [{epoch+1}/{num_epochs_ft}], 検証データ: 平均損失={avg_dev_loss:.4f}, 正解率={dev_accuracy:.2f}%\")\n",
    "\n",
    "end_time_ft = time.time()\n",
    "print(f\"\\n単語埋め込みファインチューニングありでの学習が完了しました。所要時間: {end_time_ft - start_time_ft:.2f} 秒\")\n",
    "\n",
    "# --- 学習したモデルの開発セットにおける最終的な正解率を求める ---\n",
    "if dev_dataloader_ft:\n",
    "    print(\"\\n--- 最終評価 (開発セット、ファインチューニングあり) ---\")\n",
    "    model_bow_finetune.eval()\n",
    "    final_correct_dev_ft = 0\n",
    "    final_total_dev_ft = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_data_dev in dev_dataloader_ft:\n",
    "            input_ids_dev = batch_data_dev['input_ids'].to(device)\n",
    "            labels_dev = batch_data_dev['label'].to(device)\n",
    "            \n",
    "            outputs_dev = model_bow_finetune(input_ids_dev)\n",
    "            predicted_probs = torch.sigmoid(outputs_dev)\n",
    "            predicted_labels = (predicted_probs > 0.5).float()\n",
    "            final_total_dev_ft += labels_dev.size(0)\n",
    "            final_correct_dev_ft += (predicted_labels == labels_dev).sum().item()\n",
    "            \n",
    "    if final_total_dev_ft > 0:\n",
    "        final_dev_accuracy_ft = 100 * final_correct_dev_ft / final_total_dev_ft\n",
    "        print(f\"開発セットにおける最終正解率 (ファインチューニングあり): {final_dev_accuracy_ft:.2f}%\")\n",
    "        print(f\"  正解した事例数: {final_correct_dev_ft}\")\n",
    "        print(f\"  総事例数: {final_total_dev_ft}\")\n",
    "    else:\n",
    "        print(\"開発データでの評価ができませんでした。\")\n",
    "else:\n",
    "    print(\"開発データローダーが準備されていないため、最終評価をスキップします。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jVAdWIq0evKR",
   "metadata": {
    "id": "jVAdWIq0evKR"
   },
   "source": [
    "## 79. アーキテクチャの変更\n",
    "\n",
    "ニューラルネットワークのアーキテクチャを自由に変更し、モデルを学習せよ。また、学習したモデルの開発セットにおける正解率を求めよ。例えば、テキストの特徴ベクトル（単語埋め込みの平均ベクトル）に対して多層のニューラルネットワークを通したり、畳み込みニューラルネットワーク（CNN; Convolutional Neural Network）や再帰型ニューラルネットワーク（RNN; Recurrent Neural Network）などのモデルの学習に挑戦するとよい。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef270a2-c476-452a-9b8f-150c49a2a84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# --- 前提となる変数・クラス・関数の定義 (問題78と同様) ---\n",
    "# PAD_ID, embedding_matrix, vocab_size, embedding_dim\n",
    "# train_dataset_processed, dev_dataset_processed\n",
    "# SentimentDataset クラス\n",
    "# collate_fn_custom_pad_sort 関数\n",
    "\n",
    "# --- 変数・クラスが現在のセッションに存在するか確認 ---\n",
    "required_vars_p79 = ['PAD_ID', 'embedding_matrix', 'vocab_size', 'embedding_dim', \n",
    "                     'train_dataset_processed', 'dev_dataset_processed']\n",
    "for var_name in required_vars_p79:\n",
    "    if var_name not in locals():\n",
    "        print(f\"エラー: 前提となる変数 '{var_name}' が定義されていません。\")\n",
    "        raise NameError(f\"Variable '{var_name}' is not defined.\")\n",
    "\n",
    "required_classes_funcs_p79 = ['SentimentDataset', 'collate_fn_custom_pad_sort']\n",
    "for item_name in required_classes_funcs_p79:\n",
    "    if item_name not in locals():\n",
    "        print(f\"エラー: 前提となるクラス/関数 '{item_name}' が定義されていません。\")\n",
    "        raise NameError(f\"Class/Function '{item_name}' is not defined.\")\n",
    "\n",
    "# --- ここから問題79の処理 ---\n",
    "\n",
    "# 1. MLPモデルクラスの定義\n",
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, \n",
    "                 pretrained_embeddings, padding_idx, dropout_rate=0.5, fine_tune_embeddings=True):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        \n",
    "        # 単語埋め込み層\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=padding_idx)\n",
    "        if fine_tune_embeddings:\n",
    "            self.embedding.weight = nn.Parameter(pretrained_embeddings, requires_grad=True)\n",
    "        else:\n",
    "            self.embedding.weight = nn.Parameter(pretrained_embeddings, requires_grad=False)\n",
    "        \n",
    "        # MLP部分\n",
    "        self.fc1 = nn.Linear(embedding_dim, hidden_dim) # 入力:平均ベクトル, 出力:隠れ層次元\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout_rate) # ドロップアウト層\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim) # 隠れ層から出力次元へ\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        # input_ids: (バッチサイズ, シーケンス長)\n",
    "        \n",
    "        embedded = self.embedding(input_ids)\n",
    "        # embedded: (バッチサイズ, シーケンス長, embedding_dim)\n",
    "        \n",
    "        # パディングを考慮した平均プーリング\n",
    "        mask = (input_ids != self.embedding.padding_idx).unsqueeze(-1).float()\n",
    "        lengths = mask.sum(dim=1).clamp(min=1)\n",
    "        sum_embedded = (embedded * mask).sum(dim=1)\n",
    "        mean_embedded = sum_embedded / lengths\n",
    "        # mean_embedded: (バッチサイズ, embedding_dim)\n",
    "        \n",
    "        # MLP\n",
    "        out = self.fc1(mean_embedded) # 線形層1\n",
    "        out = self.relu(out)          # ReLU活性化\n",
    "        out = self.dropout(out)       # ドロップアウト\n",
    "        logits = self.fc2(out)        # 線形層2 (出力層)\n",
    "        # logits: (バッチサイズ, output_dim)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "# --- モデルのパラメータ設定 ---\n",
    "# embedding_dim は問題70から (例: 300)\n",
    "hidden_dim = 128  # MLPの隠れ層の次元数 (自由に設定)\n",
    "output_dim = 1    # ポジネガの2値分類なので出力は1 (ロジット)\n",
    "dropout_p = 0.5   # ドロップアウト率 (過学習抑制のため)\n",
    "fine_tune_emb = True # 単語埋め込みをファインチューニングするかどうか (True or False)\n",
    "\n",
    "# デバイス設定 (問題77, 78と同様)\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"GPU ({torch.cuda.get_device_name(0)}) を使用します。\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPUが利用できません。CPUを使用します。\")\n",
    "\n",
    "# モデルを初期化し、指定デバイスへ転送\n",
    "print(\"\\nMLPモデルを初期化し、デバイスに転送します...\")\n",
    "embedding_tensor_p79 = torch.tensor(embedding_matrix, dtype=torch.float)\n",
    "mlp_model = MLPClassifier(vocab_size, embedding_dim, hidden_dim, output_dim, \n",
    "                          embedding_tensor_p79, PAD_ID, dropout_rate=dropout_p,\n",
    "                          fine_tune_embeddings=fine_tune_emb)\n",
    "mlp_model.to(device)\n",
    "print(\"設計したMLPモデルの構造:\")\n",
    "print(mlp_model)\n",
    "\n",
    "\n",
    "# データローダーの準備 (問題76, 78と同様)\n",
    "batch_size_mlp = 32\n",
    "train_torch_dataset_mlp = SentimentDataset(train_dataset_processed)\n",
    "train_dataloader_mlp = DataLoader(train_torch_dataset_mlp, \n",
    "                                 batch_size=batch_size_mlp, \n",
    "                                 shuffle=True, \n",
    "                                 collate_fn=lambda b: collate_fn_custom_pad_sort(b, padding_value=PAD_ID))\n",
    "\n",
    "if dev_dataset_processed:\n",
    "    dev_torch_dataset_mlp = SentimentDataset(dev_dataset_processed)\n",
    "    dev_dataloader_mlp = DataLoader(dev_torch_dataset_mlp, \n",
    "                                   batch_size=batch_size_mlp, \n",
    "                                   shuffle=False,\n",
    "                                   collate_fn=lambda b: collate_fn_custom_pad_sort(b, padding_value=PAD_ID))\n",
    "else:\n",
    "    dev_dataloader_mlp = None\n",
    "\n",
    "# 学習パラメータ\n",
    "learning_rate_mlp = 1e-4 # ファインチューニング時は小さめが安定しやすい\n",
    "num_epochs_mlp = 10      # エポック数を増やして学習効果を見る (適宜調整)\n",
    "\n",
    "# 損失関数と最適化アルゴリズム\n",
    "criterion_mlp = nn.BCEWithLogitsLoss()\n",
    "optimizer_mlp = optim.Adam(mlp_model.parameters(), lr=learning_rate_mlp)\n",
    "\n",
    "print(f\"\\nMLPモデルの学習 (バッチサイズ={batch_size_mlp}, 埋め込みファインチューニング={fine_tune_emb}) を開始します...\")\n",
    "start_time_mlp = time.time()\n",
    "# --- 学習ループ (問題77, 78とほぼ同じ) ---\n",
    "for epoch in range(num_epochs_mlp):\n",
    "    mlp_model.train() # モデルを学習モードに\n",
    "    running_loss = 0.0\n",
    "    num_processed_samples = 0\n",
    "    \n",
    "    for i, batch_data in enumerate(train_dataloader_mlp):\n",
    "        input_ids_batch = batch_data['input_ids'].to(device)\n",
    "        labels_batch = batch_data['label'].to(device)\n",
    "        \n",
    "        optimizer_mlp.zero_grad()\n",
    "        outputs = mlp_model(input_ids_batch) # モデルのフォワードパス\n",
    "        loss = criterion_mlp(outputs, labels_batch)\n",
    "        loss.backward()\n",
    "        optimizer_mlp.step()\n",
    "        \n",
    "        running_loss += loss.item() * input_ids_batch.size(0)\n",
    "        num_processed_samples += input_ids_batch.size(0)\n",
    "        \n",
    "        if (i + 1) % (len(train_dataloader_mlp) // 5) == 0 or (i + 1) == len(train_dataloader_mlp):\n",
    "            avg_loss_so_far = running_loss / num_processed_samples\n",
    "            print(f\"  Epoch [{epoch+1}/{num_epochs_mlp}], Batch [{i+1}/{len(train_dataloader_mlp)}], Avg Loss: {avg_loss_so_far:.4f}\")\n",
    "\n",
    "    epoch_loss = running_loss / len(train_torch_dataset_mlp)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs_mlp}] 完了, 平均訓練損失: {epoch_loss:.4f}\")\n",
    "\n",
    "    if dev_dataloader_mlp:\n",
    "        mlp_model.eval() # モデルを評価モードに\n",
    "        correct_dev = 0\n",
    "        total_dev = 0\n",
    "        dev_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch_data_dev in dev_dataloader_mlp:\n",
    "                input_ids_dev = batch_data_dev['input_ids'].to(device)\n",
    "                labels_dev = batch_data_dev['label'].to(device)\n",
    "                \n",
    "                outputs_dev = mlp_model(input_ids_dev)\n",
    "                loss_dev_batch = criterion_mlp(outputs_dev, labels_dev)\n",
    "                dev_loss += loss_dev_batch.item() * input_ids_dev.size(0)\n",
    "\n",
    "                predicted_probs = torch.sigmoid(outputs_dev)\n",
    "                predicted_labels = (predicted_probs > 0.5).float()\n",
    "                total_dev += labels_dev.size(0)\n",
    "                correct_dev += (predicted_labels == labels_dev).sum().item()\n",
    "        \n",
    "        avg_dev_loss = dev_loss / total_dev if total_dev > 0 else 0\n",
    "        dev_accuracy = 100 * correct_dev / total_dev if total_dev > 0 else 0\n",
    "        print(f\"  Epoch [{epoch+1}/{num_epochs_mlp}], 検証データ: 平均損失={avg_dev_loss:.4f}, 正解率={dev_accuracy:.2f}%\")\n",
    "\n",
    "end_time_mlp = time.time()\n",
    "print(f\"\\nMLPモデルの学習が完了しました。所要時間: {end_time_mlp - start_time_mlp:.2f} 秒\")\n",
    "\n",
    "# --- 学習したモデルの開発セットにおける最終的な正解率を求める ---\n",
    "if dev_dataloader_mlp:\n",
    "    print(\"\\n--- 最終評価 (開発セット、MLPモデル) ---\")\n",
    "    mlp_model.eval()\n",
    "    final_correct_dev_mlp = 0\n",
    "    final_total_dev_mlp = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_data_dev in dev_dataloader_mlp:\n",
    "            input_ids_dev = batch_data_dev['input_ids'].to(device)\n",
    "            labels_dev = batch_data_dev['label'].to(device)\n",
    "            \n",
    "            outputs_dev = mlp_model(input_ids_dev)\n",
    "            predicted_probs = torch.sigmoid(outputs_dev)\n",
    "            predicted_labels = (predicted_probs > 0.5).float()\n",
    "            final_total_dev_mlp += labels_dev.size(0)\n",
    "            final_correct_dev_mlp += (predicted_labels == labels_dev).sum().item()\n",
    "            \n",
    "    if final_total_dev_mlp > 0:\n",
    "        final_dev_accuracy_mlp = 100 * final_correct_dev_mlp / final_total_dev_mlp\n",
    "        print(f\"開発セットにおける最終正解率 (MLPモデル): {final_dev_accuracy_mlp:.2f}%\")\n",
    "        print(f\"  正解した事例数: {final_correct_dev_mlp}\")\n",
    "        print(f\"  総事例数: {final_total_dev_mlp}\")\n",
    "    else:\n",
    "        print(\"開発データでの評価ができませんでした。\")\n",
    "else:\n",
    "    print(\"開発データローダーが準備されていないため、最終評価をスキップします。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c1a0d6-0787-4481-83d2-1929ada265fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
