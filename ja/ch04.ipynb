{"cells":[{"cell_type":"markdown","id":"7kKJxp-jEafg","metadata":{"id":"7kKJxp-jEafg"},"source":["## 事前準備"]},{"cell_type":"code","execution_count":null,"id":"HTP9ucQMEa4g","metadata":{"id":"HTP9ucQMEa4g"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"id":"w7-mEBcQEbHx","metadata":{"id":"w7-mEBcQEbHx"},"outputs":[],"source":["%cd /content/drive/MyDrive/nlp100_2025/ja/\n","\n","import os\n","\n","current_directory = os.getcwd()\n","print(f\"The Current Directory: {current_directory}\")"]},{"cell_type":"markdown","id":"4d5cd462-0ddc-48c0-848c-bc46be0af9fd","metadata":{"id":"4d5cd462-0ddc-48c0-848c-bc46be0af9fd"},"source":["# 第4章: 言語解析\n","\n","問題30から問題35までは、以下の文章`text`（太宰治の『走れメロス』の冒頭部分）に対して、言語解析を実施せよ。問題36から問題39までは、国家を説明した文書群（日本語版ウィキペディア記事から抽出したテキスト群）をコーパスとして、言語解析を実施せよ。"]},{"cell_type":"code","execution_count":null,"id":"7e6766b0-630b-4b78-8312-27e6f8104e7b","metadata":{"id":"7e6766b0-630b-4b78-8312-27e6f8104e7b"},"outputs":[],"source":["text = \"\"\"\n","メロスは激怒した。\n","必ず、かの邪智暴虐の王を除かなければならぬと決意した。\n","メロスには政治がわからぬ。\n","メロスは、村の牧人である。\n","笛を吹き、羊と遊んで暮して来た。\n","けれども邪悪に対しては、人一倍に敏感であった。\n","\"\"\""]},{"cell_type":"code","execution_count":null,"id":"e89fe336-0a1c-43a5-9d22-426ecb578da0","metadata":{"id":"e89fe336-0a1c-43a5-9d22-426ecb578da0"},"outputs":[],"source":["# 事前準備\n","!uv pip install mecab-python3\n","!uv pip install pandas\n","!uv pip install unidic\n","!python3 -m unidic download\n","!uv pip install japanize-matplotlib"]},{"cell_type":"markdown","id":"80e3790d-077d-453a-b981-964f609fc4be","metadata":{"id":"80e3790d-077d-453a-b981-964f609fc4be"},"source":["## 30. 動詞\n","文章`text`に含まれる動詞をすべて表示せよ。"]},{"cell_type":"code","execution_count":null,"id":"b550abec-504c-4882-bbf3-a04077c92e4e","metadata":{"id":"b550abec-504c-4882-bbf3-a04077c92e4e"},"outputs":[],"source":["import MeCab\n","mecab = MeCab.Tagger()\n","\n","for i, line in enumerate(text.splitlines()):\n","    for parsed_line in mecab.parse(line).splitlines():\n","        if parsed_line.split(\"\\t\")[0] != \"EOS\":\n","            if parsed_line.split(\"\\t\")[1].split(\",\")[0] == \"動詞\":\n","                print(\"文中の動詞-> \", parsed_line.split(\"\\t\")[0])\n","                print(parsed_line.split(\"\\t\")[1])\n","                print(\"\")"]},{"cell_type":"markdown","id":"5fd1d613-8375-4837-b534-a01451eddf98","metadata":{"id":"5fd1d613-8375-4837-b534-a01451eddf98"},"source":["## 31. 動詞の原型\n","文章`text`に含まれる動詞と、その原型をすべて表示せよ。"]},{"cell_type":"code","execution_count":null,"id":"73cf9ffd-2beb-49ad-a16a-36487dd0dbfa","metadata":{"id":"73cf9ffd-2beb-49ad-a16a-36487dd0dbfa"},"outputs":[],"source":["import MeCab\n","mecab = MeCab.Tagger()\n","\n","for i, line in enumerate(text.splitlines()):\n","    for parsed_line in mecab.parse(line).splitlines():\n","        if parsed_line.split(\"\\t\")[0] != \"EOS\":\n","            if parsed_line.split(\"\\t\")[1].split(\",\")[0] == \"動詞\":\n","                print(\"文中の動詞-> \", parsed_line.split(\"\\t\")[0])\n","                print(parsed_line.split(\"\\t\")[1])\n","                print(\"動詞の原形-> \", parsed_line.split(\"\\t\")[1].split(\",\")[10])\n","                print(\"\")"]},{"cell_type":"markdown","id":"34558343-a286-429b-b3ee-b1e6b3ca4561","metadata":{"id":"34558343-a286-429b-b3ee-b1e6b3ca4561"},"source":["## 32. 「AのB」\n","文章`text`において、2つの名詞が「の」で連結されている名詞句をすべて抽出せよ。"]},{"cell_type":"code","execution_count":null,"id":"9b22f8f5-e4ae-4046-9369-b61059b97e8b","metadata":{"id":"9b22f8f5-e4ae-4046-9369-b61059b97e8b"},"outputs":[],"source":["import MeCab\n","mecab = MeCab.Tagger()\n","\n","for i, line in enumerate(text.splitlines()):\n","    parsed_list = mecab.parse(line).splitlines()\n","    for index, parsed_line in enumerate(parsed_list):\n","        if parsed_line.split(\"\\t\")[0] != \"EOS\":\n","            if parsed_line.split(\"\\t\")[1].split(\",\")[0] == \"助詞\":\n","                if parsed_line.split(\"\\t\")[0] == \"の\":\n","                    noun1 = parsed_list[index-1].split(\"\\t\")[0]\n","                    particle = parsed_line.split(\"\\t\")[1].split(\",\")[10]\n","                    noun2 = parsed_list[index+1].split(\"\\t\")[0]\n","                    print(f\"名詞句-> {noun1}{particle}{noun2}\")"]},{"cell_type":"markdown","id":"7cac9c74-cd83-42d3-8cde-a4a0f4200094","metadata":{"id":"7cac9c74-cd83-42d3-8cde-a4a0f4200094"},"source":["## 33. 係り受け解析\n","\n","文章`text`に係り受け解析を適用し、係り元と係り先のトークン（形態素や文節などの単位）をタブ区切り形式ですべて抽出せよ。"]},{"cell_type":"code","execution_count":null,"id":"272fe5b1-7c01-4287-af59-b39d61713686","metadata":{"id":"272fe5b1-7c01-4287-af59-b39d61713686"},"outputs":[],"source":["!uv pip install ginza ja_ginza"]},{"cell_type":"code","execution_count":null,"id":"ecf4d3dd-bb66-4a02-9c5b-f2da1a050146","metadata":{"id":"ecf4d3dd-bb66-4a02-9c5b-f2da1a050146"},"outputs":[],"source":["import spacy\n","\n","try:\n","    nlp = spacy.load('ja_ginza')\n","except OSError:\n","    # Fallback or specific model if ja_ginza is not directly available\n","    # For example, if you downloaded 'ja_core_news_sm'\n","    nlp = spacy.load('ja_core_news_sm')\n","\n","# Process the text\n","doc = nlp(text)\n","\n","print(\"係り元トークン\\t係り先トークン\")\n","print(\"----------------\\t----------------\")\n","\n","for sent in doc.sents:\n","    for token in sent:\n","        if token.head != token: # Avoid printing self-dependencies for the root\n","            print(f\"{token.text}\\t{token.head.text}\")\n","    print(\"--- (End of Sentence) ---\")"]},{"cell_type":"markdown","id":"eb49b4d2-9869-4f43-9cd8-9e79e0c1b40f","metadata":{"id":"eb49b4d2-9869-4f43-9cd8-9e79e0c1b40f"},"source":["## 34. 主述の関係\n","文章`text`において、「メロス」が主語であるときの述語を抽出せよ。"]},{"cell_type":"code","execution_count":null,"id":"af8d5733-c576-40b8-957e-31f09474f376","metadata":{"id":"af8d5733-c576-40b8-957e-31f09474f376"},"outputs":[],"source":["import spacy\n","\n","# 日本語の言語モデルをロードします (GiNZA または spaCy の日本語モデル)\n","# 'ja_ginza' をインストールした場合、 'ja_ginza' を使用できます\n","# 'ja_core_news_sm' のような特定のモデルをダウンロードした場合は、それを使用します\n","try:\n","    nlp = spacy.load('ja_ginza')\n","except OSError:\n","    # ja_ginza が直接利用できない場合のフォールバックまたは特定のモデル\n","    # 例: 'ja_core_news_sm' をダウンロードした場合\n","    nlp = spacy.load('ja_core_news_sm')\n","\n","# テキストを処理します\n","doc = nlp(text)\n","\n","print(\"「メロス」が主語の場合の述語:\")\n","print(\"--------------------------\")\n","\n","# 文書内の各トークンに対して反復処理を行います\n","# 「メロス」を主語としてより洗練された検索を行うアプローチ\n","for token in doc:\n","    # トークンのテキストが「メロス」で、かつその依存関係ラベルが主語を示すものであるかを確認します\n","    if token.text == \"メロス\" and token.dep_ in (\"nsubj\", \"nsubj:outer\"):\n","        # token.head が述語の主要部（多くは動詞や形容詞）です\n","        predicate_head = token.head\n","\n","        # 述語句を構成するため、述語の頭部から開始します\n","        predicate_phrase = predicate_head.text\n","\n","        # 述語の頭部に続く助動詞や助詞（右方の子要素）を取得して述語句に追加します\n","        # これにより、例えば「激怒した」のような句を捉えることができます\n","        for right_token in predicate_head.rights: # head の右側の子要素をチェック\n","            if right_token.dep_ in (\"aux\", \"mark\"): # 一般的な助動詞やマーカー(助詞など)の依存関係\n","                predicate_phrase += right_token.text\n","\n","        print(f\"メロス (主語) -> {predicate_phrase} (述語の原形: {predicate_head.lemma_})\")"]},{"cell_type":"markdown","id":"ec8ba153-85e7-4cbe-bcc0-bb659409d494","metadata":{"id":"ec8ba153-85e7-4cbe-bcc0-bb659409d494"},"source":["## 35. 係り受け木\n","「メロスは激怒した。」の係り受け木を可視化せよ。"]},{"cell_type":"code","execution_count":null,"id":"53d64aaf-d05d-4d9b-8e2e-e8cb765dce66","metadata":{"id":"53d64aaf-d05d-4d9b-8e2e-e8cb765dce66"},"outputs":[],"source":["!uv pip install spacy ginza ja_ginza matplotlib"]},{"cell_type":"code","execution_count":null,"id":"d37e543c-f286-44e3-bcfc-809f878a9144","metadata":{"id":"d37e543c-f286-44e3-bcfc-809f878a9144"},"outputs":[],"source":["import spacy\n","from spacy import displacy\n","from IPython.display import HTML, display # こちらを明示的にインポート\n","\n","# 日本語の言語モデルをロードします\n","try:\n","    nlp = spacy.load('ja_ginza')\n","except OSError:\n","    nlp = spacy.load('ja_core_news_sm') # モデルがない場合は適宜変更してください\n","\n","# 対象の文\n","sentence = \"メロスは激怒した。\"\n","\n","# テキストを処理します\n","doc = nlp(sentence)\n","\n","# 係り受け木をHTML文字列として取得します\n","html = displacy.render(doc, style='dep', jupyter=False, options={'distance': 100})\n","\n","# IPython.display を使ってHTMLを表示します\n","print(f\"「{sentence}」の係り受け木:\")\n","display(HTML(html)) # ここで display を使用\n","\n","# もしSVGとして保存したい場合は、以下のようにします：\n","# svg_output = displacy.render(doc, style='dep', page=False, minify=True, options={'distance':100}) # page=False, minify=True はSVG出力時の一般的なオプション\n","# with open(\"dependency_tree.svg\", \"w\", encoding=\"utf-8\") as f:\n","#     f.write(svg_output)\n","# print(\"係り受け木を dependency_tree.svg として保存しました。\")"]},{"cell_type":"markdown","id":"b880d760-ea6c-4722-9a65-5b97a02ed192","metadata":{"id":"b880d760-ea6c-4722-9a65-5b97a02ed192"},"source":["## 36. 単語の出現頻度\n","\n","問題36から39までは、Wikipediaの記事を以下のフォーマットで書き出したファイル[jawiki-country.json.gz](/data/jawiki-country.json.gz)をコーパスと見なし、統計的な分析を行う。\n","\n","* 1行に1記事の情報がJSON形式で格納される\n","* 各行には記事名が\"title\"キーに、記事本文が\"text\"キーの辞書オブジェクトに格納され、そのオブジェクトがJSON形式で書き出される\n","* ファイル全体はgzipで圧縮される\n","\n","まず、第3章の処理内容を参考に、Wikipedia記事からマークアップを除去し、各記事のテキストを抽出せよ。そして、コーパスにおける単語（形態素）の出現頻度を求め、出現頻度の高い20語とその出現頻度を表示せよ。"]},{"cell_type":"code","execution_count":null,"id":"00c05a09-f746-4e82-bc14-4aede23a8295","metadata":{"id":"00c05a09-f746-4e82-bc14-4aede23a8295"},"outputs":[],"source":["!uv pip install tqdm"]},{"cell_type":"code","execution_count":null,"id":"b37f45f5-f862-4bbb-b8dc-d7630e4d03a0","metadata":{"id":"b37f45f5-f862-4bbb-b8dc-d7630e4d03a0"},"outputs":[],"source":["import gzip\n","import json\n","import re\n","from collections import Counter\n","import spacy\n","from tqdm.auto import tqdm # tqdm をインポート (Jupyter Notebook用)\n","\n","# GiNZAまたは適切な日本語モデルをロード\n","try:\n","    nlp = spacy.load('ja_ginza')\n","except OSError:\n","    nlp = spacy.load('ja_core_news_sm') # 適宜変更\n","\n","# --- 1. ファイルの読み込み ---\n","filepath = '../data/jawiki-country.json' # ユーザーのエラーメッセージに合わせたパス\n","\n","# --- 2. マークアップの除去 ---\n","def remove_markup(text):\n","    if not text:\n","        return \"\"\n","    text = re.sub(r\"'{2,5}(.+?)'{2,5}\", r\"\\1\", text)\n","    text = re.sub(r\"\\[\\[(?:[^|\\]]*?\\|)*?([^|\\]]+?)\\]\\]\", r\"\\1\", text)\n","    text = re.sub(r\"\\[https?://[^\\s]+?\\s([^\\]]+?)\\]\", r\"\\1\", text)\n","    text = re.sub(r\"\\[https?://[^\\s]+?\\]\", \"\", text)\n","    text = re.sub(r\"\\{\\{.*?\\}\\}\", \"\", text, flags=re.DOTALL)\n","    text = re.sub(r\"<.+?>\", \"\", text, flags=re.DOTALL)\n","    text = re.sub(r\"#REDIRECT \\[\\[(.*?)\\]\\]\", r\"\\1\", text)\n","    text = re.sub(r\"^\\*+\\s*\", \"\", text, flags=re.MULTILINE)\n","    text = re.sub(r\"<ref(?:[^<>]|\\s)*?(?:/>|</ref>)\", \"\", text, flags=re.DOTALL)\n","    text = re.sub(r\"<references\\s*/>\", \"\", text, flags=re.DOTALL)\n","    text = re.sub(r\"\", \"\", text, flags=re.DOTALL)\n","    return text\n","\n","# --- 3 & 4. 全記事の形態素解析と出現頻度の集計 ---\n","word_counts = Counter()\n","articles_processed = 0\n","TOKENIZER_BYTE_LIMIT = 49149\n","\n","print(f\"'{filepath}' から記事を読み込み、形態素解析と単語頻度の集計を開始します...\")\n","\n","# ファイルの総行数を取得 (プログレスバーのtotalに使用)\n","total_lines = 0\n","try:\n","    with open(filepath, 'r', encoding='utf-8') as f_count:\n","        for _ in f_count:\n","            total_lines += 1\n","    if total_lines == 0:\n","        print(\"警告: ファイルが空であるか、読み込めませんでした。\")\n","except FileNotFoundError:\n","    print(f\"エラー: ファイル '{filepath}' が見つかりません。処理を中断します。\")\n","    total_lines = -1 # エラーフラグ\n","except Exception as e:\n","    print(f\"ファイルの行数カウント中にエラー: {e}\")\n","    total_lines = -1 # エラーフラグ\n","\n","\n","if total_lines > 0:\n","    try:\n","        with open(filepath, 'r', encoding='utf-8') as f:\n","            # tqdmでラップしてプログレスバーを表示 (descは説明文, totalは総数)\n","            for i, line in tqdm(enumerate(f), total=total_lines, desc=\"記事処理中\"):\n","                try:\n","                    article = json.loads(line)\n","                    title = article.get('title', '不明なタイトル')\n","                    text_with_markup = article.get('text', '')\n","\n","                    plain_text = remove_markup(text_with_markup)\n","\n","                    if plain_text.strip():\n","                        chunks = plain_text.split('\\n\\n')\n","                        for chunk_num, chunk in enumerate(chunks):\n","                            if not chunk.strip():\n","                                continue\n","                            sub_chunks = chunk.split('\\n')\n","                            for sub_chunk_num, sub_chunk in enumerate(sub_chunks):\n","                                if not sub_chunk.strip():\n","                                    continue\n","                                if len(sub_chunk.encode('utf-8')) > TOKENIZER_BYTE_LIMIT:\n","                                    # print(f\"警告: 記事「{title}」のサブチャンク ({chunk_num}-{sub_chunk_num}) が長すぎます。スキップします。\") # 詳細すぎるので抑制\n","                                    continue\n","                                doc = nlp(sub_chunk)\n","                                for token in doc:\n","                                    word_counts[token.lemma_] += 1\n","\n","                    articles_processed += 1\n","                    # tqdmが更新するので、定期的なprintは不要になることが多い\n","                    # if articles_processed % 100 == 0:\n","                    #     print(f\"{articles_processed} 件の記事を処理完了...\")\n","\n","\n","                except json.JSONDecodeError:\n","                    tqdm.write(f\"警告: {i+1}行目のJSONデータのデコードに失敗しました。スキップします。\") # tqdm.writeを使うとバーを壊さない\n","                except Exception as e:\n","                    tqdm.write(f\"警告: {i+1}行目の記事「{title}」処理中にエラーが発生しました: {e}\")\n","\n","    except FileNotFoundError:\n","        # この部分はファイルの行数カウントで既にチェックされているが、念のため\n","        print(f\"エラー: ファイル '{filepath}' が見つかりませんでした。\")\n","        word_counts = Counter()\n","    except Exception as e:\n","        print(f\"ファイルの読み込みまたは処理中に予期せぬエラーが発生しました: {e}\")\n","        word_counts = Counter()\n","\n","if articles_processed > 0:\n","    print(f\"\\n全 {articles_processed} 件の記事の処理が完了しました。\")\n","    print(\"\\n単語の出現頻度 上位20語 (基本形):\")\n","    for word, count in word_counts.most_common(20):\n","        print(f\"{word}: {count}\")\n","elif total_lines == 0: # ファイルは存在したが空だった場合\n","    print(\"処理対象の記事がファイル内にありませんでした。\")\n","elif total_lines == -1: # ファイルが見つからなかったか、行数カウントでエラー\n","    print(\"ファイル処理エラーのため、結果はありません。\")\n","else: # total_lines が 0 で、articles_processed も 0 (正常にループしたが処理対象なし)\n","    print(\"処理できる記事がありませんでした。\")"]},{"cell_type":"markdown","id":"e38bd27c-0644-4468-8b4d-99318b91c0cb","metadata":{"id":"e38bd27c-0644-4468-8b4d-99318b91c0cb"},"source":["## 37. 名詞の出現頻度\n","コーパスにおける名詞の出現頻度を求め、出現頻度の高い20語とその出現頻度を表示せよ。"]},{"cell_type":"code","execution_count":null,"id":"cdb0ffd1-75e6-4605-9a8b-3c39b82a9232","metadata":{"colab":{"background_save":true},"id":"cdb0ffd1-75e6-4605-9a8b-3c39b82a9232"},"outputs":[],"source":["import gzip\n","import json\n","import re\n","from collections import Counter\n","import spacy\n","from tqdm.auto import tqdm # tqdm をインポート (Jupyter Notebook用)\n","\n","# GiNZAまたは適切な日本語モデルをロード\n","try:\n","    nlp = spacy.load('ja_ginza')\n","except OSError:\n","    nlp = spacy.load('ja_core_news_sm') # 適宜変更\n","\n","# --- 1. ファイルの読み込み ---\n","# ユーザーの環境に合わせてファイルパスを確認してください。\n","# 前回の実行で '../data/jawiki-country.json' を使用していたので、それを継続します。\n","filepath = '../data/jawiki-country.json'\n","\n","# --- 2. マークアップの除去 ---\n","def remove_markup(text):\n","    if not text:\n","        return \"\"\n","    text = re.sub(r\"'{2,5}(.+?)'{2,5}\", r\"\\1\", text)\n","    text = re.sub(r\"\\[\\[(?:[^|\\]]*?\\|)*?([^|\\]]+?)\\]\\]\", r\"\\1\", text)\n","    text = re.sub(r\"\\[https?://[^\\s]+?\\s([^\\]]+?)\\]\", r\"\\1\", text)\n","    text = re.sub(r\"\\[https?://[^\\s]+?\\]\", \"\", text)\n","    text = re.sub(r\"\\{\\{.*?\\}\\}\", \"\", text, flags=re.DOTALL)\n","    text = re.sub(r\"<.+?>\", \"\", text, flags=re.DOTALL)\n","    text = re.sub(r\"#REDIRECT \\[\\[(.*?)\\]\\]\", r\"\\1\", text)\n","    text = re.sub(r\"^\\*+\\s*\", \"\", text, flags=re.MULTILINE)\n","    text = re.sub(r\"<ref(?:[^<>]|\\s)*?(?:/>|</ref>)\", \"\", text, flags=re.DOTALL)\n","    text = re.sub(r\"<references\\s*/>\", \"\", text, flags=re.DOTALL)\n","    text = re.sub(r\"\", \"\", text, flags=re.DOTALL)\n","    return text\n","\n","# --- 3 & 4. 全記事の形態素解析と「名詞」の出現頻度の集計 ---\n","noun_counts = Counter() # 名詞の頻度を格納するCounter\n","articles_processed = 0\n","TOKENIZER_BYTE_LIMIT = 49149\n","\n","print(f\"'{filepath}' から記事を読み込み、形態素解析と「名詞」の頻度の集計を開始します...\")\n","\n","# ファイルの総行数を取得 (プログレスバーのtotalに使用)\n","total_lines = 0\n","try:\n","    with open(filepath, 'r', encoding='utf-8') as f_count:\n","        for _ in f_count:\n","            total_lines += 1\n","    if total_lines == 0:\n","        print(\"警告: ファイルが空であるか、読み込めませんでした。\")\n","except FileNotFoundError:\n","    print(f\"エラー: ファイル '{filepath}' が見つかりません。処理を中断します。\")\n","    total_lines = -1 # エラーフラグ\n","except Exception as e:\n","    print(f\"ファイルの行数カウント中にエラー: {e}\")\n","    total_lines = -1 # エラーフラグ\n","\n","if total_lines > 0:\n","    try:\n","        with open(filepath, 'r', encoding='utf-8') as f:\n","            for i, line in tqdm(enumerate(f), total=total_lines, desc=\"名詞カウント中\"):\n","                try:\n","                    article = json.loads(line)\n","                    title = article.get('title', '不明なタイトル')\n","                    text_with_markup = article.get('text', '')\n","\n","                    plain_text = remove_markup(text_with_markup)\n","\n","                    if plain_text.strip():\n","                        chunks = plain_text.split('\\n\\n')\n","                        for chunk_num, chunk in enumerate(chunks):\n","                            if not chunk.strip():\n","                                continue\n","                            sub_chunks = chunk.split('\\n')\n","                            for sub_chunk_num, sub_chunk in enumerate(sub_chunks):\n","                                if not sub_chunk.strip():\n","                                    continue\n","                                if len(sub_chunk.encode('utf-8')) > TOKENIZER_BYTE_LIMIT:\n","                                    # tqdm.write(f\"警告: 記事「{title}」のサブチャンク長すぎスキップ\") # 詳細すぎるログは抑制\n","                                    continue\n","\n","                                doc = nlp(sub_chunk)\n","                                for token in doc:\n","                                    # 品詞が名詞 (NOUN: 普通名詞, PROPN: 固有名詞) の場合のみカウント\n","                                    if token.pos_ in ['NOUN', 'PROPN']:\n","                                        noun_counts[token.lemma_] += 1 # 名詞の基本形をカウント\n","                                        # noun_counts[token.text] += 1 # 表層形をカウントする場合はこちら\n","\n","                    articles_processed += 1\n","\n","                except json.JSONDecodeError:\n","                    tqdm.write(f\"警告: {i+1}行目のJSONデコード失敗。スキップします。\")\n","                except Exception as e:\n","                    tqdm.write(f\"警告: {i+1}行目記事「{title}」処理中エラー: {e}\")\n","\n","    except FileNotFoundError:\n","        print(f\"エラー: ファイル '{filepath}' が見つかりませんでした。\")\n","        noun_counts = Counter()\n","    except Exception as e:\n","        print(f\"ファイル読み込み/処理中エラー: {e}\")\n","        noun_counts = Counter()\n","\n","if articles_processed > 0:\n","    print(f\"\\n全 {articles_processed} 件の記事の処理が完了しました。\")\n","    # --- 5. 上位20語の名詞の表示 ---\n","    print(\"\\n「名詞」の出現頻度 上位20語 (基本形):\")\n","    for noun, count in noun_counts.most_common(20):\n","        print(f\"{noun}: {count}\")\n","elif total_lines == 0:\n","    print(\"処理対象の記事がファイル内にありませんでした。\")\n","elif total_lines == -1:\n","    print(\"ファイル処理エラーのため、結果はありません。\")\n","else:\n","    print(\"処理できる記事がありませんでした。\")"]},{"cell_type":"markdown","id":"8a64fda0-5ee3-48ad-9071-f37f805951d3","metadata":{"id":"8a64fda0-5ee3-48ad-9071-f37f805951d3"},"source":["## 38. TF・IDF\n","日本に関する記事における名詞のTF・IDFスコアを求め、TF・IDFスコア上位20語とそのTF, IDF, TF・IDFを表示せよ。"]},{"cell_type":"code","execution_count":null,"id":"becb66a6-d6b5-435b-a01d-561266443ee1","metadata":{"id":"becb66a6-d6b5-435b-a01d-561266443ee1"},"outputs":[],"source":["import json\n","import re\n","from collections import Counter\n","import spacy\n","from tqdm.auto import tqdm\n","import math # IDF計算のため\n","\n","# GiNZAまたは適切な日本語モデルをロード\n","try:\n","    nlp = spacy.load('ja_ginza')\n","except OSError:\n","    nlp = spacy.load('ja_core_news_sm')\n","\n","filepath = '../data/jawiki-country.json' # ユーザーの環境に合わせてファイルパスを確認\n","TOKENIZER_BYTE_LIMIT = 49149\n","\n","def remove_markup(text):\n","    if not text: return \"\"\n","    text = re.sub(r\"'{2,5}(.+?)'{2,5}\", r\"\\1\", text)\n","    text = re.sub(r\"\\[\\[(?:[^|\\]]*?\\|)*?([^|\\]]+?)\\]\\]\", r\"\\1\", text)\n","    text = re.sub(r\"\\[https?://[^\\s]+?\\s([^\\]]+?)\\]\", r\"\\1\", text)\n","    text = re.sub(r\"\\[https?://[^\\s]+?\\]\", \"\", text)\n","    text = re.sub(r\"\\{\\{.*?\\}\\}\", \"\", text, flags=re.DOTALL)\n","    text = re.sub(r\"<.+?>\", \"\", text, flags=re.DOTALL)\n","    text = re.sub(r\"^\\*+\\s*\", \"\", text, flags=re.MULTILINE)\n","    text = re.sub(r\"<ref(?:[^<>]|\\s)*?(?:/>|</ref>)\", \"\", text, flags=re.DOTALL)\n","    text = re.sub(r\"<references\\s*/>\", \"\", text, flags=re.DOTALL)\n","    text = re.sub(r\"\", \"\", text, flags=re.DOTALL)\n","    return text\n","\n","print(f\"'{filepath}' から記事を読み込み、TF-IDF計算のための前処理を開始します...\")\n","\n","# --- ステップ1: 全記事の名詞とその出現回数、DFを収集 ---\n","# all_doc_noun_counts: 各記事の名詞のCounterを格納するリスト\n","# doc_freq: 各名詞がいくつの文書に出現したか (Document Frequency)\n","all_doc_noun_counts = []\n","doc_freq = Counter()\n","titles_list = [] # IDF計算時の総文書数Nと、「日本」の記事のインデックス特定のため\n","\n","total_lines = 0\n","try:\n","    with open(filepath, 'r', encoding='utf-8') as f_count:\n","        for _ in f_count:\n","            total_lines += 1\n","except FileNotFoundError:\n","    print(f\"エラー: ファイル '{filepath}' が見つかりません。\")\n","    total_lines = -1\n","\n","if total_lines > 0:\n","    try:\n","        with open(filepath, 'r', encoding='utf-8') as f:\n","            for line in tqdm(f, total=total_lines, desc=\"全記事の前処理中\"):\n","                article = json.loads(line)\n","                titles_list.append(article.get('title', ''))\n","                text_with_markup = article.get('text', '')\n","                plain_text = remove_markup(text_with_markup)\n","\n","                current_doc_nouns = Counter()\n","                if plain_text.strip():\n","                    chunks = plain_text.split('\\n\\n')\n","                    for chunk in chunks:\n","                        if not chunk.strip(): continue\n","                        sub_chunks = chunk.split('\\n')\n","                        for sub_chunk in sub_chunks:\n","                            if not sub_chunk.strip(): continue\n","                            if len(sub_chunk.encode('utf-8')) > TOKENIZER_BYTE_LIMIT: continue\n","\n","                            doc_chunk = nlp(sub_chunk)\n","                            for token in doc_chunk:\n","                                if token.pos_ in ['NOUN', 'PROPN']:\n","                                    current_doc_nouns[token.lemma_] += 1\n","\n","                all_doc_noun_counts.append(current_doc_nouns)\n","                # DFの更新 (その文書に出現したユニークな名詞をカウント)\n","                for noun in current_doc_nouns.keys():\n","                    doc_freq[noun] += 1\n","\n","    except Exception as e:\n","        print(f\"処理中にエラーが発生しました: {e}\")\n","else:\n","    print(\"処理対象ファイルが見つからないか空です。\")\n","\n","if not all_doc_noun_counts:\n","    print(\"有効な記事が処理されませんでした。TF-IDF計算をスキップします。\")\n","else:\n","    # --- ステップ2: 「日本」の記事の特定 ---\n","    target_article_title = \"日本\"\n","    target_doc_index = -1\n","    try:\n","        target_doc_index = titles_list.index(target_article_title)\n","    except ValueError:\n","        print(f\"エラー: 記事「{target_article_title}」が見つかりませんでした。\")\n","        target_doc_index = -1\n","\n","    if target_doc_index != -1:\n","        print(f\"\\n記事「{target_article_title}」のTF-IDFを計算します...\")\n","        target_doc_nouns = all_doc_noun_counts[target_doc_index]\n","        total_nouns_in_target_doc = sum(target_doc_nouns.values())\n","\n","        if total_nouns_in_target_doc == 0:\n","            print(f\"記事「{target_article_title}」には名詞が含まれていません。\")\n","        else:\n","            # --- ステップ3, 4, 5: TF, IDF, TF-IDF の計算 ---\n","            tfidf_scores = {}\n","            num_total_docs = len(all_doc_noun_counts) # N: 総文書数\n","\n","            for noun, tf_count in target_doc_nouns.items():\n","                # TFの計算\n","                tf = tf_count / total_nouns_in_target_doc\n","\n","                # IDFの計算\n","                # N / (DF(t) + 1) : +1 はDFが0の場合やスムージングのため\n","                df = doc_freq.get(noun, 0) # もしdoc_freqにない場合は0 (理論上ありえないはずだが安全のため)\n","                idf = math.log(num_total_docs / (df + 1)) # 自然対数\n","                # スムージングなしで、DFが0の単語を除外する場合\n","                # if df > 0:\n","                #     idf = math.log(num_total_docs / df) + 1 # 別のIDF計算式\n","                # else:\n","                #     idf = math.log(num_total_docs / 1) + 1 # DFが0の単語のIDF（例）\n","\n","                tfidf = tf * idf\n","                tfidf_scores[noun] = {'tf': tf, 'idf': idf, 'tfidf': tfidf}\n","\n","            # --- ステップ6: 上位20語の表示 ---\n","            # TF-IDFスコアでソート\n","            sorted_tfidf = sorted(tfidf_scores.items(), key=lambda item: item[1]['tfidf'], reverse=True)\n","\n","            print(f\"\\n記事「{target_article_title}」における名詞のTF-IDFスコア上位20:\")\n","            print(\"----------------------------------------------------------\")\n","            print(f\"{'名詞':<15} {'TF':<10} {'IDF':<10} {'TF-IDF':<10}\")\n","            print(\"----------------------------------------------------------\")\n","            for noun, scores in sorted_tfidf[:20]:\n","                print(f\"{noun:<15} {scores['tf']:.4f}     {scores['idf']:.4f}     {scores['tfidf']:.4f}\")\n","    else:\n","        if titles_list: # titles_listが空でない（＝記事はあった）が「日本」がなかった場合\n","             print(f\"記事リストに「{target_article_title}」が含まれていませんでした。利用可能なタイトル例: {titles_list[:5]}\")"]},{"cell_type":"markdown","id":"5f3b60e9-5d8e-4c2e-a68d-385c91713113","metadata":{"id":"5f3b60e9-5d8e-4c2e-a68d-385c91713113"},"source":["## 39. Zipfの法則\n","コーパスにおける単語の出現頻度順位を横軸、その出現頻度を縦軸として、両対数グラフをプロットせよ。"]},{"cell_type":"code","execution_count":null,"id":"88050fbf-941a-4b49-ab0e-08e19138acda","metadata":{"id":"88050fbf-941a-4b49-ab0e-08e19138acda"},"outputs":[],"source":["!uv pip install japanize-matplotlib"]},{"cell_type":"code","execution_count":null,"id":"44252207-6bed-410a-b32a-cdf896806b80","metadata":{"id":"44252207-6bed-410a-b32a-cdf896806b80"},"outputs":[],"source":["import json\n","import re\n","from collections import Counter\n","import spacy\n","from tqdm.auto import tqdm\n","import matplotlib.pyplot as plt\n","import japanize_matplotlib # 日本語フォント設定のため\n","\n","# --- (36番の処理: word_counts を得るまで) ---\n","# この部分は36番のコードを再利用します。\n","# word_counts が既に計算済みであれば、このセクションはスキップできます。\n","# もし未計算の場合は、36番のコードを実行して word_counts を作成してください。\n","\n","# GiNZAまたは適切な日本語モデルをロード\n","try:\n","    nlp = spacy.load('ja_ginza')\n","except OSError:\n","    nlp = spacy.load('ja_core_news_sm')\n","\n","filepath = '../data/jawiki-country.json' # ユーザーの環境に合わせてファイルパスを確認\n","TOKENIZER_BYTE_LIMIT = 49149\n","\n","def remove_markup(text):\n","    if not text: return \"\"\n","    text = re.sub(r\"'{2,5}(.+?)'{2,5}\", r\"\\1\", text)\n","    text = re.sub(r\"\\[\\[(?:[^|\\]]*?\\|)*?([^|\\]]+?)\\]\\]\", r\"\\1\", text)\n","    text = re.sub(r\"\\[https?://[^\\s]+?\\s([^\\]]+?)\\]\", r\"\\1\", text)\n","    text = re.sub(r\"\\[https?://[^\\s]+?\\]\", \"\", text)\n","    text = re.sub(r\"\\{\\{.*?\\}\\}\", \"\", text, flags=re.DOTALL)\n","    text = re.sub(r\"<.+?>\", \"\", text, flags=re.DOTALL)\n","    text = re.sub(r\"^\\*+\\s*\", \"\", text, flags=re.MULTILINE)\n","    text = re.sub(r\"<ref(?:[^<>]|\\s)*?(?:/>|</ref>)\", \"\", text, flags=re.DOTALL)\n","    text = re.sub(r\"<references\\s*/>\", \"\", text, flags=re.DOTALL)\n","    text = re.sub(r\"\", \"\", text, flags=re.DOTALL)\n","    return text\n","\n","# word_counts がこのスコープで利用可能か確認\n","# もし前のセルで計算済みなら、再計算は不要\n","if 'word_counts' not in locals() or not isinstance(word_counts, Counter) or not word_counts:\n","    print(\"word_counts が未計算または空です。36番の処理を実行して単語頻度を計算します...\")\n","    word_counts = Counter()\n","    articles_processed = 0\n","\n","    total_lines = 0\n","    try:\n","        with open(filepath, 'r', encoding='utf-8') as f_count:\n","            for _ in f_count:\n","                total_lines += 1\n","    except FileNotFoundError:\n","        print(f\"エラー: ファイル '{filepath}' が見つかりません。\")\n","        total_lines = -1\n","\n","    if total_lines > 0:\n","        try:\n","            with open(filepath, 'r', encoding='utf-8') as f:\n","                for i, line in tqdm(enumerate(f), total=total_lines, desc=\"単語頻度計算中 (36番相当)\"):\n","                    try:\n","                        article = json.loads(line)\n","                        title = article.get('title', '不明なタイトル')\n","                        text_with_markup = article.get('text', '')\n","                        plain_text = remove_markup(text_with_markup)\n","\n","                        if plain_text.strip():\n","                            chunks = plain_text.split('\\n\\n')\n","                            for chunk in chunks:\n","                                if not chunk.strip(): continue\n","                                sub_chunks = chunk.split('\\n')\n","                                for sub_chunk in sub_chunks:\n","                                    if not sub_chunk.strip(): continue\n","                                    if len(sub_chunk.encode('utf-8')) > TOKENIZER_BYTE_LIMIT: continue\n","                                    doc_chunk = nlp(sub_chunk)\n","                                    for token in doc_chunk:\n","                                        word_counts[token.lemma_] += 1 # 基本形をカウント\n","                        articles_processed += 1\n","                    except json.JSONDecodeError:\n","                        tqdm.write(f\"警告(36): {i+1}行目JSONデコード失敗。スキップ\")\n","                    except Exception as e:\n","                        tqdm.write(f\"警告(36): {i+1}行目記事「{title}」処理中エラー: {e}\")\n","            print(f\"全 {articles_processed} 件の記事の単語頻度計算が完了しました。\")\n","        except FileNotFoundError:\n","             print(f\"エラー(36): ファイル '{filepath}' が見つかりませんでした。\")\n","             word_counts = Counter() # 念のため空に\n","        except Exception as e:\n","            print(f\"エラー(36): ファイル処理中にエラー: {e}\")\n","            word_counts = Counter() # 念のため空に\n","    else:\n","        print(\"処理対象ファイルが見つからないか空です(36)。Zipfのプロットはできません。\")\n","\n","# --- 39. Zipfの法則のプロット ---\n","if word_counts: # word_counts にデータがある場合のみプロット\n","    # 1. 単語の出現頻度を取得し、降順にソート\n","    frequencies = [count for word, count in word_counts.most_common()]\n","\n","    # 2. 順位を生成 (1位, 2位, 3位, ...)\n","    ranks = range(1, len(frequencies) + 1)\n","\n","    # 3. 両対数グラフのプロット\n","    plt.figure(figsize=(10, 6))\n","    plt.plot(ranks, frequencies, marker='.', linestyle='none') # 各点をプロット\n","\n","    plt.xscale('log') # 横軸を対数スケールに\n","    plt.yscale('log') # 縦軸を対数スケールに\n","\n","    plt.xlabel('出現頻度順位 (log scale)')\n","    plt.ylabel('出現頻度 (log scale)')\n","    plt.title('Zipfの法則: 単語の出現頻度と順位 (両対数グラフ)')\n","    plt.grid(True)\n","    plt.show()\n","else:\n","    print(\"単語の出現頻度データがありません。グラフをプロットできません。\")"]},{"cell_type":"code","execution_count":null,"id":"05ffba8c-edf9-4913-a177-275b3bf4320a","metadata":{"id":"05ffba8c-edf9-4913-a177-275b3bf4320a"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","toc_visible":true,"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.12"}},"nbformat":4,"nbformat_minor":5}