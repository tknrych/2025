{"cells":[{"cell_type":"markdown","source":["## 事前準備"],"metadata":{"id":"sel4ZiqGQ2dZ"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"ogy6hDNyQ2uX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/nlp100_2025/ja/\n","\n","import os\n","\n","current_directory = os.getcwd()\n","print(f\"The Current Directory: {current_directory}\")"],"metadata":{"id":"fVt2uNA-Q3KR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"editable":true,"id":"X5YgWh7ldCR4","tags":[]},"source":["# 第7章: 機械学習\n","\n","本章では、[Stanford Sentiment Treebank (SST)](https://nlp.stanford.edu/sentiment/) データセットを用い、評判分析器（ポジネガ分類器）を構築する。ここでは処理を簡略化するため、[General Language Understanding Evaluation (GLUE)](https://gluebenchmark.com/) ベンチマークで配布されているSSTデータセットを用いる。\n"]},{"cell_type":"markdown","metadata":{"id":"xaAFO9l2fE19"},"source":["## 60. データの入手・整形\n","\n","GLUEのウェブサイトから[SST-2](https://dl.fbaipublicfiles.com/glue/data/SST-2.zip)データセットを取得せよ。学習データ（`train.tsv`）と検証データ（`dev.tsv`）のぞれぞれについて、ポジティブ (1) とネガティブ (0) の事例数をカウントせよ。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iEgCyG2-Qv6e"},"outputs":[],"source":["import requests\n","import zipfile\n","import io\n","import pandas as pd\n","import os # ファイル/ディレクトリ操作用\n","\n","# 1. データセットのダウンロードと展開\n","sst2_zip_url = \"https://dl.fbaipublicfiles.com/glue/data/SST-2.zip\"\n","download_dir = \"../data/SST-2_data\" # ダウンロードしたファイルを展開するディレクトリ名\n","train_file_path = os.path.join(download_dir, \"SST-2/train.tsv\")\n","dev_file_path = os.path.join(download_dir, \"SST-2/dev.tsv\")\n","\n","# ディレクトリがなければ作成\n","if not os.path.exists(download_dir):\n","    os.makedirs(download_dir)\n","    print(f\"ディレクトリ '{download_dir}' を作成しました。\")\n","\n","try:\n","    print(f\"'{sst2_zip_url}' からSST-2データセットをダウンロードしています...\")\n","    response = requests.get(sst2_zip_url)\n","    response.raise_for_status() # エラーチェック\n","\n","    print(\"ダウンロード完了。zipファイルを展開しています...\")\n","    with zipfile.ZipFile(io.BytesIO(response.content)) as z:\n","        z.extractall(download_dir) # 指定したディレクトリに展開\n","    print(f\"SST-2データセットを '{download_dir}' に展開しました。\")\n","\n","except requests.exceptions.RequestException as e:\n","    print(f\"データセットのダウンロード中にエラーが発生しました: {e}\")\n","except zipfile.BadZipFile:\n","    print(\"エラー: ダウンロードしたファイルが正しいzipファイルではありません。\")\n","except Exception as e:\n","    print(f\"データセットの処理中に予期せぬエラーが発生しました: {e}\")\n","\n","\n","# 3. データの読み込みとラベル数のカウント\n","def count_labels(file_path, dataset_name):\n","    \"\"\"指定されたTSVファイルを読み込み、ラベルの数をカウントして表示する関数\"\"\"\n","    try:\n","        df = pd.read_csv(file_path, sep='\\t')\n","        print(f\"\\n--- {dataset_name} ({os.path.basename(file_path)}) ---\")\n","        print(\"最初の数行:\")\n","        print(df.head())\n","        print(\"\\nラベルごとの事例数:\")\n","        print(df['label'].value_counts())\n","        return df['label'].value_counts()\n","    except FileNotFoundError:\n","        print(f\"エラー: ファイル '{file_path}' が見つかりません。ダウンロードと展開が正しく行われたか確認してください。\")\n","        return None\n","    except KeyError:\n","        print(f\"エラー: ファイル '{file_path}' に 'label' 列が見つかりません。ファイルの形式を確認してください。\")\n","        return None\n","    except Exception as e:\n","        print(f\"ファイル '{file_path}' の処理中にエラーが発生しました: {e}\")\n","        return None\n","\n","# train.tsv のラベル数をカウント\n","print(\"\\n--- 学習データ (train.tsv) ---\")\n","train_label_counts = count_labels(train_file_path, \"学習データ\")\n","\n","# dev.tsv のラベル数をカウント\n","print(\"\\n--- 検証データ (dev.tsv) ---\")\n","dev_label_counts = count_labels(dev_file_path, \"検証データ\")\n","\n","# (オプション) 結果をまとめて表示\n","if train_label_counts is not None:\n","    print(f\"\\n学習データのポジティブ(1)事例数: {train_label_counts.get(1, 0)}\")\n","    print(f\"学習データのネガティブ(0)事例数: {train_label_counts.get(0, 0)}\")\n","\n","if dev_label_counts is not None:\n","    print(f\"\\n検証データのポジティブ(1)事例数: {dev_label_counts.get(1, 0)}\")\n","    print(f\"検証データのネガティブ(0)事例数: {dev_label_counts.get(0, 0)}\")"]},{"cell_type":"markdown","metadata":{"id":"0EY71AIPgmJF"},"source":["## 61. 特徴ベクトル\n","\n","Bag of Words (BoW) に基づき、学習データ（`train.tsv`）および検証データ（`dev.tsv`）のテキストを特徴ベクトルに変換したい。ここで、ある事例のテキストの特徴ベクトルは、テキスト中に含まれる単語（スペース区切りのトークン）の出現頻度で構成する。例えば、\"too loud , too goofy\"というテキストに対応する特徴ベクトルは、以下のような辞書オブジェクトで表現される。\n","\n","```python\n","{'too': 2, 'loud': 1, ',': 1, 'goofy': 1}\n","```\n","\n","各事例はテキスト、特徴ベクトル、ラベルを格納した辞書オブジェクトでまとめておく。例えば、先ほどの\"too loud , too goofy\"に対してラベル\"0\"（ネガティブ）が付与された事例は、以下のオブジェクトで表現される。\n","\n","```python\n","{'text': 'too loud , too goofy', 'label': '0', 'feature': {'too': 2, 'loud': 1, ',': 1, 'goofy': 1}}\n","```\n","\n","学習データと検証データの各事例を上記のような辞書オブジェクトに変換したうえで、学習データと検証データのそれぞれを、辞書オブジェクトのリストとして表現せよ。さらに、学習データの最初の事例について、正しく特徴ベクトルに変換できたか、目視で確認せよ。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9qs2TpX-Qv6g"},"outputs":[],"source":["import pandas as pd\n","from collections import Counter\n","import os # ファイルパスの操作にosモジュールを使用\n","\n","# 問題60でSST-2データを展開した親ディレクトリへのパス\n","base_data_dir = '../data/SST-2_data'\n","\n","# train.tsv と dev.tsv へのフルパスを構築\n","train_file_path = os.path.join(base_data_dir, \"SST-2/train.tsv\")\n","dev_file_path = os.path.join(base_data_dir, \"SST-2/dev.tsv\")\n","# --- 設定箇所ここまで ---\n","\n","# DataFrameを格納する変数を初期化\n","df_train = None\n","df_dev = None\n","\n","# TSVファイルの読み込み\n","try:\n","    df_train = pd.read_csv(train_file_path, sep='\\t')\n","    df_dev = pd.read_csv(dev_file_path, sep='\\t')\n","    print(f\"'{train_file_path}' (学習データ) を読み込みました。件数: {len(df_train)}\")\n","    print(f\"'{dev_file_path}' (検証データ) を読み込みました。件数: {len(df_dev)}\")\n","except FileNotFoundError:\n","    print(f\"エラー: ファイルが見つかりません。\")\n","    print(f\"train_file_path: {train_file_path}\")\n","    print(f\"dev_file_path: {dev_file_path}\")\n","    print(\"上記のパスに train.tsv と dev.tsv が存在するか確認してください。\")\n","    print(\"問題60のデータダウンロード・展開が正しく完了しているか、パス指定が正しいか確認してください。\")\n","    # この後の処理に進めないため、ここで処理を中断する意味で例外を発生させるか、df_train, df_devをNoneのままにする\n","    # raise\n","\n","def text_to_bow_feature(text):\n","    \"\"\"テキストをスペースで分割し、BoW特徴ベクトル（単語の出現頻度辞書）を返す\"\"\"\n","    # テキストがNoneやfloat型(欠損値NaNなど)の場合を考慮して文字列に変換\n","    if not isinstance(text, str):\n","        text = str(text)\n","    tokens = text.split(' ') # スペースで分割\n","    return Counter(tokens)\n","\n","def convert_to_feature_list(df, df_name=\"データ\"):\n","    \"\"\"DataFrameを処理し、指定された形式の辞書オブジェクトのリストに変換する\"\"\"\n","    if df is None:\n","        print(f\"エラー: {df_name}のDataFrameがNoneです。読み込みに失敗している可能性があります。\")\n","        return []\n","\n","    feature_list = []\n","    # DataFrameの 'sentence' と 'label' 列が存在するか確認\n","    if 'sentence' not in df.columns or 'label' not in df.columns:\n","        print(f\"エラー: {df_name}のDataFrameに必要な列 'sentence' または 'label' がありません。\")\n","        print(f\"実際の列名: {df.columns.tolist()}\")\n","        return []\n","\n","    for index, row in df.iterrows():\n","        text = str(row['sentence']) # sentence列のテキスト\n","        label = str(row['label'])   # label列のラベル (文字列として保持)\n","        feature_bow = text_to_bow_feature(text)\n","\n","        feature_list.append({\n","            'text': text,\n","            'label': label,\n","            'feature': dict(feature_bow) # Counterを通常のdictに変換\n","        })\n","    return feature_list\n","\n","# --- main処理 ---\n","if df_train is not None and df_dev is not None:\n","    print(\"\\n学習データの変換を開始します...\")\n","    train_data_features = convert_to_feature_list(df_train, \"学習データ\")\n","    if train_data_features: # 変換が成功した場合のみ件数を表示\n","        print(f\"学習データの変換完了。{len(train_data_features)} 件の事例を処理しました。\")\n","\n","    print(\"\\n検証データの変換を開始します...\")\n","    dev_data_features = convert_to_feature_list(df_dev, \"検証データ\")\n","    if dev_data_features: # 変換が成功した場合のみ件数を表示\n","        print(f\"検証データの変換完了。{len(dev_data_features)} 件の事例を処理しました。\")\n","\n","    # 学習データの最初の事例について、正しく特徴ベクトルに変換できたか目視で確認\n","    if train_data_features: # リストが空でないことを確認\n","        print(\"\\n学習データの最初の事例:\")\n","        first_train_example = train_data_features[0]\n","        print(f\"  Text: {first_train_example['text']}\")\n","        print(f\"  Label: {first_train_example['label']}\")\n","        print(f\"  Feature (BoW): {first_train_example['feature']}\")\n","    else:\n","        if df_train is not None: # DataFrameはあったが変換結果が空の場合\n","             print(\"\\n学習データの特徴ベクトルリストが空です。変換処理に問題があった可能性があります。\")\n","else:\n","    print(\"\\n学習データまたは検証データのDataFrameの読み込みに失敗したため、特徴ベクトルへの変換処理をスキップしました。\")"]},{"cell_type":"markdown","metadata":{"id":"_i40n9yZnCsv"},"source":["## 62. 学習\n","\n","61で構築した学習データの特徴ベクトルを用いて、ロジスティック回帰モデルを学習せよ。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PX21NMzhQv6j"},"outputs":[],"source":["from sklearn.feature_extraction import DictVectorizer\n","from sklearn.linear_model import LogisticRegression\n","# import numpy as np # 必要に応じて\n","\n","# train_data_features は問題61で作成された学習データの特徴ベクトルのリストとします。\n","# 各要素は {'text': ..., 'label': ..., 'feature': ...} という辞書。\n","\n","if 'train_data_features' not in locals() or not train_data_features:\n","    print(\"エラー: 'train_data_features' が存在しないか空です。\")\n","    print(\"問題61を先に実行して、学習データの特徴ベクトルリストを生成してください。\")\n","    # この後の処理に進めないため、ここで処理を中断\n","    # raise NameError(\"train_data_features not defined or empty\")\n","else:\n","    print(f\"学習データ: {len(train_data_features)} 件\")\n","\n","    # 1. 学習データから特徴ベクトル(辞書)のリストとラベルのリストを抽出\n","    train_features_dicts = [item['feature'] for item in train_data_features]\n","    train_labels = [int(item['label']) for item in train_data_features] # ラベルを整数に変換\n","\n","    # 2. DictVectorizer を使って特徴ベクトルを数値行列に変換\n","    print(\"\\nDictVectorizer を使って特徴ベクトルを数値行列に変換します...\")\n","    vectorizer = DictVectorizer()\n","    X_train = vectorizer.fit_transform(train_features_dicts)\n","    # X_train は疎行列 (scipy.sparse.csr_matrix) になります。\n","    # ロジスティック回帰モデルは疎行列をそのまま扱えます。\n","\n","    print(f\"変換後の学習データ特徴ベクトルの形状: {X_train.shape}\")\n","    # X_train.shape は (事例数, ユニークな単語数) となるはずです。\n","    # print(\"最初の5つの特徴名（単語）:\", vectorizer.get_feature_names_out()[:5]) # どんな単語が特徴になったか確認\n","\n","    # 3. ラベルのリスト (y_train) は既に train_labels として準備済み\n","    y_train = train_labels\n","    # y_train = np.array(train_labels) # NumPy配列にしてもよい\n","\n","    # 4. ロジスティック回帰モデルの学習\n","    print(\"\\nロジスティック回帰モデルの学習を開始します...\")\n","    # モデルのインスタンスを作成\n","    # solver='liblinear' は比較的小さなデータセットや二値分類に適しています。\n","    # max_iter は収束計算の最大反復回数。データセットによっては増やす必要がある場合も。\n","    logreg_model = LogisticRegression(solver='liblinear', max_iter=1000, random_state=42)\n","\n","    # モデルを学習\n","    logreg_model.fit(X_train, y_train)\n","\n","    print(\"ロジスティック回帰モデルの学習が完了しました。\")\n","    print(f\"学習済みモデル: {logreg_model}\")"]},{"cell_type":"markdown","metadata":{"id":"1wuvGIAonZEF"},"source":["## 63. 予測\n","\n","学習したロジスティック回帰モデルを用い、検証データの先頭の事例のラベル（ポジネガ）を予測せよ。また、予測されたラベルが検証データで付与されていたラベルと一致しているか、確認せよ。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gSa_IEsCQv6k"},"outputs":[],"source":["# logreg_model (学習済みモデル) と vectorizer (適合済みDictVectorizer) は\n","# 問題62から引き継がれていると仮定します。\n","# dev_data_features (検証データの特徴リスト) は問題61から引き継がれていると仮定します。\n","\n","if 'logreg_model' not in locals() or 'vectorizer' not in locals() \\\n","   or 'dev_data_features' not in locals() or not dev_data_features:\n","    print(\"エラー: 'logreg_model', 'vectorizer', または 'dev_data_features' が存在しないか、\")\n","    print(\"       dev_data_featuresが空です。\")\n","    print(\"問題61および62を先に実行して、これらの変数とデータを準備してください。\")\n","else:\n","    # 2. 検証データの先頭の事例を取得\n","    first_dev_example = dev_data_features[0]\n","    text_to_predict = first_dev_example['text']\n","    true_label = int(first_dev_example['label']) # 比較のため整数に変換\n","    feature_dict_to_predict = first_dev_example['feature']\n","\n","    print(f\"検証データの先頭の事例:\")\n","    print(f\"  Text: {text_to_predict}\")\n","    print(f\"  True Label: {true_label}\")\n","\n","    # 3. 特徴ベクトルの変換 (学習済みのvectorizerでtransform)\n","    # transformメソッドは辞書のリストを期待するので、単一の辞書をリストに入れる\n","    X_pred_vec = vectorizer.transform([feature_dict_to_predict])\n","\n","    # 4. ラベルの予測\n","    predicted_label_array = logreg_model.predict(X_pred_vec)\n","    predicted_label = predicted_label_array[0] # predictは配列を返すので最初の要素を取得\n","\n","    print(f\"\\n予測されたラベル: {predicted_label}\")\n","\n","    # 5. 結果の確認\n","    if predicted_label == true_label:\n","        print(\"予測は正解です！\")\n","    else:\n","        print(\"予測は不正解です。\")"]},{"cell_type":"markdown","metadata":{"editable":true,"id":"ZHZht1jNoJPL","tags":[]},"source":["## 64. 条件付き確率\n","\n","学習したロジスティック回帰モデルを用い、検証データの先頭の事例を各ラベル（ポジネガ）に分類するときの条件付き確率を求めよ。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C2nHe-0HQv6q"},"outputs":[],"source":["# logreg_model (学習済みモデル) と vectorizer (適合済みDictVectorizer) は\n","# 問題62から引き継がれていると仮定します。\n","# dev_data_features (検証データの特徴リスト) は問題61から引き継がれていると仮定します。\n","\n","if 'logreg_model' not in locals() or 'vectorizer' not in locals() \\\n","   or 'dev_data_features' not in locals() or not dev_data_features:\n","    print(\"エラー: 'logreg_model', 'vectorizer', または 'dev_data_features' が存在しないか、\")\n","    print(\"       dev_data_featuresが空です。\")\n","    print(\"問題61および62を先に実行して、これらの変数とデータを準備してください。\")\n","else:\n","    # 2. 検証データの先頭の事例を取得\n","    first_dev_example = dev_data_features[0]\n","    text_to_analyze = first_dev_example['text']\n","    true_label = int(first_dev_example['label']) # 比較のため整数に変換\n","    feature_dict_to_analyze = first_dev_example['feature']\n","\n","    print(f\"検証データの先頭の事例:\")\n","    print(f\"  Text: {text_to_analyze}\")\n","    print(f\"  True Label: {true_label}\")\n","\n","    # 3. 特徴ベクトルの変換 (学習済みのvectorizerでtransform)\n","    X_analyze_vec = vectorizer.transform([feature_dict_to_analyze])\n","\n","    # 4. 条件付き確率の計算\n","    # predict_proba() は [クラス0の確率, クラス1の確率] のような配列を返す\n","    # (事例ごとに1行。今回は1事例なので1行2列)\n","    probabilities = logreg_model.predict_proba(X_analyze_vec)\n","\n","    # クラスの順序を確認 (通常は [0, 1])\n","    # print(f\"モデルのクラス: {logreg_model.classes_}\")\n","\n","    prob_negative = probabilities[0, 0] # 最初の事例の、最初のクラス(0:ネガティブ)の確率\n","    prob_positive = probabilities[0, 1] # 最初の事例の、2番目のクラス(1:ポジティブ)の確率\n","\n","    # logreg_model.classes_ を使って、より一般的に確率を取り出す場合\n","    # class_index_negative = list(logreg_model.classes_).index(0)\n","    # class_index_positive = list(logreg_model.classes_).index(1)\n","    # prob_negative = probabilities[0, class_index_negative]\n","    # prob_positive = probabilities[0, class_index_positive]\n","\n","    print(f\"\\n各ラベルに分類される条件付き確率:\")\n","    print(f\"  P(ネガティブ | 事例) = {prob_negative:.4f} (ラベル 0)\")\n","    print(f\"  P(ポジティブ | 事例) = {prob_positive:.4f} (ラベル 1)\")\n","\n","    # 問題63で予測したラベルも参考までに表示\n","    predicted_label = logreg_model.predict(X_analyze_vec)[0]\n","    print(f\"\\n参考: predict() による予測ラベル: {predicted_label}\")"]},{"cell_type":"markdown","metadata":{"editable":true,"id":"sa4FRl8kos_0","tags":[]},"source":["## 65. テキストのポジネガの予測\n","\n","与えられたテキストのポジネガを予測するプログラムを実装せよ。例えば、テキストとして\"the worst movie I 've ever seen\"を与え、ロジスティック回帰モデルの予測結果を確認せよ。\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZV7D8gQnQv6s"},"outputs":[],"source":["# logreg_model (学習済みモデル) と vectorizer (適合済みDictVectorizer) は\n","# 問題62から引き継がれていると仮定します。\n","# text_to_bow_feature 関数は問題61から引き継がれていると仮定します。\n","# from collections import Counter # もし Counter が未インポートなら必要\n","\n","if 'logreg_model' not in locals() or 'vectorizer' not in locals() or \\\n","   'text_to_bow_feature' not in locals():\n","    print(\"エラー: 'logreg_model', 'vectorizer', または 'text_to_bow_feature' が定義されていません。\")\n","    print(\"問題61および62を先に実行して、これらの変数と関数を準備してください。\")\n","else:\n","    def predict_sentiment(text_input, model, bow_vectorizer):\n","        \"\"\"\n","        与えられたテキストのポジネガを予測し、ラベルと確率を返す関数。\n","\n","        Args:\n","            text_input (str): 予測対象のテキスト。\n","            model (LogisticRegression): 学習済みのロジスティック回帰モデル。\n","            bow_vectorizer (DictVectorizer): 学習済みのDictVectorizer。\n","\n","        Returns:\n","            tuple: (予測ラベル(int), ポジティブ確率(float), ネガティブ確率(float))\n","        \"\"\"\n","        # 1. テキストをBoW特徴辞書に変換\n","        feature_dict = text_to_bow_feature(text_input)\n","\n","        # 2. BoW特徴辞書を数値ベクトルに変換 (transformは辞書のリストを期待)\n","        X_vec = bow_vectorizer.transform([feature_dict])\n","\n","        # 3. ラベルを予測\n","        predicted_label = model.predict(X_vec)[0]\n","\n","        # 4. 各クラスの確率を予測\n","        probabilities = model.predict_proba(X_vec)[0] # [P(class_0), P(class_1)]\n","\n","        # model.classes_ を確認して、どのインデックスが0と1に対応するかを見る\n","        # 通常は [0, 1] の順\n","        class_index_negative = 0\n","        class_index_positive = 1\n","        if hasattr(model, 'classes_'): # classes_属性があるか確認\n","            try:\n","                class_index_negative = list(model.classes_).index(0)\n","                class_index_positive = list(model.classes_).index(1)\n","            except ValueError:\n","                print(\"警告: モデルのクラス属性に0または1が見つかりません。確率の順序が不正確な可能性があります。\")\n","\n","\n","        prob_negative = probabilities[class_index_negative]\n","        prob_positive = probabilities[class_index_positive]\n","\n","        return int(predicted_label), prob_positive, prob_negative\n","\n","    # 問題文で指定されたテキストでテスト\n","    test_text = \"the worst movie I 've ever seen\"\n","\n","    predicted_label, positive_prob, negative_prob = predict_sentiment(test_text, logreg_model, vectorizer)\n","\n","    print(f\"入力テキスト: \\\"{test_text}\\\"\")\n","    print(f\"  予測ラベル: {predicted_label} ({'ポジティブ' if predicted_label == 1 else 'ネガティブ'})\")\n","    print(f\"  ポジティブである確率: {positive_prob:.4f}\")\n","    print(f\"  ネガティブである確率: {negative_prob:.4f}\")\n","\n","    print(\"\\n別のテキストでも検証\")\n","    test_text_positive = \"This is a fantastic and wonderful film with great acting.\"\n","    predicted_label_p, positive_prob_p, negative_prob_p = predict_sentiment(test_text_positive, logreg_model, vectorizer)\n","    print(f\"\\n入力テキスト: \\\"{test_text_positive}\\\"\")\n","    print(f\"  予測ラベル: {predicted_label_p} ({'ポジティブ' if predicted_label_p == 1 else 'ネガティブ'})\")\n","    print(f\"  ポジティブである確率: {positive_prob_p:.4f}\")\n","    print(f\"  ネガティブである確率: {negative_prob_p:.4f}\")"]},{"cell_type":"markdown","metadata":{"editable":true,"id":"JpIsH36Upci_","tags":[]},"source":["## 66. 混同行列の作成\n","\n","学習したロジスティック回帰モデルの検証データにおける混同行列（confusion matrix）を求めよ。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RtAmqlXHQv6y"},"outputs":[],"source":["from sklearn.metrics import confusion_matrix\n","# import matplotlib.pyplot as plt # 視覚化する場合\n","# import seaborn as sns          # 視覚化する場合\n","# import numpy as np             # 視覚化のラベル設定などに使う場合\n","\n","# logreg_model, vectorizer, dev_data_features は前の問題から引き継がれていると仮定\n","\n","if 'logreg_model' not in locals() or 'vectorizer' not in locals() \\\n","   or 'dev_data_features' not in locals() or not dev_data_features:\n","    print(\"エラー: 'logreg_model', 'vectorizer', または 'dev_data_features' が存在しないか、\")\n","    print(\"       dev_data_featuresが空です。\")\n","    print(\"問題61および62を先に実行して、これらの変数とデータを準備してください。\")\n","else:\n","    print(f\"検証データ: {len(dev_data_features)} 件\")\n","\n","    # 2. 検証データから特徴辞書のリストと真のラベルのリストを抽出\n","    dev_features_dicts = [item['feature'] for item in dev_data_features]\n","    dev_true_labels = [int(item['label']) for item in dev_data_features] # ラベルを整数に変換\n","\n","    # 3. 検証データの特徴ベクトルを数値行列に変換し、ラベルを予測\n","    if dev_features_dicts: # 特徴辞書リストが空でないことを確認\n","        X_dev = vectorizer.transform(dev_features_dicts)\n","        dev_predicted_labels = logreg_model.predict(X_dev)\n","\n","        # 4. 混同行列の計算\n","        # labels=[0, 1] を指定することで、行列の行と列が ネガティブ(0), ポジティブ(1) の順になることを保証\n","        cm = confusion_matrix(dev_true_labels, dev_predicted_labels, labels=[0, 1])\n","\n","        print(\"\\n検証データにおける混同行列:\")\n","        print(cm)\n","        # cm の見方:\n","        # [[TN, FP],\n","        #  [FN, TP]]\n","        # TN: True Negative (正しくネガティブと予測)\n","        # FP: False Positive (ネガティブをポジティブと誤予測)\n","        # FN: False Negative (ポジティブをネガティブと誤予測)\n","        # TP: True Positive (正しくポジティブと予測)\n","\n","        print(f\"\\n  True Negative (TN) [予測=0, 真=0]: {cm[0, 0]}\")\n","        print(f\"  False Positive (FP)[予測=1, 真=0]: {cm[0, 1]}\")\n","        print(f\"  False Negative (FN)[予測=0, 真=1]: {cm[1, 0]}\")\n","        print(f\"  True Positive (TP) [予測=1, 真=1]: {cm[1, 1]}\")\n","    else:\n","        print(\"検証データの特徴辞書リストが空です。問題61の処理を確認してください。\")"]},{"cell_type":"markdown","metadata":{"editable":true,"id":"wVGFDz94oaqV","tags":[]},"source":["## 67. 精度の計測\n","\n","学習したロジスティック回帰モデルの正解率、適合率、再現率、F1スコアを、学習データおよび検証データ上で計測せよ。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2elO0GVPQv6z"},"outputs":[],"source":["from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","# import numpy as np # 必要に応じて\n","\n","# logreg_model, vectorizer, train_data_features, dev_data_features は\n","# 前の問題から引き継がれていると仮定します。\n","\n","if 'logreg_model' not in locals() or 'vectorizer' not in locals() or \\\n","   'train_data_features' not in locals() or not train_data_features or \\\n","   'dev_data_features' not in locals() or not dev_data_features:\n","    print(\"エラー: 必要な変数 ('logreg_model', 'vectorizer', 'train_data_features', 'dev_data_features')\")\n","    print(\"       が存在しないか、データが空です。\")\n","    print(\"問題61および62を先に実行して、これらの変数とデータを準備してください。\")\n","else:\n","    def calculate_and_print_metrics(dataset_name, data_features, model, vec):\n","        \"\"\"\n","        与えられたデータセットの特徴から予測を行い、評価指標を計算・表示する関数\n","        \"\"\"\n","        print(f\"\\n--- {dataset_name} ---\")\n","        if not data_features:\n","            print(f\"{dataset_name} が空のため、評価をスキップします。\")\n","            return\n","\n","        features_dicts = [item['feature'] for item in data_features]\n","        true_labels = [int(item['label']) for item in data_features]\n","\n","        if not features_dicts: # まれにdata_featuresはあるが中身が空の場合\n","            print(f\"{dataset_name} の特徴辞書リストが空です。\")\n","            return\n","\n","        # 特徴ベクトルを数値行列に変換\n","        X_data = vec.transform(features_dicts)\n","\n","        # ラベルを予測\n","        predicted_labels = model.predict(X_data)\n","\n","        # 評価指標の計算 (ポジティブクラス(1)に対するものを主に計算)\n","        accuracy = accuracy_score(true_labels, predicted_labels)\n","        # precision, recall, F1 は二値分類でラベルが {0, 1} の場合、pos_label=1 がデフォルト動作だが明示しても良い\n","        precision = precision_score(true_labels, predicted_labels, pos_label=1, zero_division=0)\n","        recall = recall_score(true_labels, predicted_labels, pos_label=1, zero_division=0)\n","        f1 = f1_score(true_labels, predicted_labels, pos_label=1, zero_division=0)\n","        # zero_division=0 は、もし適合率などが計算不能(例:ポジティブと予測したものが0件)の場合に0を出力する設定\n","\n","        print(f\"  正解率 (Accuracy): {accuracy:.4f}\")\n","        print(f\"  適合率 (Precision for positive class): {precision:.4f}\")\n","        print(f\"  再現率 (Recall for positive class): {recall:.4f}\")\n","        print(f\"  F1スコア (F1 Score for positive class): {f1:.4f}\")\n","\n","    # 学習データに対する評価指標を計算・表示\n","    calculate_and_print_metrics(\"学習データ\", train_data_features, logreg_model, vectorizer)\n","\n","    # 検証データに対する評価指標を計算・表示\n","    calculate_and_print_metrics(\"検証データ\", dev_data_features, logreg_model, vectorizer)"]},{"cell_type":"markdown","metadata":{"id":"N14sd55qq-xS"},"source":["## 68. 特徴量の重みの確認\n","\n","学習したロジスティック回帰モデルの中で、重みの高い特徴量トップ20と、重みの低い特徴量トップ20を確認せよ。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jxw7doTOQv60"},"outputs":[],"source":["import numpy as np\n","\n","# logreg_model (学習済みモデル) と vectorizer (適合済みDictVectorizer) は\n","# 問題62から引き継がれていると仮定します。\n","\n","if 'logreg_model' not in locals() or 'vectorizer' not in locals():\n","    print(\"エラー: 'logreg_model' または 'vectorizer' が定義されていません。\")\n","    print(\"問題62を先に実行して、モデルとvectorizerを準備してください。\")\n","else:\n","    # 2. 特徴量（単語）と重みの対応付け\n","    # logreg_model.coef_ は通常 [[重み1, 重み2, ...]] のような形状 (1行N特徴量列)\n","    # なので、[0]で最初の（そして唯一の）行の重みリストを取得\n","    weights = logreg_model.coef_[0]\n","\n","    # vectorizer.get_feature_names_out() で特徴名（単語）のリストを取得\n","    # 古いscikit-learnでは get_feature_names()\n","    try:\n","        feature_names = vectorizer.get_feature_names_out()\n","    except AttributeError:\n","        # 古いバージョン用のフォールバック\n","        feature_names = vectorizer.get_feature_names()\n","\n","    if len(weights) != len(feature_names):\n","        print(\"エラー: 重みの数と特徴名の数が一致しません。何かがおかしいです。\")\n","    else:\n","        # 特徴名と重みをペアにしてリスト化\n","        feature_weights = list(zip(feature_names, weights))\n","\n","        # 3. 重みに基づくソート\n","        # 重みで昇順ソート (最もネガティブに寄与するものが先頭)\n","        sorted_by_weight_asc = sorted(feature_weights, key=lambda x: x[1])\n","\n","        # 重みで降順ソート (最もポジティブに寄与するものが先頭)\n","        sorted_by_weight_desc = sorted(feature_weights, key=lambda x: x[1], reverse=True)\n","\n","        # 4. トップ20とワースト20の表示\n","        num_top_features = 20\n","\n","        print(f\"\\n--- 重みの高い特徴量トップ{num_top_features} (ポジティブに寄与) ---\")\n","        for feature, weight in sorted_by_weight_desc[:num_top_features]:\n","            print(f\"  {feature}: {weight:.4f}\")\n","\n","        print(f\"\\n--- 重みの低い特徴量トップ{num_top_features} (ネガティブに寄与) ---\")\n","        # 昇順ソートの結果なので、先頭からが最も低い (負に大きい)\n","        for feature, weight in sorted_by_weight_asc[:num_top_features]:\n","            print(f\"  {feature}: {weight:.4f}\")"]},{"cell_type":"markdown","metadata":{"id":"QiEXYV__rJYR"},"source":["## 69. 正則化パラメータの変更\n","\n","ロジスティック回帰モデルを学習するとき、正則化の係数（ハイパーパラメータ）を調整することで、学習時の適合度合いを制御できる。正則化の係数を変化させながらロジスティック回帰モデルを学習し、検証データ上の正解率を求めよ。実験の結果は、正則化パラメータを横軸、正解率を縦軸としたグラフにまとめよ。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tw5ZXeuNQv61"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score\n","# from sklearn.feature_extraction import DictVectorizer # vectorizerは既にロード済みのはず\n","\n","# vectorizer, train_data_features, dev_data_features, logreg_model は\n","# 前の問題から引き継がれていると仮定します。\n","\n","if 'vectorizer' not in locals() or \\\n","   'train_data_features' not in locals() or not train_data_features or \\\n","   'dev_data_features' not in locals() or not dev_data_features:\n","    print(\"エラー: 必要な変数 ('vectorizer', 'train_data_features', 'dev_data_features')\")\n","    print(\"       が存在しないか、データが空です。\")\n","    print(\"問題61および62を先に実行して、これらの変数とデータを準備してください。\")\n","else:\n","    # 1. 学習データと検証データの特徴ベクトルとラベルを準備\n","    # 学習データ\n","    train_features_dicts = [item['feature'] for item in train_data_features]\n","    y_train = [int(item['label']) for item in train_data_features]\n","    X_train = vectorizer.transform(train_features_dicts)\n","\n","    # 検証データ\n","    dev_features_dicts = [item['feature'] for item in dev_data_features]\n","    y_dev_true = [int(item['label']) for item in dev_data_features]\n","    X_dev = vectorizer.transform(dev_features_dicts)\n","\n","    # 2. 試行する正則化パラメータ C の範囲を設定\n","    # C の値は正則化の強さの「逆数」。小さいほど正則化が強い。\n","    c_values = [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]\n","    # c_values = np.logspace(-4, 3, 8) # np.logspaceを使うと対数スケールで値を生成できる\n","\n","    dev_accuracies = [] # 検証データの正解率を格納するリスト\n","\n","    print(\"正則化パラメータCを変化させてモデルを学習し、検証データで評価します...\")\n","    # 3. 各 C の値についてモデルを学習し、検証データで評価\n","    for c_param in c_values:\n","        print(f\"  C = {c_param}: 学習中...\")\n","        # ロジスティック回帰モデルのインスタンスを作成 (Cパラメータを指定)\n","        model = LogisticRegression(C=c_param, solver='liblinear', max_iter=1000, random_state=42)\n","\n","        # モデルを学習\n","        model.fit(X_train, y_train)\n","\n","        # 検証データで予測\n","        y_dev_pred = model.predict(X_dev)\n","\n","        # 正解率を計算\n","        accuracy = accuracy_score(y_dev_true, y_dev_pred)\n","        dev_accuracies.append(accuracy)\n","        print(f\"    検証データの正解率: {accuracy:.4f}\")\n","\n","    # 4. 結果のグラフ化\n","    plt.figure(figsize=(10, 6))\n","    plt.plot(c_values, dev_accuracies, marker='o')\n","\n","    plt.xscale('log') # 横軸を対数スケールに\n","    plt.xlabel('Regularization Parameter C (Inverse of regularization strength)')\n","    plt.ylabel('Accuracy on Validation Set')\n","    plt.title('Logistic Regression Accuracy vs. Regularization Parameter C')\n","    plt.grid(True, which=\"both\", ls=\"--\") # グリッド線を表示 (主グリッドと補助グリッド)\n","\n","    # 各点にCの値をテキスト表示（任意）\n","    # for i, c_val in enumerate(c_values):\n","    #     plt.text(c_val, dev_accuracies[i] + 0.001, f\"C={c_val}\\nAcc={dev_accuracies[i]:.3f}\", ha='center', va='bottom', fontsize=8)\n","\n","    plt.show()\n","\n","    # 最も正解率が高かったCの値と、その時の正解率を表示\n","    if dev_accuracies:\n","        best_accuracy_idx = np.argmax(dev_accuracies)\n","        best_c = c_values[best_accuracy_idx]\n","        best_acc = dev_accuracies[best_accuracy_idx]\n","        print(f\"\\n検証データで最も正解率が高かったのは C = {best_c} のときで、正解率は {best_acc:.4f} でした。\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JvYNZCyAQv61"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.12"}},"nbformat":4,"nbformat_minor":0}